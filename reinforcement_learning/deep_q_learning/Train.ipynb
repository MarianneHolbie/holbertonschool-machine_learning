{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a60100ce-b1e7-4080-ba48-31d3f88ec0ed",
   "metadata": {},
   "source": [
    "# Deep Q Learning pour Breakout d'Atari: **train script**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2044e739-46d3-4b6a-9614-4cbb798f9646",
   "metadata": {},
   "source": [
    "Dans ce notebook, on implémente l'entraînement d'un agent de Deep Q-Learning (DQN) pour jouer au jeu Breakout d'Atari. \n",
    "Pour cela, on utilise les bibliothèques Keras-rl, TensorFlow et Gymnasium pour construire l'environnement d'entraînement et entraîner l'agent.\n",
    "\n",
    "Rq: pour avoir les jeux Atari dans gymnasium utiliser `pip install gymnasium[atari]`\n",
    "\n",
    "## 1. Configuration de l'environnement\n",
    "\n",
    "Importation des différentes bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ca04ff0-8595-4fe6-8fde-9b6f39f8e0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33626\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import FrameStack, AtariPreprocessing\n",
    "from keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Permute\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from rl.util import *\n",
    "from rl.core import Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d504d537-6f0a-44fc-95ef-23616e0cb90d",
   "metadata": {},
   "source": [
    "On définit ensuite les paramètres contrôlant les divers aspects de l'apprentissage (notamment le taux d'exploration epsilon), le facteur de réduction des récompenses (gamma), la taille des batch pour l'apprentissage, le nombre maximums d'épisodes ou d'étapes dans les épisodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c047e3da-a1fa-4480-a9f4-7d7f5127bae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres de configuration\n",
    "seed = 42\n",
    "gamma = 0.99  # Facteur de réduction pour les récompenses futures\n",
    "epsilon = 1.0  # Paramètre epsilon-greedy\n",
    "epsilon_min = 0.1  # Valeur minimale d'epsilon\n",
    "epsilon_max = 1.0  # Valeur maximale d'epsilon\n",
    "epsilon_interval = epsilon_max - epsilon_min\n",
    "batch_size = 32  # Taille du batch pour l'apprentissage\n",
    "max_steps_per_episode = 10000\n",
    "max_episodes = 10  # Limite le nombre d'épisodes d'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a97293-1e52-437b-bd69-b8c6782f9587",
   "metadata": {},
   "source": [
    "## 2. Préparation des données\n",
    "\n",
    "Dans cette partie, on définit une class `CompatibilityWrapper` qui sert d'interface entre les différentes versions de l'API Gymnasium. La bibliothèque `Keras-rl` a été conçue pour fonctionner avec l'ancienne version de l'API Gym, elle attend donc une certaine forme de l'environnement.\n",
    "Les changements dans l'API Gymnasium concernent essentiellement les méthodes `step()` et `reset()`.\n",
    "\n",
    "* Différence dans la méthode `step()` :\n",
    "\n",
    "    - Ancienne version : `observation, reward, done, info = env.step(action)`\n",
    "    - Nouvelle version : `observation, reward, terminated, truncated, info = env.step(action)`\n",
    "\n",
    "Le `CompatibilityWrapper` combine terminated et truncated en un seul booléen done, ce qui correspond à l'ancien format.\n",
    "\n",
    "* Différence dans la méthode `reset()` :\n",
    "\n",
    "    - Ancienne version : `observation = env.reset()`\n",
    "    - Nouvelle version : `observation, info = env.reset()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49679568-b00f-4d18-b2b8-4f7301147b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompatibilityWrapper(gym.Wrapper):\n",
    "    def step(self, action):\n",
    "        # Appelle la méthode step de l'environnement sous-jacent\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        # Combine terminated et truncated en un seul booléen 'done'\n",
    "        done = terminated or truncated\n",
    "        # Retourne le format attendu par l'ancien API\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        # Appelle la méthode reset de l'environnement sous-jacent\n",
    "        observation, info = self.env.reset(**kwargs)\n",
    "        # Ne renvoie que l'observation pour correspondre à l'ancien format\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f353ae42-f3c6-4fae-9da2-cb1a33a94ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function pour la création de l'environnement\n",
    "def create_atari_environment(env_name, num_stack=4):\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    env = AtariPreprocessing(env, screen_size=84, grayscale_obs=True, frame_skip=1, noop_max=30)\n",
    "    env = CompatibilityWrapper(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9089607a-5aad-400a-8e3e-48abb588eb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of stacked observation: (84, 84)\n",
      "Type of observation: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Créer l'environnement\n",
    "env = create_atari_environment('ALE/Breakout-v5')\n",
    "\n",
    "# Réinitialiser l'environnement\n",
    "observation = env.reset()\n",
    "\n",
    "print(f\"Shape of stacked observation: {observation.shape}\")\n",
    "print(f\"Type of observation: {type(observation)}\")\n",
    "\n",
    "# Visualiser une frame\n",
    "plt.imshow(observation, cmap='gray')\n",
    "plt.title(\"Initial Observation\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60accfdb-e4ac-4f08-b0d4-9f7ecbd80231",
   "metadata": {},
   "source": [
    "### Pretaitement des observations\n",
    "\n",
    "`AtariPreprocessing`est un wrapper fourni par Gymnasium pour appliquer un prétraitement commun aux environnement des jeux de type Atari. Il permet de réduire la complexité de l'entrée et d'accélérer l'apprentissage des agents.\n",
    "\n",
    "`env = AtariPreprocessing(env, screen_size=84, grayscale_obs=True, frame_skip=1, noop_max=30)`\n",
    "\n",
    "* **Redimensionnement de l'image (screen_size=84)** : réduction de l'image à 84x84 pixels.\n",
    "Objectif : Diminuer la complexité de l'entrée tout en conservant les informations essentielles.\n",
    "* **Conversion en niveaux de gris (grayscale_obs=True)** : conversion de l'image RGB en niveaux de gris.\n",
    "Objectif : Réduire la dimensionnalité de l'entrée (de 3 canaux à 1) sans perdre d'informations cruciales pour le jeu.\n",
    "* **Saut de frames (frame_skip=1)** : Détermine combien de frames sont sautées entre chaque action.\n",
    "Ici, réglé à 1, ce qui signifie qu'aucune frame n'est sautée.\n",
    "Objectif : Peut être augmenté pour accélérer l'apprentissage en réduisant la fréquence des décisions.\n",
    "* **Actions \"no-op\" au début (noop_max=30)** : Applique un nombre aléatoire (jusqu'à 30) d'actions \"no-op\" au début de chaque épisode.\n",
    "Objectif : Introduire de la variabilité dans les conditions initiales pour une meilleure généralisation.\n",
    "\n",
    "`AtariProcessor`est une classe personnalisée qui hérite de Processor de keras-rl. Elle effectue des traitements supplémentaires pour les observations, les états et les récompenses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "362053e0-1ed6-4791-9de8-2feeb6f37d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        # gère le cas où l'observation est un tuple\n",
    "        if isinstance(observation, tuple):\n",
    "            observation = observation[0]\n",
    "        img = np.array(observation)\n",
    "        img = img.astype('uint8')\n",
    "        return img\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        # conversion et normalisation des valeurs entre 0 et 1 pour l'entrée dans le réseau de neurones\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        # pour stabiliser l'apprentissage en éviant les grandes variations de récompenses\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb086c5-3381-4fb7-bbda-9dc664a5bf5d",
   "metadata": {},
   "source": [
    "Rq : Importance du prétraitement des données dans le Deep Q learning\n",
    "* **Réduction de la complexité** : En réduisant la taille et la dimensionnalité des images, on diminue la complexité du problème d'apprentissage.\n",
    "* **Stabilité de l'apprentissage** : La normalisation des entrées et le clipping des récompenses aident à stabiliser le processus d'apprentissage.\n",
    "* **Généralisation** : L'introduction de variabilité (comme les actions no-op) aide l'agent à mieux généraliser.\n",
    "* **Efficacité computationnelle** : Des entrées plus petites et plus simples permettent un apprentissage plus rapide et moins gourmand en ressources.\n",
    "* **Cohérence des données** : Assure que les données fournies au réseau neuronal sont toujours dans un format cohérent et approprié."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6484614c-6f26-44ec-9167-c5474d3d8b73",
   "metadata": {},
   "source": [
    "# 4. Construction du modèle\n",
    "\n",
    "- **Entrée** : état (une séquence d'images de jeu)\n",
    "- **Sortie**: les valeurs Q estimées pour chaque action possible\n",
    "- **Architecture** inspirée de l'[article original](http://arxiv.org/abs/1312.5602) (Mnih et al. 2015)\n",
    "\n",
    "Détails de l'architecture:\n",
    "* Les couches convolutionnelles permettent d'extraire les caractéristiques spatiales importantes de l'image du jeu comme la position de la balle, de la raquette et des briques\n",
    "* On réduit progressivement la dimension : les strides réduisent progressivement la dimension spatiale tout en augmentant le nombre de filtres, le réseau va donc capture des caractéristiques de plus en plus abstraites.\n",
    "* Les couches denses finales vont permettre de combiner les caractéristiques extraites pour estimer les valeurs Q pour chaque action.\n",
    "* Activation ReLU: dans toutes les couches sauf la dernière, elle introduit la non linéarité et aide à l'apprentissage de représentations complexes.\n",
    "* Sortie linéaire : prédiction des valeurs Q sans restriction de plage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91604eff-9be8-47b0-a91c-00ed04a3e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(window_length, shape, actions):\n",
    "    model = Sequential()\n",
    "    # réorganise les dimensions d'entrée en (h,w,window_length)\n",
    "    model.add(Permute((2, 3, 1), input_shape=(window_length,) + shape))\n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu'))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model\n",
    "\n",
    "# entrée = sequence de 4 frames consécutives\n",
    "window_length = 4\n",
    "model = build_model(window_length, observation.shape, nb_actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71c1f48-7bc7-418e-b910-40f9cb3f9fb8",
   "metadata": {},
   "source": [
    "## 4. Définition de l'Agent DQN\n",
    "\n",
    "### Memoire Séquentielle\n",
    "Utilisation d'une mémoire séquentielle pour stocker les expériences des agents\n",
    "Elle va stocker jusqu'à 1 million d'expériences\n",
    "Elle utilisera le nombre de frames consécutives définies ultérieurement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aec62bbf-5857-419a-96d4-1a607e6d5c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sequential memory to store agent's experience\n",
    "memory = SequentialMemory(limit=1000000,\n",
    "                          window_length=window_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7677361-38a3-4af1-930f-fa246437345c",
   "metadata": {},
   "source": [
    "### Prétraitement des observations, des états et des récompenses\n",
    "Processeur Atari défini et ajusté pour les besoins.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44d0e5bd-63e2-4523-9a50-c9eef9745c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define processor to preprocess obs, states and reward\n",
    "processor = AtariProcessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eea5a3-3282-4d6f-98f6-5e2c1b34d519",
   "metadata": {},
   "source": [
    "### Politique d'exploration\n",
    "\n",
    "On définit une politique epsilon-greedy avec décroissance linéaire.\n",
    "Elle commence avec une exploration maximale (eps=1) et décroie progressivement de 0.1 sur 1 million d'étapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ac0dcd6-017a-43a8-ac9d-75a11993dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an epsilon-greedy policy with linear annealing\n",
    "# for exploration-exploitation trade-off\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
    "                              attr='eps',\n",
    "                              value_max=1.,\n",
    "                              value_min=.1,\n",
    "                              value_test=.05,\n",
    "                              nb_steps=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7464fda-4f2d-4b81-871b-f817cc324d28",
   "metadata": {},
   "source": [
    "#### Définition de l'agent DQN\n",
    "\n",
    "* `model`: réseau de neurone défini\n",
    "* `nb_actions`: Nombre d'actions possibles dans l'env\n",
    "* `policy`: politique d'exploration\n",
    "* `memory`: mémoire séquentielle\n",
    "* `processor`: processeur adapté\n",
    "* `nb_steps_warmup=5000`: nombre d'étapes avant de commencer l'apprentissage\n",
    "* `gamma=0.99`: facteur de réduction pour les récompenses futures\n",
    "* `target_model_update=10000`: Fréquence de mise à jour du modèle cible\n",
    "* `train_interval=4`: intervalle entre les mises à jour du réseau\n",
    "* `delta_clip=1`: Clip de l'erreur pour la stabilité de l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d2aaa9c-7ee2-4fe2-81f4-4b2f399c9344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a DQN agent with specified components and parameters\n",
    "dqn = DQNAgent(model=model,\n",
    "               nb_actions=nb_actions,\n",
    "               policy=policy,\n",
    "               memory=memory,\n",
    "               processor=processor,\n",
    "               nb_steps_warmup=50000,\n",
    "               gamma=.99,\n",
    "               target_model_update=10000,\n",
    "               train_interval=4,\n",
    "               delta_clip=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ea8e62-fa43-481f-ba01-c922eb5c336b",
   "metadata": {},
   "source": [
    "#### Compilation de l'agent\n",
    "\n",
    "**optimiser** Adam avec un taux d'apprentissage de 0.00025\n",
    "**métrique**: Erreur absolue moyenne (mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bb198f4-e670-4e0d-9d29-cee172043bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the DQN agent with Adam optimizer and mae as metrics\n",
    "dqn.compile(Adam(learning_rate=0.00025),\n",
    "            metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302efc19-6cf0-4b8b-ac71-21c32142e3c4",
   "metadata": {},
   "source": [
    "## 5. Entraînement du modèle\n",
    "\n",
    "L'agent est entrainé pendant 1 million d'étapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e547977-6faa-4cb9-a60d-e4c0c54f3428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33626\\anaconda3\\envs\\deep\\lib\\site-packages\\keras\\src\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    155/1000000: episode: 1, duration: 0.434s, episode steps: 155, steps per second: 357, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    282/1000000: episode: 2, duration: 0.263s, episode steps: 127, steps per second: 483, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.315 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    465/1000000: episode: 3, duration: 0.342s, episode steps: 183, steps per second: 536, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.497 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    649/1000000: episode: 4, duration: 0.345s, episode steps: 184, steps per second: 533, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.446 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    865/1000000: episode: 5, duration: 0.421s, episode steps: 216, steps per second: 513, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   1029/1000000: episode: 6, duration: 0.340s, episode steps: 164, steps per second: 482, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.433 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   1215/1000000: episode: 7, duration: 0.372s, episode steps: 186, steps per second: 500, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   1434/1000000: episode: 8, duration: 0.429s, episode steps: 219, steps per second: 511, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   1617/1000000: episode: 9, duration: 0.373s, episode steps: 183, steps per second: 491, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.388 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   1832/1000000: episode: 10, duration: 0.441s, episode steps: 215, steps per second: 488, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   2048/1000000: episode: 11, duration: 0.458s, episode steps: 216, steps per second: 472, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.389 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   2233/1000000: episode: 12, duration: 0.388s, episode steps: 185, steps per second: 477, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   2374/1000000: episode: 13, duration: 0.260s, episode steps: 141, steps per second: 543, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.433 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   2549/1000000: episode: 14, duration: 0.332s, episode steps: 175, steps per second: 527, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   2752/1000000: episode: 15, duration: 0.451s, episode steps: 203, steps per second: 450, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   2956/1000000: episode: 16, duration: 0.461s, episode steps: 204, steps per second: 442, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.309 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   3174/1000000: episode: 17, duration: 0.491s, episode steps: 218, steps per second: 444, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.528 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   3435/1000000: episode: 18, duration: 0.569s, episode steps: 261, steps per second: 459, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   3633/1000000: episode: 19, duration: 0.378s, episode steps: 198, steps per second: 524, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   3854/1000000: episode: 20, duration: 0.474s, episode steps: 221, steps per second: 466, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   4078/1000000: episode: 21, duration: 0.502s, episode steps: 224, steps per second: 446, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   4203/1000000: episode: 22, duration: 0.275s, episode steps: 125, steps per second: 455, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.424 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   4428/1000000: episode: 23, duration: 0.500s, episode steps: 225, steps per second: 450, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.440 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   4633/1000000: episode: 24, duration: 0.427s, episode steps: 205, steps per second: 481, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   4771/1000000: episode: 25, duration: 0.266s, episode steps: 138, steps per second: 520, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   4983/1000000: episode: 26, duration: 0.420s, episode steps: 212, steps per second: 505, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   5162/1000000: episode: 27, duration: 0.370s, episode steps: 179, steps per second: 484, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.458 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   5418/1000000: episode: 28, duration: 0.550s, episode steps: 256, steps per second: 466, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.457 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   5549/1000000: episode: 29, duration: 0.252s, episode steps: 131, steps per second: 520, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.588 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   5729/1000000: episode: 30, duration: 0.333s, episode steps: 180, steps per second: 541, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   5901/1000000: episode: 31, duration: 0.374s, episode steps: 172, steps per second: 460, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   6033/1000000: episode: 32, duration: 0.281s, episode steps: 132, steps per second: 470, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.447 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   6244/1000000: episode: 33, duration: 0.425s, episode steps: 211, steps per second: 497, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   6368/1000000: episode: 34, duration: 0.268s, episode steps: 124, steps per second: 462, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.669 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   6576/1000000: episode: 35, duration: 0.431s, episode steps: 208, steps per second: 482, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.351 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   6707/1000000: episode: 36, duration: 0.276s, episode steps: 131, steps per second: 475, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.450 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   6836/1000000: episode: 37, duration: 0.279s, episode steps: 129, steps per second: 463, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.426 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   7037/1000000: episode: 38, duration: 0.344s, episode steps: 201, steps per second: 584, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   7184/1000000: episode: 39, duration: 0.263s, episode steps: 147, steps per second: 559, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.469 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   7374/1000000: episode: 40, duration: 0.380s, episode steps: 190, steps per second: 500, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   7603/1000000: episode: 41, duration: 0.466s, episode steps: 229, steps per second: 492, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   7731/1000000: episode: 42, duration: 0.409s, episode steps: 128, steps per second: 313, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   7854/1000000: episode: 43, duration: 0.294s, episode steps: 123, steps per second: 419, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.480 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   8139/1000000: episode: 44, duration: 0.606s, episode steps: 285, steps per second: 470, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   8365/1000000: episode: 45, duration: 0.486s, episode steps: 226, steps per second: 465, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   8564/1000000: episode: 46, duration: 0.433s, episode steps: 199, steps per second: 460, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.508 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   8780/1000000: episode: 47, duration: 0.451s, episode steps: 216, steps per second: 479, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.449 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   8986/1000000: episode: 48, duration: 0.374s, episode steps: 206, steps per second: 551, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   9112/1000000: episode: 49, duration: 0.245s, episode steps: 126, steps per second: 515, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.405 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   9275/1000000: episode: 50, duration: 0.281s, episode steps: 163, steps per second: 581, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.650 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   9566/1000000: episode: 51, duration: 0.527s, episode steps: 291, steps per second: 552, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   9825/1000000: episode: 52, duration: 0.570s, episode steps: 259, steps per second: 454, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  10050/1000000: episode: 53, duration: 0.525s, episode steps: 225, steps per second: 428, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  10183/1000000: episode: 54, duration: 0.272s, episode steps: 133, steps per second: 490, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.421 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  10318/1000000: episode: 55, duration: 0.308s, episode steps: 135, steps per second: 438, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.141 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  10552/1000000: episode: 56, duration: 0.595s, episode steps: 234, steps per second: 393, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  10682/1000000: episode: 57, duration: 0.349s, episode steps: 130, steps per second: 372, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.477 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  10836/1000000: episode: 58, duration: 0.393s, episode steps: 154, steps per second: 392, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  11060/1000000: episode: 59, duration: 0.481s, episode steps: 224, steps per second: 465, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  11197/1000000: episode: 60, duration: 0.285s, episode steps: 137, steps per second: 481, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.438 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  11404/1000000: episode: 61, duration: 0.492s, episode steps: 207, steps per second: 421, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.440 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  11582/1000000: episode: 62, duration: 0.365s, episode steps: 178, steps per second: 487, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  11813/1000000: episode: 63, duration: 0.424s, episode steps: 231, steps per second: 544, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  11970/1000000: episode: 64, duration: 0.309s, episode steps: 157, steps per second: 508, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  12145/1000000: episode: 65, duration: 0.339s, episode steps: 175, steps per second: 516, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.394 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  12304/1000000: episode: 66, duration: 0.337s, episode steps: 159, steps per second: 472, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.629 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  12534/1000000: episode: 67, duration: 0.537s, episode steps: 230, steps per second: 429, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.648 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  12834/1000000: episode: 68, duration: 0.707s, episode steps: 300, steps per second: 424, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.367 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  13027/1000000: episode: 69, duration: 0.506s, episode steps: 193, steps per second: 381, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.632 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  13203/1000000: episode: 70, duration: 0.536s, episode steps: 176, steps per second: 329, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  13380/1000000: episode: 71, duration: 0.619s, episode steps: 177, steps per second: 286, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.542 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  13551/1000000: episode: 72, duration: 0.384s, episode steps: 171, steps per second: 445, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.409 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  13691/1000000: episode: 73, duration: 0.340s, episode steps: 140, steps per second: 412, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.436 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  13817/1000000: episode: 74, duration: 0.263s, episode steps: 126, steps per second: 479, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.516 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  14020/1000000: episode: 75, duration: 0.433s, episode steps: 203, steps per second: 469, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  14188/1000000: episode: 76, duration: 0.363s, episode steps: 168, steps per second: 463, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.440 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  14368/1000000: episode: 77, duration: 0.350s, episode steps: 180, steps per second: 515, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.644 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  14569/1000000: episode: 78, duration: 0.340s, episode steps: 201, steps per second: 591, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.378 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  14826/1000000: episode: 79, duration: 0.531s, episode steps: 257, steps per second: 484, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.479 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  14992/1000000: episode: 80, duration: 0.366s, episode steps: 166, steps per second: 454, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  15222/1000000: episode: 81, duration: 0.511s, episode steps: 230, steps per second: 450, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.461 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  15404/1000000: episode: 82, duration: 0.409s, episode steps: 182, steps per second: 445, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  15537/1000000: episode: 83, duration: 0.296s, episode steps: 133, steps per second: 449, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.654 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  15689/1000000: episode: 84, duration: 0.327s, episode steps: 152, steps per second: 465, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.526 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  15870/1000000: episode: 85, duration: 0.440s, episode steps: 181, steps per second: 411, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.398 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  16054/1000000: episode: 86, duration: 0.372s, episode steps: 184, steps per second: 494, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.473 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  16288/1000000: episode: 87, duration: 0.449s, episode steps: 234, steps per second: 521, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.402 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  16417/1000000: episode: 88, duration: 0.247s, episode steps: 129, steps per second: 522, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  16570/1000000: episode: 89, duration: 0.302s, episode steps: 153, steps per second: 507, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  16752/1000000: episode: 90, duration: 0.382s, episode steps: 182, steps per second: 477, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  16886/1000000: episode: 91, duration: 0.300s, episode steps: 134, steps per second: 447, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.396 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  17022/1000000: episode: 92, duration: 0.360s, episode steps: 136, steps per second: 378, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.676 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  17194/1000000: episode: 93, duration: 0.394s, episode steps: 172, steps per second: 437, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  17413/1000000: episode: 94, duration: 0.448s, episode steps: 219, steps per second: 489, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.516 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  17650/1000000: episode: 95, duration: 0.549s, episode steps: 237, steps per second: 432, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.456 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  17858/1000000: episode: 96, duration: 0.498s, episode steps: 208, steps per second: 418, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  17998/1000000: episode: 97, duration: 0.330s, episode steps: 140, steps per second: 424, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.479 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  18165/1000000: episode: 98, duration: 0.353s, episode steps: 167, steps per second: 473, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.695 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  18296/1000000: episode: 99, duration: 0.287s, episode steps: 131, steps per second: 456, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.527 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  18432/1000000: episode: 100, duration: 0.294s, episode steps: 136, steps per second: 462, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.478 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  18611/1000000: episode: 101, duration: 0.464s, episode steps: 179, steps per second: 386, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.302 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  18745/1000000: episode: 102, duration: 0.309s, episode steps: 134, steps per second: 434, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.493 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  18876/1000000: episode: 103, duration: 0.290s, episode steps: 131, steps per second: 452, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.359 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  19173/1000000: episode: 104, duration: 0.602s, episode steps: 297, steps per second: 493, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.411 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  19369/1000000: episode: 105, duration: 0.496s, episode steps: 196, steps per second: 395, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  19598/1000000: episode: 106, duration: 0.536s, episode steps: 229, steps per second: 427, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.371 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  19912/1000000: episode: 107, duration: 0.764s, episode steps: 314, steps per second: 411, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.516 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  20041/1000000: episode: 108, duration: 0.287s, episode steps: 129, steps per second: 449, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.395 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  20257/1000000: episode: 109, duration: 0.539s, episode steps: 216, steps per second: 400, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.606 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  20436/1000000: episode: 110, duration: 0.355s, episode steps: 179, steps per second: 504, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  20611/1000000: episode: 111, duration: 0.483s, episode steps: 175, steps per second: 362, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.440 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  20795/1000000: episode: 112, duration: 0.427s, episode steps: 184, steps per second: 431, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  20946/1000000: episode: 113, duration: 0.354s, episode steps: 151, steps per second: 427, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.358 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  21177/1000000: episode: 114, duration: 0.539s, episode steps: 231, steps per second: 429, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  21306/1000000: episode: 115, duration: 0.276s, episode steps: 129, steps per second: 468, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.605 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  21480/1000000: episode: 116, duration: 0.403s, episode steps: 174, steps per second: 432, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.552 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  21657/1000000: episode: 117, duration: 0.360s, episode steps: 177, steps per second: 491, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.435 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  21840/1000000: episode: 118, duration: 0.390s, episode steps: 183, steps per second: 469, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.404 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  21980/1000000: episode: 119, duration: 0.266s, episode steps: 140, steps per second: 526, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.407 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  22133/1000000: episode: 120, duration: 0.338s, episode steps: 153, steps per second: 452, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  22286/1000000: episode: 121, duration: 0.300s, episode steps: 153, steps per second: 510, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  22462/1000000: episode: 122, duration: 0.344s, episode steps: 176, steps per second: 512, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.392 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  22684/1000000: episode: 123, duration: 0.452s, episode steps: 222, steps per second: 491, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.446 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  22830/1000000: episode: 124, duration: 0.308s, episode steps: 146, steps per second: 474, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.541 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  23102/1000000: episode: 125, duration: 0.620s, episode steps: 272, steps per second: 439, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  23262/1000000: episode: 126, duration: 0.365s, episode steps: 160, steps per second: 438, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.400 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  23443/1000000: episode: 127, duration: 0.404s, episode steps: 181, steps per second: 448, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.669 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  23650/1000000: episode: 128, duration: 0.430s, episode steps: 207, steps per second: 481, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  23778/1000000: episode: 129, duration: 0.233s, episode steps: 128, steps per second: 549, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.672 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  23934/1000000: episode: 130, duration: 0.308s, episode steps: 156, steps per second: 506, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  24071/1000000: episode: 131, duration: 0.272s, episode steps: 137, steps per second: 503, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.635 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  24253/1000000: episode: 132, duration: 0.341s, episode steps: 182, steps per second: 534, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  24401/1000000: episode: 133, duration: 0.298s, episode steps: 148, steps per second: 497, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.588 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  24646/1000000: episode: 134, duration: 0.529s, episode steps: 245, steps per second: 463, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  24854/1000000: episode: 135, duration: 0.453s, episode steps: 208, steps per second: 460, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.572 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  25108/1000000: episode: 136, duration: 0.520s, episode steps: 254, steps per second: 488, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.555 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  25341/1000000: episode: 137, duration: 0.484s, episode steps: 233, steps per second: 482, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  25548/1000000: episode: 138, duration: 0.439s, episode steps: 207, steps per second: 471, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.348 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  25749/1000000: episode: 139, duration: 0.437s, episode steps: 201, steps per second: 460, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  26174/1000000: episode: 140, duration: 0.890s, episode steps: 425, steps per second: 478, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.438 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  26341/1000000: episode: 141, duration: 0.359s, episode steps: 167, steps per second: 465, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  26542/1000000: episode: 142, duration: 0.446s, episode steps: 201, steps per second: 451, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.542 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  26800/1000000: episode: 143, duration: 0.605s, episode steps: 258, steps per second: 426, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  26996/1000000: episode: 144, duration: 0.451s, episode steps: 196, steps per second: 434, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  27136/1000000: episode: 145, duration: 0.325s, episode steps: 140, steps per second: 430, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  27323/1000000: episode: 146, duration: 0.419s, episode steps: 187, steps per second: 446, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  27478/1000000: episode: 147, duration: 0.404s, episode steps: 155, steps per second: 384, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.594 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  27657/1000000: episode: 148, duration: 0.434s, episode steps: 179, steps per second: 413, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.402 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  27869/1000000: episode: 149, duration: 0.439s, episode steps: 212, steps per second: 483, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  28060/1000000: episode: 150, duration: 0.539s, episode steps: 191, steps per second: 354, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  28272/1000000: episode: 151, duration: 0.399s, episode steps: 212, steps per second: 531, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.377 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  28476/1000000: episode: 152, duration: 0.390s, episode steps: 204, steps per second: 523, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  28617/1000000: episode: 153, duration: 0.288s, episode steps: 141, steps per second: 490, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.539 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  28912/1000000: episode: 154, duration: 0.716s, episode steps: 295, steps per second: 412, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.363 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  29174/1000000: episode: 155, duration: 0.525s, episode steps: 262, steps per second: 499, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.435 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  29357/1000000: episode: 156, duration: 0.436s, episode steps: 183, steps per second: 420, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.475 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  29564/1000000: episode: 157, duration: 0.437s, episode steps: 207, steps per second: 474, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.353 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  29819/1000000: episode: 158, duration: 0.516s, episode steps: 255, steps per second: 494, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.294 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  30154/1000000: episode: 159, duration: 0.732s, episode steps: 335, steps per second: 458, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.537 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  30338/1000000: episode: 160, duration: 0.338s, episode steps: 184, steps per second: 544, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.321 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  30463/1000000: episode: 161, duration: 0.222s, episode steps: 125, steps per second: 564, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.472 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  30593/1000000: episode: 162, duration: 0.290s, episode steps: 130, steps per second: 449, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.515 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  30801/1000000: episode: 163, duration: 0.421s, episode steps: 208, steps per second: 495, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  30960/1000000: episode: 164, duration: 0.356s, episode steps: 159, steps per second: 446, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.409 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  31163/1000000: episode: 165, duration: 0.421s, episode steps: 203, steps per second: 482, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  31373/1000000: episode: 166, duration: 0.456s, episode steps: 210, steps per second: 460, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.457 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  31610/1000000: episode: 167, duration: 0.527s, episode steps: 237, steps per second: 450, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  31899/1000000: episode: 168, duration: 0.641s, episode steps: 289, steps per second: 451, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.474 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  32055/1000000: episode: 169, duration: 0.374s, episode steps: 156, steps per second: 417, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.417 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  32251/1000000: episode: 170, duration: 0.505s, episode steps: 196, steps per second: 388, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  32390/1000000: episode: 171, duration: 0.301s, episode steps: 139, steps per second: 461, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.554 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  32686/1000000: episode: 172, duration: 0.821s, episode steps: 296, steps per second: 360, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.385 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  32819/1000000: episode: 173, duration: 0.422s, episode steps: 133, steps per second: 315, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.278 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  33027/1000000: episode: 174, duration: 0.578s, episode steps: 208, steps per second: 360, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  33152/1000000: episode: 175, duration: 0.329s, episode steps: 125, steps per second: 380, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.424 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  33355/1000000: episode: 176, duration: 0.495s, episode steps: 203, steps per second: 410, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.369 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  33575/1000000: episode: 177, duration: 0.527s, episode steps: 220, steps per second: 418, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  33739/1000000: episode: 178, duration: 0.486s, episode steps: 164, steps per second: 338, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  33875/1000000: episode: 179, duration: 0.415s, episode steps: 136, steps per second: 328, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.610 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  34049/1000000: episode: 180, duration: 0.446s, episode steps: 174, steps per second: 391, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.397 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  34254/1000000: episode: 181, duration: 0.707s, episode steps: 205, steps per second: 290, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  34546/1000000: episode: 182, duration: 0.817s, episode steps: 292, steps per second: 357, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  34761/1000000: episode: 183, duration: 0.671s, episode steps: 215, steps per second: 321, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  34923/1000000: episode: 184, duration: 0.446s, episode steps: 162, steps per second: 364, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  35107/1000000: episode: 185, duration: 0.544s, episode steps: 184, steps per second: 338, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.582 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  35247/1000000: episode: 186, duration: 0.392s, episode steps: 140, steps per second: 357, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.657 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  35500/1000000: episode: 187, duration: 0.645s, episode steps: 253, steps per second: 392, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  35672/1000000: episode: 188, duration: 0.455s, episode steps: 172, steps per second: 378, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.384 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  35872/1000000: episode: 189, duration: 0.494s, episode steps: 200, steps per second: 405, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  36156/1000000: episode: 190, duration: 0.648s, episode steps: 284, steps per second: 438, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.366 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  36340/1000000: episode: 191, duration: 0.424s, episode steps: 184, steps per second: 434, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  36497/1000000: episode: 192, duration: 0.343s, episode steps: 157, steps per second: 457, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.548 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  36709/1000000: episode: 193, duration: 0.504s, episode steps: 212, steps per second: 421, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  36916/1000000: episode: 194, duration: 0.542s, episode steps: 207, steps per second: 382, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.604 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  37141/1000000: episode: 195, duration: 0.591s, episode steps: 225, steps per second: 381, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  37353/1000000: episode: 196, duration: 0.530s, episode steps: 212, steps per second: 400, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.448 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  37536/1000000: episode: 197, duration: 0.482s, episode steps: 183, steps per second: 380, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.393 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  37766/1000000: episode: 198, duration: 0.623s, episode steps: 230, steps per second: 369, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  37946/1000000: episode: 199, duration: 0.527s, episode steps: 180, steps per second: 342, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.389 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  38105/1000000: episode: 200, duration: 0.379s, episode steps: 159, steps per second: 420, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  38287/1000000: episode: 201, duration: 0.389s, episode steps: 182, steps per second: 468, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  38452/1000000: episode: 202, duration: 0.365s, episode steps: 165, steps per second: 452, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  38586/1000000: episode: 203, duration: 0.295s, episode steps: 134, steps per second: 454, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.463 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  38743/1000000: episode: 204, duration: 0.354s, episode steps: 157, steps per second: 444, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.611 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  38953/1000000: episode: 205, duration: 0.484s, episode steps: 210, steps per second: 433, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  39084/1000000: episode: 206, duration: 0.311s, episode steps: 131, steps per second: 421, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.527 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  39273/1000000: episode: 207, duration: 0.431s, episode steps: 189, steps per second: 439, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.339 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  39541/1000000: episode: 208, duration: 0.607s, episode steps: 268, steps per second: 441, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.388 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  39677/1000000: episode: 209, duration: 0.332s, episode steps: 136, steps per second: 410, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.522 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  39821/1000000: episode: 210, duration: 0.356s, episode steps: 144, steps per second: 404, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.479 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  40012/1000000: episode: 211, duration: 0.560s, episode steps: 191, steps per second: 341, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  40142/1000000: episode: 212, duration: 0.366s, episode steps: 130, steps per second: 355, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.392 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  40273/1000000: episode: 213, duration: 0.348s, episode steps: 131, steps per second: 377, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.534 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  40495/1000000: episode: 214, duration: 0.517s, episode steps: 222, steps per second: 430, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.446 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  40626/1000000: episode: 215, duration: 0.341s, episode steps: 131, steps per second: 384, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.511 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  40764/1000000: episode: 216, duration: 0.372s, episode steps: 138, steps per second: 371, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.493 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  40967/1000000: episode: 217, duration: 0.490s, episode steps: 203, steps per second: 414, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.468 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  41095/1000000: episode: 218, duration: 0.329s, episode steps: 128, steps per second: 389, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.422 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  41334/1000000: episode: 219, duration: 0.587s, episode steps: 239, steps per second: 407, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  41497/1000000: episode: 220, duration: 0.418s, episode steps: 163, steps per second: 390, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  41678/1000000: episode: 221, duration: 0.537s, episode steps: 181, steps per second: 337, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.354 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  41857/1000000: episode: 222, duration: 0.447s, episode steps: 179, steps per second: 400, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.447 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  42032/1000000: episode: 223, duration: 0.441s, episode steps: 175, steps per second: 397, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.377 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  42189/1000000: episode: 224, duration: 0.394s, episode steps: 157, steps per second: 399, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.478 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  42429/1000000: episode: 225, duration: 0.559s, episode steps: 240, steps per second: 429, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.596 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  42556/1000000: episode: 226, duration: 0.308s, episode steps: 127, steps per second: 412, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.449 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  42691/1000000: episode: 227, duration: 0.398s, episode steps: 135, steps per second: 339, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.437 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  42906/1000000: episode: 228, duration: 0.540s, episode steps: 215, steps per second: 398, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.479 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  43034/1000000: episode: 229, duration: 0.315s, episode steps: 128, steps per second: 407, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.477 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  43262/1000000: episode: 230, duration: 0.557s, episode steps: 228, steps per second: 409, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  43533/1000000: episode: 231, duration: 0.628s, episode steps: 271, steps per second: 432, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.402 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  43663/1000000: episode: 232, duration: 0.320s, episode steps: 130, steps per second: 406, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.354 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  43843/1000000: episode: 233, duration: 0.467s, episode steps: 180, steps per second: 385, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  44001/1000000: episode: 234, duration: 0.397s, episode steps: 158, steps per second: 398, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  44138/1000000: episode: 235, duration: 0.352s, episode steps: 137, steps per second: 389, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.708 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  44322/1000000: episode: 236, duration: 0.481s, episode steps: 184, steps per second: 383, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.620 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  44552/1000000: episode: 237, duration: 0.664s, episode steps: 230, steps per second: 346, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  44681/1000000: episode: 238, duration: 0.386s, episode steps: 129, steps per second: 334, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.426 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  44837/1000000: episode: 239, duration: 0.452s, episode steps: 156, steps per second: 345, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  45051/1000000: episode: 240, duration: 0.609s, episode steps: 214, steps per second: 351, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  45183/1000000: episode: 241, duration: 0.385s, episode steps: 132, steps per second: 343, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.447 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  45384/1000000: episode: 242, duration: 0.534s, episode steps: 201, steps per second: 376, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  45598/1000000: episode: 243, duration: 0.577s, episode steps: 214, steps per second: 371, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  45812/1000000: episode: 244, duration: 0.581s, episode steps: 214, steps per second: 368, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  45976/1000000: episode: 245, duration: 0.449s, episode steps: 164, steps per second: 365, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.372 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  46156/1000000: episode: 246, duration: 0.517s, episode steps: 180, steps per second: 348, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.356 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  46362/1000000: episode: 247, duration: 0.508s, episode steps: 206, steps per second: 406, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.369 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  46498/1000000: episode: 248, duration: 0.340s, episode steps: 136, steps per second: 401, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.434 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  46826/1000000: episode: 249, duration: 0.800s, episode steps: 328, steps per second: 410, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  47004/1000000: episode: 250, duration: 0.465s, episode steps: 178, steps per second: 383, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.393 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  47140/1000000: episode: 251, duration: 0.371s, episode steps: 136, steps per second: 367, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.404 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  47347/1000000: episode: 252, duration: 0.674s, episode steps: 207, steps per second: 307, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.454 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  47542/1000000: episode: 253, duration: 0.643s, episode steps: 195, steps per second: 303, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.410 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  47766/1000000: episode: 254, duration: 0.642s, episode steps: 224, steps per second: 349, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  47976/1000000: episode: 255, duration: 0.522s, episode steps: 210, steps per second: 402, episode reward:  3.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.695 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  48338/1000000: episode: 256, duration: 0.902s, episode steps: 362, steps per second: 401, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.467 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  48546/1000000: episode: 257, duration: 0.481s, episode steps: 208, steps per second: 432, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  48747/1000000: episode: 258, duration: 0.489s, episode steps: 201, steps per second: 411, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.423 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  48942/1000000: episode: 259, duration: 0.510s, episode steps: 195, steps per second: 383, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.497 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  49102/1000000: episode: 260, duration: 0.463s, episode steps: 160, steps per second: 346, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  49344/1000000: episode: 261, duration: 0.754s, episode steps: 242, steps per second: 321, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  49501/1000000: episode: 262, duration: 0.497s, episode steps: 157, steps per second: 316, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.643 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  49736/1000000: episode: 263, duration: 0.760s, episode steps: 235, steps per second: 309, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  49865/1000000: episode: 264, duration: 0.467s, episode steps: 129, steps per second: 276, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.450 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33626\\anaconda3\\envs\\deep\\lib\\site-packages\\keras\\src\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  50098/1000000: episode: 265, duration: 2.511s, episode steps: 233, steps per second:  93, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: 0.007167, mae: 0.073081, mean_q: 0.118706, mean_eps: 0.954955\n",
      "  50286/1000000: episode: 266, duration: 3.020s, episode steps: 188, steps per second:  62, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.002414, mae: 0.058720, mean_q: 0.087936, mean_eps: 0.954827\n",
      "  50466/1000000: episode: 267, duration: 2.936s, episode steps: 180, steps per second:  61, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.004943, mae: 0.064342, mean_q: 0.090163, mean_eps: 0.954662\n",
      "  50641/1000000: episode: 268, duration: 2.771s, episode steps: 175, steps per second:  63, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.343 [0.000, 3.000],  loss: 0.001834, mae: 0.058926, mean_q: 0.085381, mean_eps: 0.954501\n",
      "  50768/1000000: episode: 269, duration: 1.995s, episode steps: 127, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.528 [0.000, 3.000],  loss: 0.004663, mae: 0.061780, mean_q: 0.083403, mean_eps: 0.954366\n",
      "  50999/1000000: episode: 270, duration: 3.939s, episode steps: 231, steps per second:  59, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.005485, mae: 0.067008, mean_q: 0.095155, mean_eps: 0.954206\n",
      "  51155/1000000: episode: 271, duration: 2.639s, episode steps: 156, steps per second:  59, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.385 [0.000, 3.000],  loss: 0.002847, mae: 0.059400, mean_q: 0.090171, mean_eps: 0.954032\n",
      "  51292/1000000: episode: 272, duration: 2.127s, episode steps: 137, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.394 [0.000, 3.000],  loss: 0.005075, mae: 0.063344, mean_q: 0.086082, mean_eps: 0.953900\n",
      "  51428/1000000: episode: 273, duration: 2.149s, episode steps: 136, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.449 [0.000, 3.000],  loss: 0.005997, mae: 0.065378, mean_q: 0.093221, mean_eps: 0.953778\n",
      "  51612/1000000: episode: 274, duration: 2.923s, episode steps: 184, steps per second:  63, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.408 [0.000, 3.000],  loss: 0.003793, mae: 0.063225, mean_q: 0.089877, mean_eps: 0.953634\n",
      "  51755/1000000: episode: 275, duration: 2.235s, episode steps: 143, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.434 [0.000, 3.000],  loss: 0.002214, mae: 0.058594, mean_q: 0.081323, mean_eps: 0.953486\n",
      "  51894/1000000: episode: 276, duration: 2.332s, episode steps: 139, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.568 [0.000, 3.000],  loss: 0.004491, mae: 0.061772, mean_q: 0.082831, mean_eps: 0.953358\n",
      "  52034/1000000: episode: 277, duration: 2.618s, episode steps: 140, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.436 [0.000, 3.000],  loss: 0.003982, mae: 0.063949, mean_q: 0.087207, mean_eps: 0.953232\n",
      "  52350/1000000: episode: 278, duration: 5.037s, episode steps: 316, steps per second:  63, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.449 [0.000, 3.000],  loss: 0.003411, mae: 0.061433, mean_q: 0.085994, mean_eps: 0.953027\n",
      "  52494/1000000: episode: 279, duration: 2.225s, episode steps: 144, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.632 [0.000, 3.000],  loss: 0.003177, mae: 0.061357, mean_q: 0.087704, mean_eps: 0.952820\n",
      "  52625/1000000: episode: 280, duration: 1.927s, episode steps: 131, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.725 [0.000, 3.000],  loss: 0.003255, mae: 0.064703, mean_q: 0.095963, mean_eps: 0.952696\n",
      "  52781/1000000: episode: 281, duration: 2.328s, episode steps: 156, steps per second:  67, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.391 [0.000, 3.000],  loss: 0.002609, mae: 0.061454, mean_q: 0.085103, mean_eps: 0.952566\n",
      "  52930/1000000: episode: 282, duration: 2.419s, episode steps: 149, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.262 [0.000, 3.000],  loss: 0.002837, mae: 0.061462, mean_q: 0.085572, mean_eps: 0.952430\n",
      "  53119/1000000: episode: 283, duration: 3.266s, episode steps: 189, steps per second:  58, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 0.001771, mae: 0.061319, mean_q: 0.084955, mean_eps: 0.952278\n",
      "  53258/1000000: episode: 284, duration: 2.071s, episode steps: 139, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.770 [0.000, 3.000],  loss: 0.002873, mae: 0.061722, mean_q: 0.085965, mean_eps: 0.952131\n",
      "  53433/1000000: episode: 285, duration: 2.658s, episode steps: 175, steps per second:  66, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: 0.001643, mae: 0.059691, mean_q: 0.086388, mean_eps: 0.951989\n",
      "  53604/1000000: episode: 286, duration: 2.582s, episode steps: 171, steps per second:  66, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 0.001893, mae: 0.060329, mean_q: 0.082206, mean_eps: 0.951834\n",
      "  53734/1000000: episode: 287, duration: 2.007s, episode steps: 130, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.546 [0.000, 3.000],  loss: 0.004225, mae: 0.064567, mean_q: 0.085038, mean_eps: 0.951699\n",
      "  54017/1000000: episode: 288, duration: 4.541s, episode steps: 283, steps per second:  62, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 0.003083, mae: 0.064460, mean_q: 0.087707, mean_eps: 0.951512\n",
      "  54191/1000000: episode: 289, duration: 2.852s, episode steps: 174, steps per second:  61, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.002000, mae: 0.061440, mean_q: 0.084744, mean_eps: 0.951306\n",
      "  54355/1000000: episode: 290, duration: 2.473s, episode steps: 164, steps per second:  66, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.555 [0.000, 3.000],  loss: 0.002111, mae: 0.058949, mean_q: 0.080848, mean_eps: 0.951155\n",
      "  54555/1000000: episode: 291, duration: 2.936s, episode steps: 200, steps per second:  68, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001534, mae: 0.062527, mean_q: 0.083100, mean_eps: 0.950991\n",
      "  54866/1000000: episode: 292, duration: 4.848s, episode steps: 311, steps per second:  64, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 0.001322, mae: 0.059179, mean_q: 0.080578, mean_eps: 0.950761\n",
      "  55042/1000000: episode: 293, duration: 3.319s, episode steps: 176, steps per second:  53, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 0.001195, mae: 0.060003, mean_q: 0.080534, mean_eps: 0.950541\n",
      "  55287/1000000: episode: 294, duration: 4.828s, episode steps: 245, steps per second:  51, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.682 [0.000, 3.000],  loss: 0.000552, mae: 0.059432, mean_q: 0.080499, mean_eps: 0.950352\n",
      "  55446/1000000: episode: 295, duration: 3.487s, episode steps: 159, steps per second:  46, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.428 [0.000, 3.000],  loss: 0.001866, mae: 0.063944, mean_q: 0.086593, mean_eps: 0.950171\n",
      "  55648/1000000: episode: 296, duration: 4.043s, episode steps: 202, steps per second:  50, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.376 [0.000, 3.000],  loss: 0.001281, mae: 0.058237, mean_q: 0.079645, mean_eps: 0.950009\n",
      "  55829/1000000: episode: 297, duration: 3.427s, episode steps: 181, steps per second:  53, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.000739, mae: 0.057627, mean_q: 0.078415, mean_eps: 0.949836\n",
      "  55986/1000000: episode: 298, duration: 2.301s, episode steps: 157, steps per second:  68, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.631 [0.000, 3.000],  loss: 0.000470, mae: 0.058936, mean_q: 0.079810, mean_eps: 0.949683\n",
      "  56233/1000000: episode: 299, duration: 3.440s, episode steps: 247, steps per second:  72, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.356 [0.000, 3.000],  loss: 0.001835, mae: 0.062032, mean_q: 0.082967, mean_eps: 0.949501\n",
      "  56403/1000000: episode: 300, duration: 2.031s, episode steps: 170, steps per second:  84, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.741 [0.000, 3.000],  loss: 0.001693, mae: 0.059005, mean_q: 0.078842, mean_eps: 0.949314\n",
      "  56571/1000000: episode: 301, duration: 2.360s, episode steps: 168, steps per second:  71, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 0.002631, mae: 0.063666, mean_q: 0.085763, mean_eps: 0.949163\n",
      "  56760/1000000: episode: 302, duration: 2.507s, episode steps: 189, steps per second:  75, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 0.000915, mae: 0.060626, mean_q: 0.081903, mean_eps: 0.949002\n",
      "  56943/1000000: episode: 303, duration: 2.739s, episode steps: 183, steps per second:  67, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: 0.001384, mae: 0.059464, mean_q: 0.081514, mean_eps: 0.948835\n",
      "  57100/1000000: episode: 304, duration: 2.288s, episode steps: 157, steps per second:  69, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.001259, mae: 0.060834, mean_q: 0.082647, mean_eps: 0.948682\n",
      "  57309/1000000: episode: 305, duration: 2.994s, episode steps: 209, steps per second:  70, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 0.001109, mae: 0.061025, mean_q: 0.081948, mean_eps: 0.948516\n",
      "  57562/1000000: episode: 306, duration: 3.383s, episode steps: 253, steps per second:  75, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: 0.002282, mae: 0.061747, mean_q: 0.084421, mean_eps: 0.948308\n",
      "  57697/1000000: episode: 307, duration: 1.803s, episode steps: 135, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.556 [0.000, 3.000],  loss: 0.001136, mae: 0.058611, mean_q: 0.082214, mean_eps: 0.948133\n",
      "  57880/1000000: episode: 308, duration: 2.425s, episode steps: 183, steps per second:  75, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.000611, mae: 0.060269, mean_q: 0.081792, mean_eps: 0.947991\n",
      "  58166/1000000: episode: 309, duration: 4.193s, episode steps: 286, steps per second:  68, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.552 [0.000, 3.000],  loss: 0.001080, mae: 0.059595, mean_q: 0.081474, mean_eps: 0.947780\n",
      "  58393/1000000: episode: 310, duration: 3.260s, episode steps: 227, steps per second:  70, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: 0.000428, mae: 0.057416, mean_q: 0.077448, mean_eps: 0.947548\n",
      "  58625/1000000: episode: 311, duration: 2.975s, episode steps: 232, steps per second:  78, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 0.001632, mae: 0.063535, mean_q: 0.087470, mean_eps: 0.947341\n",
      "  58786/1000000: episode: 312, duration: 2.099s, episode steps: 161, steps per second:  77, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 0.000948, mae: 0.058971, mean_q: 0.080012, mean_eps: 0.947165\n",
      "  58963/1000000: episode: 313, duration: 2.512s, episode steps: 177, steps per second:  70, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.429 [0.000, 3.000],  loss: 0.001064, mae: 0.060808, mean_q: 0.082958, mean_eps: 0.947013\n",
      "  59261/1000000: episode: 314, duration: 4.158s, episode steps: 298, steps per second:  72, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 0.001330, mae: 0.060932, mean_q: 0.082828, mean_eps: 0.946799\n",
      "  59392/1000000: episode: 315, duration: 1.977s, episode steps: 131, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.435 [0.000, 3.000],  loss: 0.001418, mae: 0.062041, mean_q: 0.085168, mean_eps: 0.946607\n",
      "  59529/1000000: episode: 316, duration: 1.968s, episode steps: 137, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.693 [0.000, 3.000],  loss: 0.001508, mae: 0.057488, mean_q: 0.078074, mean_eps: 0.946486\n",
      "  59795/1000000: episode: 317, duration: 3.299s, episode steps: 266, steps per second:  81, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.000321, mae: 0.057189, mean_q: 0.077127, mean_eps: 0.946304\n",
      "  60089/1000000: episode: 318, duration: 3.940s, episode steps: 294, steps per second:  75, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: 0.002167, mae: 0.062403, mean_q: 0.084190, mean_eps: 0.946052\n",
      "  60269/1000000: episode: 319, duration: 2.537s, episode steps: 180, steps per second:  71, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 0.003280, mae: 0.062844, mean_q: 0.083046, mean_eps: 0.945838\n",
      "  60453/1000000: episode: 320, duration: 3.068s, episode steps: 184, steps per second:  60, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: 0.002094, mae: 0.066412, mean_q: 0.088814, mean_eps: 0.945674\n",
      "  60592/1000000: episode: 321, duration: 2.337s, episode steps: 139, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.374 [0.000, 3.000],  loss: 0.003643, mae: 0.068602, mean_q: 0.089742, mean_eps: 0.945530\n",
      "  60806/1000000: episode: 322, duration: 3.173s, episode steps: 214, steps per second:  67, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.388 [0.000, 3.000],  loss: 0.002332, mae: 0.063122, mean_q: 0.085103, mean_eps: 0.945372\n",
      "  60961/1000000: episode: 323, duration: 2.428s, episode steps: 155, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.001152, mae: 0.065294, mean_q: 0.090461, mean_eps: 0.945204\n",
      "  61136/1000000: episode: 324, duration: 2.347s, episode steps: 175, steps per second:  75, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 0.003186, mae: 0.072179, mean_q: 0.097450, mean_eps: 0.945057\n",
      "  61422/1000000: episode: 325, duration: 4.733s, episode steps: 286, steps per second:  60, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: 0.001752, mae: 0.064999, mean_q: 0.087500, mean_eps: 0.944850\n",
      "  61562/1000000: episode: 326, duration: 2.311s, episode steps: 140, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.536 [0.000, 3.000],  loss: 0.002256, mae: 0.065408, mean_q: 0.087879, mean_eps: 0.944657\n",
      "  61699/1000000: episode: 327, duration: 2.713s, episode steps: 137, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.445 [0.000, 3.000],  loss: 0.001017, mae: 0.063937, mean_q: 0.086384, mean_eps: 0.944533\n",
      "  61859/1000000: episode: 328, duration: 2.903s, episode steps: 160, steps per second:  55, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.001531, mae: 0.062030, mean_q: 0.082897, mean_eps: 0.944400\n",
      "  61992/1000000: episode: 329, duration: 1.828s, episode steps: 133, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.293 [0.000, 3.000],  loss: 0.001574, mae: 0.066055, mean_q: 0.088267, mean_eps: 0.944268\n",
      "  62139/1000000: episode: 330, duration: 2.324s, episode steps: 147, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.612 [0.000, 3.000],  loss: 0.000732, mae: 0.062196, mean_q: 0.084766, mean_eps: 0.944142\n",
      "  62272/1000000: episode: 331, duration: 2.514s, episode steps: 133, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.489 [0.000, 3.000],  loss: 0.000931, mae: 0.063516, mean_q: 0.085690, mean_eps: 0.944016\n",
      "  62476/1000000: episode: 332, duration: 4.455s, episode steps: 204, steps per second:  46, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.426 [0.000, 3.000],  loss: 0.001174, mae: 0.064530, mean_q: 0.087775, mean_eps: 0.943865\n",
      "  62696/1000000: episode: 333, duration: 2.876s, episode steps: 220, steps per second:  76, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: 0.001107, mae: 0.061672, mean_q: 0.084405, mean_eps: 0.943674\n",
      "  62923/1000000: episode: 334, duration: 3.011s, episode steps: 227, steps per second:  75, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.419 [0.000, 3.000],  loss: 0.001976, mae: 0.067713, mean_q: 0.091080, mean_eps: 0.943473\n",
      "  63183/1000000: episode: 335, duration: 4.403s, episode steps: 260, steps per second:  59, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.000940, mae: 0.065715, mean_q: 0.089827, mean_eps: 0.943253\n",
      "  63403/1000000: episode: 336, duration: 4.097s, episode steps: 220, steps per second:  54, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 0.001698, mae: 0.068173, mean_q: 0.092617, mean_eps: 0.943037\n",
      "  63540/1000000: episode: 337, duration: 2.355s, episode steps: 137, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.387 [0.000, 3.000],  loss: 0.001326, mae: 0.071144, mean_q: 0.097018, mean_eps: 0.942877\n",
      "  63744/1000000: episode: 338, duration: 2.864s, episode steps: 204, steps per second:  71, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.407 [0.000, 3.000],  loss: 0.001483, mae: 0.066146, mean_q: 0.090542, mean_eps: 0.942724\n",
      "  63967/1000000: episode: 339, duration: 3.069s, episode steps: 223, steps per second:  73, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.516 [0.000, 3.000],  loss: 0.000967, mae: 0.064886, mean_q: 0.090691, mean_eps: 0.942531\n",
      "  64148/1000000: episode: 340, duration: 2.528s, episode steps: 181, steps per second:  72, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.442 [0.000, 3.000],  loss: 0.000783, mae: 0.063205, mean_q: 0.086033, mean_eps: 0.942350\n",
      "  64285/1000000: episode: 341, duration: 2.194s, episode steps: 137, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.000542, mae: 0.067338, mean_q: 0.091654, mean_eps: 0.942206\n",
      "  64573/1000000: episode: 342, duration: 4.026s, episode steps: 288, steps per second:  72, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 0.000878, mae: 0.062602, mean_q: 0.084734, mean_eps: 0.942013\n",
      "  64790/1000000: episode: 343, duration: 2.822s, episode steps: 217, steps per second:  77, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.336 [0.000, 3.000],  loss: 0.000808, mae: 0.064816, mean_q: 0.088570, mean_eps: 0.941786\n",
      "  64996/1000000: episode: 344, duration: 2.824s, episode steps: 206, steps per second:  73, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 0.001823, mae: 0.065060, mean_q: 0.090382, mean_eps: 0.941597\n",
      "  65151/1000000: episode: 345, duration: 2.043s, episode steps: 155, steps per second:  76, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.000367, mae: 0.062891, mean_q: 0.086092, mean_eps: 0.941435\n",
      "  65302/1000000: episode: 346, duration: 2.216s, episode steps: 151, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.596 [0.000, 3.000],  loss: 0.000562, mae: 0.063181, mean_q: 0.085983, mean_eps: 0.941297\n",
      "  65460/1000000: episode: 347, duration: 2.461s, episode steps: 158, steps per second:  64, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 0.000560, mae: 0.060010, mean_q: 0.081082, mean_eps: 0.941158\n",
      "  65620/1000000: episode: 348, duration: 2.596s, episode steps: 160, steps per second:  62, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.456 [0.000, 3.000],  loss: 0.000732, mae: 0.063319, mean_q: 0.086383, mean_eps: 0.941016\n",
      "  65800/1000000: episode: 349, duration: 2.497s, episode steps: 180, steps per second:  72, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 0.000473, mae: 0.061225, mean_q: 0.083780, mean_eps: 0.940863\n",
      "  65941/1000000: episode: 350, duration: 1.895s, episode steps: 141, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.447 [0.000, 3.000],  loss: 0.000232, mae: 0.058023, mean_q: 0.078382, mean_eps: 0.940717\n",
      "  66182/1000000: episode: 351, duration: 3.065s, episode steps: 241, steps per second:  79, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.635 [0.000, 3.000],  loss: 0.001789, mae: 0.066979, mean_q: 0.090362, mean_eps: 0.940544\n",
      "  66391/1000000: episode: 352, duration: 3.746s, episode steps: 209, steps per second:  56, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.001634, mae: 0.067274, mean_q: 0.092436, mean_eps: 0.940343\n",
      "  66560/1000000: episode: 353, duration: 3.503s, episode steps: 169, steps per second:  48, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.000922, mae: 0.063219, mean_q: 0.086325, mean_eps: 0.940173\n",
      "  66770/1000000: episode: 354, duration: 3.189s, episode steps: 210, steps per second:  66, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000964, mae: 0.062118, mean_q: 0.084330, mean_eps: 0.940002\n",
      "  66908/1000000: episode: 355, duration: 1.974s, episode steps: 138, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.326 [0.000, 3.000],  loss: 0.000903, mae: 0.067209, mean_q: 0.090625, mean_eps: 0.939846\n",
      "  67081/1000000: episode: 356, duration: 2.498s, episode steps: 173, steps per second:  69, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 0.000560, mae: 0.062450, mean_q: 0.085429, mean_eps: 0.939705\n",
      "  67268/1000000: episode: 357, duration: 2.857s, episode steps: 187, steps per second:  65, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.428 [0.000, 3.000],  loss: 0.000919, mae: 0.065365, mean_q: 0.087940, mean_eps: 0.939543\n",
      "  67494/1000000: episode: 358, duration: 3.141s, episode steps: 226, steps per second:  72, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: 0.000955, mae: 0.065892, mean_q: 0.089964, mean_eps: 0.939358\n",
      "  67717/1000000: episode: 359, duration: 3.189s, episode steps: 223, steps per second:  70, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.318 [0.000, 3.000],  loss: 0.000977, mae: 0.060151, mean_q: 0.081848, mean_eps: 0.939155\n",
      "  67862/1000000: episode: 360, duration: 2.094s, episode steps: 145, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.379 [0.000, 3.000],  loss: 0.000863, mae: 0.066398, mean_q: 0.089352, mean_eps: 0.938989\n",
      "  68069/1000000: episode: 361, duration: 2.763s, episode steps: 207, steps per second:  75, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: 0.000909, mae: 0.064271, mean_q: 0.088686, mean_eps: 0.938831\n",
      "  68287/1000000: episode: 362, duration: 3.423s, episode steps: 218, steps per second:  64, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.482 [0.000, 3.000],  loss: 0.000823, mae: 0.065924, mean_q: 0.089017, mean_eps: 0.938640\n",
      "  68469/1000000: episode: 363, duration: 3.593s, episode steps: 182, steps per second:  51, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.000973, mae: 0.066630, mean_q: 0.089725, mean_eps: 0.938460\n",
      "  68626/1000000: episode: 364, duration: 3.099s, episode steps: 157, steps per second:  51, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.471 [0.000, 3.000],  loss: 0.000956, mae: 0.062303, mean_q: 0.085150, mean_eps: 0.938307\n",
      "  68843/1000000: episode: 365, duration: 4.316s, episode steps: 217, steps per second:  50, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.001217, mae: 0.065287, mean_q: 0.090077, mean_eps: 0.938139\n",
      "  69098/1000000: episode: 366, duration: 4.946s, episode steps: 255, steps per second:  52, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.000989, mae: 0.067128, mean_q: 0.090381, mean_eps: 0.937927\n",
      "  69382/1000000: episode: 367, duration: 5.406s, episode steps: 284, steps per second:  53, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: 0.000771, mae: 0.062132, mean_q: 0.084261, mean_eps: 0.937684\n",
      "  69563/1000000: episode: 368, duration: 3.782s, episode steps: 181, steps per second:  48, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 0.000842, mae: 0.063856, mean_q: 0.086465, mean_eps: 0.937475\n",
      "  69784/1000000: episode: 369, duration: 4.189s, episode steps: 221, steps per second:  53, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.548 [0.000, 3.000],  loss: 0.001032, mae: 0.065747, mean_q: 0.089583, mean_eps: 0.937295\n",
      "  69998/1000000: episode: 370, duration: 4.462s, episode steps: 214, steps per second:  48, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 0.000603, mae: 0.064207, mean_q: 0.087228, mean_eps: 0.937099\n",
      "  70206/1000000: episode: 371, duration: 3.133s, episode steps: 208, steps per second:  66, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.322 [0.000, 3.000],  loss: 0.002448, mae: 0.072011, mean_q: 0.096136, mean_eps: 0.936908\n",
      "  70395/1000000: episode: 372, duration: 2.772s, episode steps: 189, steps per second:  68, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.418 [0.000, 3.000],  loss: 0.002511, mae: 0.075907, mean_q: 0.100592, mean_eps: 0.936730\n",
      "  70556/1000000: episode: 373, duration: 2.227s, episode steps: 161, steps per second:  72, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.001141, mae: 0.067329, mean_q: 0.089781, mean_eps: 0.936573\n",
      "  70709/1000000: episode: 374, duration: 2.262s, episode steps: 153, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.386 [0.000, 3.000],  loss: 0.001371, mae: 0.067672, mean_q: 0.090981, mean_eps: 0.936431\n",
      "  71190/1000000: episode: 375, duration: 6.463s, episode steps: 481, steps per second:  74, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.468 [0.000, 3.000],  loss: 0.000856, mae: 0.066094, mean_q: 0.088724, mean_eps: 0.936145\n",
      "  71326/1000000: episode: 376, duration: 1.729s, episode steps: 136, steps per second:  79, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.456 [0.000, 3.000],  loss: 0.001077, mae: 0.067619, mean_q: 0.091216, mean_eps: 0.935868\n",
      "  71462/1000000: episode: 377, duration: 2.111s, episode steps: 136, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.346 [0.000, 3.000],  loss: 0.000833, mae: 0.067151, mean_q: 0.090832, mean_eps: 0.935745\n",
      "  71587/1000000: episode: 378, duration: 1.826s, episode steps: 125, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.464 [0.000, 3.000],  loss: 0.000582, mae: 0.068929, mean_q: 0.094454, mean_eps: 0.935628\n",
      "  71769/1000000: episode: 379, duration: 2.417s, episode steps: 182, steps per second:  75, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: 0.001165, mae: 0.074296, mean_q: 0.100437, mean_eps: 0.935490\n",
      "  71931/1000000: episode: 380, duration: 2.220s, episode steps: 162, steps per second:  73, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 0.000392, mae: 0.070708, mean_q: 0.095982, mean_eps: 0.935335\n",
      "  72116/1000000: episode: 381, duration: 2.374s, episode steps: 185, steps per second:  78, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: 0.000576, mae: 0.070304, mean_q: 0.095150, mean_eps: 0.935180\n",
      "  72254/1000000: episode: 382, duration: 1.907s, episode steps: 138, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.464 [0.000, 3.000],  loss: 0.000328, mae: 0.068607, mean_q: 0.091993, mean_eps: 0.935034\n",
      "  72380/1000000: episode: 383, duration: 1.927s, episode steps: 126, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.524 [0.000, 3.000],  loss: 0.000991, mae: 0.070925, mean_q: 0.095119, mean_eps: 0.934916\n",
      "  72586/1000000: episode: 384, duration: 3.106s, episode steps: 206, steps per second:  66, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 0.000615, mae: 0.069465, mean_q: 0.094850, mean_eps: 0.934766\n",
      "  72715/1000000: episode: 385, duration: 1.847s, episode steps: 129, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.558 [0.000, 3.000],  loss: 0.000468, mae: 0.069845, mean_q: 0.094594, mean_eps: 0.934615\n",
      "  72947/1000000: episode: 386, duration: 3.087s, episode steps: 232, steps per second:  75, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: 0.000460, mae: 0.069022, mean_q: 0.092880, mean_eps: 0.934453\n",
      "  73158/1000000: episode: 387, duration: 3.065s, episode steps: 211, steps per second:  69, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.441 [0.000, 3.000],  loss: 0.000977, mae: 0.067613, mean_q: 0.090381, mean_eps: 0.934253\n",
      "  73353/1000000: episode: 388, duration: 2.712s, episode steps: 195, steps per second:  72, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.000555, mae: 0.071956, mean_q: 0.097002, mean_eps: 0.934070\n",
      "  73525/1000000: episode: 389, duration: 2.432s, episode steps: 172, steps per second:  71, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.384 [0.000, 3.000],  loss: 0.000251, mae: 0.064363, mean_q: 0.086838, mean_eps: 0.933904\n",
      "  73733/1000000: episode: 390, duration: 3.326s, episode steps: 208, steps per second:  63, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.548 [0.000, 3.000],  loss: 0.000397, mae: 0.067277, mean_q: 0.090088, mean_eps: 0.933733\n",
      "  73901/1000000: episode: 391, duration: 2.541s, episode steps: 168, steps per second:  66, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.000573, mae: 0.073202, mean_q: 0.099126, mean_eps: 0.933564\n",
      "  74057/1000000: episode: 392, duration: 2.062s, episode steps: 156, steps per second:  76, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.000657, mae: 0.071897, mean_q: 0.097296, mean_eps: 0.933418\n",
      "  74253/1000000: episode: 393, duration: 2.633s, episode steps: 196, steps per second:  74, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.429 [0.000, 3.000],  loss: 0.000492, mae: 0.062621, mean_q: 0.084389, mean_eps: 0.933260\n",
      "  74541/1000000: episode: 394, duration: 4.168s, episode steps: 288, steps per second:  69, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: 0.000595, mae: 0.067471, mean_q: 0.090779, mean_eps: 0.933042\n",
      "  74680/1000000: episode: 395, duration: 2.028s, episode steps: 139, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.338 [0.000, 3.000],  loss: 0.000316, mae: 0.066126, mean_q: 0.089673, mean_eps: 0.932851\n",
      "  74809/1000000: episode: 396, duration: 1.935s, episode steps: 129, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.574 [0.000, 3.000],  loss: 0.000714, mae: 0.068369, mean_q: 0.092124, mean_eps: 0.932730\n",
      "  75077/1000000: episode: 397, duration: 3.821s, episode steps: 268, steps per second:  70, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.638 [0.000, 3.000],  loss: 0.000776, mae: 0.071316, mean_q: 0.094912, mean_eps: 0.932550\n",
      "  75291/1000000: episode: 398, duration: 2.786s, episode steps: 214, steps per second:  77, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.458 [0.000, 3.000],  loss: 0.000511, mae: 0.068943, mean_q: 0.093294, mean_eps: 0.932334\n",
      "  75455/1000000: episode: 399, duration: 2.268s, episode steps: 164, steps per second:  72, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.445 [0.000, 3.000],  loss: 0.000870, mae: 0.073081, mean_q: 0.099184, mean_eps: 0.932165\n",
      "  75595/1000000: episode: 400, duration: 1.916s, episode steps: 140, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000530, mae: 0.064228, mean_q: 0.089292, mean_eps: 0.932028\n",
      "  75900/1000000: episode: 401, duration: 4.318s, episode steps: 305, steps per second:  71, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000340, mae: 0.066427, mean_q: 0.089748, mean_eps: 0.931829\n",
      "  76112/1000000: episode: 402, duration: 3.113s, episode steps: 212, steps per second:  68, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.000525, mae: 0.069358, mean_q: 0.093622, mean_eps: 0.931596\n",
      "  76316/1000000: episode: 403, duration: 2.847s, episode steps: 204, steps per second:  72, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 0.000448, mae: 0.072571, mean_q: 0.097558, mean_eps: 0.931409\n",
      "  76484/1000000: episode: 404, duration: 2.444s, episode steps: 168, steps per second:  69, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.000407, mae: 0.066774, mean_q: 0.090466, mean_eps: 0.931242\n",
      "  76625/1000000: episode: 405, duration: 1.980s, episode steps: 141, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.553 [0.000, 3.000],  loss: 0.000236, mae: 0.066269, mean_q: 0.088650, mean_eps: 0.931101\n",
      "  76764/1000000: episode: 406, duration: 1.999s, episode steps: 139, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.597 [0.000, 3.000],  loss: 0.000256, mae: 0.064546, mean_q: 0.086873, mean_eps: 0.930975\n",
      "  77027/1000000: episode: 407, duration: 3.932s, episode steps: 263, steps per second:  67, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: 0.000480, mae: 0.071013, mean_q: 0.095094, mean_eps: 0.930795\n",
      "  77166/1000000: episode: 408, duration: 2.231s, episode steps: 139, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.590 [0.000, 3.000],  loss: 0.000155, mae: 0.071783, mean_q: 0.097176, mean_eps: 0.930614\n",
      "  77371/1000000: episode: 409, duration: 2.965s, episode steps: 205, steps per second:  69, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.000370, mae: 0.066830, mean_q: 0.090161, mean_eps: 0.930459\n",
      "  77528/1000000: episode: 410, duration: 2.223s, episode steps: 157, steps per second:  71, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.000594, mae: 0.068537, mean_q: 0.092457, mean_eps: 0.930297\n",
      "  77733/1000000: episode: 411, duration: 2.942s, episode steps: 205, steps per second:  70, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 0.000469, mae: 0.068881, mean_q: 0.092811, mean_eps: 0.930133\n",
      "  77934/1000000: episode: 412, duration: 2.803s, episode steps: 201, steps per second:  72, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: 0.000547, mae: 0.069282, mean_q: 0.093529, mean_eps: 0.929949\n",
      "  78190/1000000: episode: 413, duration: 5.295s, episode steps: 256, steps per second:  48, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 0.000439, mae: 0.070530, mean_q: 0.096789, mean_eps: 0.929744\n",
      "  78415/1000000: episode: 414, duration: 3.948s, episode steps: 225, steps per second:  57, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.000180, mae: 0.068766, mean_q: 0.092604, mean_eps: 0.929528\n",
      "  78627/1000000: episode: 415, duration: 3.211s, episode steps: 212, steps per second:  66, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 0.000190, mae: 0.071228, mean_q: 0.095577, mean_eps: 0.929332\n",
      "  78840/1000000: episode: 416, duration: 3.213s, episode steps: 213, steps per second:  66, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.352 [0.000, 3.000],  loss: 0.000461, mae: 0.068549, mean_q: 0.092619, mean_eps: 0.929141\n",
      "  79012/1000000: episode: 417, duration: 2.759s, episode steps: 172, steps per second:  62, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: 0.000185, mae: 0.070723, mean_q: 0.094680, mean_eps: 0.928968\n",
      "  79152/1000000: episode: 418, duration: 2.140s, episode steps: 140, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.336 [0.000, 3.000],  loss: 0.000135, mae: 0.067629, mean_q: 0.090879, mean_eps: 0.928828\n",
      "  79303/1000000: episode: 419, duration: 2.243s, episode steps: 151, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.762 [0.000, 3.000],  loss: 0.000208, mae: 0.066184, mean_q: 0.089272, mean_eps: 0.928697\n",
      "  79434/1000000: episode: 420, duration: 1.796s, episode steps: 131, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.588 [0.000, 3.000],  loss: 0.000143, mae: 0.067353, mean_q: 0.090369, mean_eps: 0.928569\n",
      "  79686/1000000: episode: 421, duration: 3.597s, episode steps: 252, steps per second:  70, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.456 [0.000, 3.000],  loss: 0.000373, mae: 0.069296, mean_q: 0.093529, mean_eps: 0.928396\n",
      "  79823/1000000: episode: 422, duration: 1.919s, episode steps: 137, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.540 [0.000, 3.000],  loss: 0.000255, mae: 0.065703, mean_q: 0.087963, mean_eps: 0.928221\n",
      "  80013/1000000: episode: 423, duration: 2.699s, episode steps: 190, steps per second:  70, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.001156, mae: 0.073735, mean_q: 0.098785, mean_eps: 0.928074\n",
      "  80153/1000000: episode: 424, duration: 2.148s, episode steps: 140, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.002478, mae: 0.077226, mean_q: 0.102828, mean_eps: 0.927924\n",
      "  80338/1000000: episode: 425, duration: 2.723s, episode steps: 185, steps per second:  68, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 0.001974, mae: 0.083562, mean_q: 0.114119, mean_eps: 0.927779\n",
      "  80519/1000000: episode: 426, duration: 2.518s, episode steps: 181, steps per second:  72, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 0.001519, mae: 0.080389, mean_q: 0.107507, mean_eps: 0.927615\n",
      "  80651/1000000: episode: 427, duration: 1.752s, episode steps: 132, steps per second:  75, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.001204, mae: 0.083171, mean_q: 0.111391, mean_eps: 0.927474\n",
      "  80782/1000000: episode: 428, duration: 1.865s, episode steps: 131, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.702 [0.000, 3.000],  loss: 0.001889, mae: 0.087722, mean_q: 0.117475, mean_eps: 0.927356\n",
      "  81126/1000000: episode: 429, duration: 4.895s, episode steps: 344, steps per second:  70, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.000987, mae: 0.078998, mean_q: 0.106461, mean_eps: 0.927141\n",
      "  81413/1000000: episode: 430, duration: 4.438s, episode steps: 287, steps per second:  65, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: 0.000828, mae: 0.081151, mean_q: 0.109130, mean_eps: 0.926857\n",
      "  81539/1000000: episode: 431, duration: 1.955s, episode steps: 126, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.405 [0.000, 3.000],  loss: 0.000572, mae: 0.065928, mean_q: 0.088940, mean_eps: 0.926672\n",
      "  81681/1000000: episode: 432, duration: 2.343s, episode steps: 142, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.655 [0.000, 3.000],  loss: 0.001088, mae: 0.079457, mean_q: 0.109585, mean_eps: 0.926551\n",
      "  81806/1000000: episode: 433, duration: 2.069s, episode steps: 125, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.472 [0.000, 3.000],  loss: 0.000850, mae: 0.077061, mean_q: 0.104389, mean_eps: 0.926430\n",
      "  81932/1000000: episode: 434, duration: 1.762s, episode steps: 126, steps per second:  72, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.627 [0.000, 3.000],  loss: 0.000559, mae: 0.074534, mean_q: 0.100445, mean_eps: 0.926319\n",
      "  82116/1000000: episode: 435, duration: 2.756s, episode steps: 184, steps per second:  67, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.000314, mae: 0.072789, mean_q: 0.097253, mean_eps: 0.926180\n",
      "  82333/1000000: episode: 436, duration: 3.855s, episode steps: 217, steps per second:  56, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 0.000364, mae: 0.075058, mean_q: 0.100851, mean_eps: 0.925998\n",
      "  82474/1000000: episode: 437, duration: 2.427s, episode steps: 141, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.489 [0.000, 3.000],  loss: 0.000266, mae: 0.074788, mean_q: 0.100699, mean_eps: 0.925836\n",
      "  82610/1000000: episode: 438, duration: 2.081s, episode steps: 136, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.632 [0.000, 3.000],  loss: 0.000335, mae: 0.078427, mean_q: 0.105411, mean_eps: 0.925712\n",
      "  82761/1000000: episode: 439, duration: 2.221s, episode steps: 151, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.450 [0.000, 3.000],  loss: 0.000630, mae: 0.077270, mean_q: 0.104272, mean_eps: 0.925583\n",
      "  82928/1000000: episode: 440, duration: 2.456s, episode steps: 167, steps per second:  68, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.000282, mae: 0.075421, mean_q: 0.101613, mean_eps: 0.925440\n",
      "  83164/1000000: episode: 441, duration: 3.194s, episode steps: 236, steps per second:  74, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.000857, mae: 0.071549, mean_q: 0.095401, mean_eps: 0.925260\n",
      "  83297/1000000: episode: 442, duration: 2.006s, episode steps: 133, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.586 [0.000, 3.000],  loss: 0.000506, mae: 0.074219, mean_q: 0.098915, mean_eps: 0.925093\n",
      "  83454/1000000: episode: 443, duration: 2.349s, episode steps: 157, steps per second:  67, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.000519, mae: 0.079594, mean_q: 0.107672, mean_eps: 0.924962\n",
      "  83615/1000000: episode: 444, duration: 2.212s, episode steps: 161, steps per second:  73, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.590 [0.000, 3.000],  loss: 0.000402, mae: 0.073254, mean_q: 0.098152, mean_eps: 0.924819\n",
      "  83751/1000000: episode: 445, duration: 2.016s, episode steps: 136, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.640 [0.000, 3.000],  loss: 0.000548, mae: 0.069815, mean_q: 0.094808, mean_eps: 0.924686\n",
      "  83918/1000000: episode: 446, duration: 2.470s, episode steps: 167, steps per second:  68, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.431 [0.000, 3.000],  loss: 0.000158, mae: 0.073903, mean_q: 0.099400, mean_eps: 0.924549\n",
      "  84122/1000000: episode: 447, duration: 2.703s, episode steps: 204, steps per second:  75, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 0.000571, mae: 0.075592, mean_q: 0.102607, mean_eps: 0.924382\n",
      "  84363/1000000: episode: 448, duration: 3.148s, episode steps: 241, steps per second:  77, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.461 [0.000, 3.000],  loss: 0.000350, mae: 0.075409, mean_q: 0.101542, mean_eps: 0.924182\n",
      "  84591/1000000: episode: 449, duration: 3.451s, episode steps: 228, steps per second:  66, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: 0.000424, mae: 0.072232, mean_q: 0.097139, mean_eps: 0.923972\n",
      "  84917/1000000: episode: 450, duration: 4.560s, episode steps: 326, steps per second:  71, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.457 [0.000, 3.000],  loss: 0.000664, mae: 0.078109, mean_q: 0.105129, mean_eps: 0.923721\n",
      "  85122/1000000: episode: 451, duration: 2.864s, episode steps: 205, steps per second:  72, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 0.000363, mae: 0.071539, mean_q: 0.095453, mean_eps: 0.923482\n",
      "  85337/1000000: episode: 452, duration: 2.972s, episode steps: 215, steps per second:  72, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.367 [0.000, 3.000],  loss: 0.000234, mae: 0.074205, mean_q: 0.099633, mean_eps: 0.923293\n",
      "  85499/1000000: episode: 453, duration: 2.085s, episode steps: 162, steps per second:  78, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.438 [0.000, 3.000],  loss: 0.000120, mae: 0.069402, mean_q: 0.093626, mean_eps: 0.923124\n",
      "  85644/1000000: episode: 454, duration: 2.393s, episode steps: 145, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.641 [0.000, 3.000],  loss: 0.000142, mae: 0.067043, mean_q: 0.090886, mean_eps: 0.922987\n",
      "  85843/1000000: episode: 455, duration: 2.800s, episode steps: 199, steps per second:  71, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.000239, mae: 0.081426, mean_q: 0.109665, mean_eps: 0.922832\n",
      "  85973/1000000: episode: 456, duration: 1.696s, episode steps: 130, steps per second:  77, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.515 [0.000, 3.000],  loss: 0.000270, mae: 0.077159, mean_q: 0.103076, mean_eps: 0.922683\n",
      "  86183/1000000: episode: 457, duration: 3.610s, episode steps: 210, steps per second:  58, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.000239, mae: 0.072252, mean_q: 0.097158, mean_eps: 0.922530\n",
      "  86372/1000000: episode: 458, duration: 3.418s, episode steps: 189, steps per second:  55, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 0.000714, mae: 0.079189, mean_q: 0.106055, mean_eps: 0.922352\n",
      "  86597/1000000: episode: 459, duration: 4.763s, episode steps: 225, steps per second:  47, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 0.000341, mae: 0.078525, mean_q: 0.107071, mean_eps: 0.922164\n",
      "  86726/1000000: episode: 460, duration: 2.103s, episode steps: 129, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.326 [0.000, 3.000],  loss: 0.000178, mae: 0.082132, mean_q: 0.111022, mean_eps: 0.922004\n",
      "  86862/1000000: episode: 461, duration: 2.298s, episode steps: 136, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000483, mae: 0.075235, mean_q: 0.100856, mean_eps: 0.921885\n",
      "  87105/1000000: episode: 462, duration: 4.211s, episode steps: 243, steps per second:  58, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: 0.000157, mae: 0.072600, mean_q: 0.097766, mean_eps: 0.921714\n",
      "  87303/1000000: episode: 463, duration: 3.878s, episode steps: 198, steps per second:  51, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: 0.001008, mae: 0.076947, mean_q: 0.103790, mean_eps: 0.921516\n",
      "  87527/1000000: episode: 464, duration: 4.054s, episode steps: 224, steps per second:  55, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.000301, mae: 0.077088, mean_q: 0.103649, mean_eps: 0.921327\n",
      "  87704/1000000: episode: 465, duration: 2.963s, episode steps: 177, steps per second:  60, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.000335, mae: 0.073735, mean_q: 0.098368, mean_eps: 0.921147\n",
      "  87892/1000000: episode: 466, duration: 2.672s, episode steps: 188, steps per second:  70, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.394 [0.000, 3.000],  loss: 0.000170, mae: 0.073429, mean_q: 0.098432, mean_eps: 0.920984\n",
      "  88094/1000000: episode: 467, duration: 3.257s, episode steps: 202, steps per second:  62, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: 0.000190, mae: 0.071862, mean_q: 0.096065, mean_eps: 0.920807\n",
      "  88326/1000000: episode: 468, duration: 3.170s, episode steps: 232, steps per second:  73, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 0.000375, mae: 0.073801, mean_q: 0.099456, mean_eps: 0.920611\n",
      "  88462/1000000: episode: 469, duration: 2.085s, episode steps: 136, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.515 [0.000, 3.000],  loss: 0.000157, mae: 0.070444, mean_q: 0.094689, mean_eps: 0.920445\n",
      "  88616/1000000: episode: 470, duration: 2.328s, episode steps: 154, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.305 [0.000, 3.000],  loss: 0.000260, mae: 0.074156, mean_q: 0.099071, mean_eps: 0.920316\n",
      "  88786/1000000: episode: 471, duration: 2.389s, episode steps: 170, steps per second:  71, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.453 [0.000, 3.000],  loss: 0.000223, mae: 0.076486, mean_q: 0.102461, mean_eps: 0.920170\n",
      "  89104/1000000: episode: 472, duration: 4.648s, episode steps: 318, steps per second:  68, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.000264, mae: 0.070113, mean_q: 0.094361, mean_eps: 0.919950\n",
      "  89260/1000000: episode: 473, duration: 2.276s, episode steps: 156, steps per second:  69, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 0.000135, mae: 0.072218, mean_q: 0.097382, mean_eps: 0.919738\n",
      "  89463/1000000: episode: 474, duration: 2.711s, episode steps: 203, steps per second:  75, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: 0.000501, mae: 0.076113, mean_q: 0.102612, mean_eps: 0.919576\n",
      "  89717/1000000: episode: 475, duration: 3.916s, episode steps: 254, steps per second:  65, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 0.000194, mae: 0.076736, mean_q: 0.103203, mean_eps: 0.919369\n",
      "  89875/1000000: episode: 476, duration: 2.359s, episode steps: 158, steps per second:  67, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.000159, mae: 0.072133, mean_q: 0.096884, mean_eps: 0.919184\n",
      "  90147/1000000: episode: 477, duration: 3.926s, episode steps: 272, steps per second:  69, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.537 [0.000, 3.000],  loss: 0.001362, mae: 0.078998, mean_q: 0.106003, mean_eps: 0.918991\n",
      "  90308/1000000: episode: 478, duration: 2.251s, episode steps: 161, steps per second:  72, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 0.002096, mae: 0.082709, mean_q: 0.110971, mean_eps: 0.918797\n",
      "  90472/1000000: episode: 479, duration: 2.365s, episode steps: 164, steps per second:  69, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.457 [0.000, 3.000],  loss: 0.001478, mae: 0.081265, mean_q: 0.109453, mean_eps: 0.918651\n",
      "  90598/1000000: episode: 480, duration: 1.718s, episode steps: 126, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.001214, mae: 0.078711, mean_q: 0.107619, mean_eps: 0.918519\n",
      "  90794/1000000: episode: 481, duration: 3.514s, episode steps: 196, steps per second:  56, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.449 [0.000, 3.000],  loss: 0.001408, mae: 0.088382, mean_q: 0.119044, mean_eps: 0.918374\n",
      "  90998/1000000: episode: 482, duration: 3.406s, episode steps: 204, steps per second:  60, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.417 [0.000, 3.000],  loss: 0.000850, mae: 0.077394, mean_q: 0.107218, mean_eps: 0.918194\n",
      "  91180/1000000: episode: 483, duration: 3.083s, episode steps: 182, steps per second:  59, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 0.000819, mae: 0.080846, mean_q: 0.107785, mean_eps: 0.918021\n",
      "  91355/1000000: episode: 484, duration: 2.734s, episode steps: 175, steps per second:  64, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.000441, mae: 0.075251, mean_q: 0.101108, mean_eps: 0.917861\n",
      "  91489/1000000: episode: 485, duration: 2.254s, episode steps: 134, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.537 [0.000, 3.000],  loss: 0.000674, mae: 0.075537, mean_q: 0.101478, mean_eps: 0.917720\n",
      "  91756/1000000: episode: 486, duration: 4.524s, episode steps: 267, steps per second:  59, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.000577, mae: 0.075522, mean_q: 0.101705, mean_eps: 0.917540\n",
      "  91885/1000000: episode: 487, duration: 2.142s, episode steps: 129, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.504 [0.000, 3.000],  loss: 0.000662, mae: 0.082253, mean_q: 0.111089, mean_eps: 0.917362\n",
      "  92013/1000000: episode: 488, duration: 1.989s, episode steps: 128, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.484 [0.000, 3.000],  loss: 0.001042, mae: 0.083622, mean_q: 0.111858, mean_eps: 0.917245\n",
      "  92191/1000000: episode: 489, duration: 2.862s, episode steps: 178, steps per second:  62, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.000641, mae: 0.074038, mean_q: 0.099025, mean_eps: 0.917108\n",
      "  92417/1000000: episode: 490, duration: 3.772s, episode steps: 226, steps per second:  60, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 0.000784, mae: 0.080496, mean_q: 0.109275, mean_eps: 0.916926\n",
      "  92582/1000000: episode: 491, duration: 2.592s, episode steps: 165, steps per second:  64, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.000641, mae: 0.085977, mean_q: 0.116768, mean_eps: 0.916750\n",
      "  92973/1000000: episode: 492, duration: 7.707s, episode steps: 391, steps per second:  51, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.000379, mae: 0.077184, mean_q: 0.103776, mean_eps: 0.916500\n",
      "  93183/1000000: episode: 493, duration: 3.737s, episode steps: 210, steps per second:  56, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: 0.000447, mae: 0.076925, mean_q: 0.103457, mean_eps: 0.916230\n",
      "  93398/1000000: episode: 494, duration: 3.838s, episode steps: 215, steps per second:  56, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 0.000807, mae: 0.082539, mean_q: 0.110446, mean_eps: 0.916039\n",
      "  93654/1000000: episode: 495, duration: 4.832s, episode steps: 256, steps per second:  53, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.691 [0.000, 3.000],  loss: 0.000554, mae: 0.081330, mean_q: 0.110233, mean_eps: 0.915827\n",
      "  93792/1000000: episode: 496, duration: 2.478s, episode steps: 138, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.565 [0.000, 3.000],  loss: 0.000552, mae: 0.082244, mean_q: 0.110372, mean_eps: 0.915650\n",
      "  93930/1000000: episode: 497, duration: 2.102s, episode steps: 138, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000277, mae: 0.072897, mean_q: 0.098612, mean_eps: 0.915526\n",
      "  94132/1000000: episode: 498, duration: 3.155s, episode steps: 202, steps per second:  64, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.480 [0.000, 3.000],  loss: 0.000236, mae: 0.071818, mean_q: 0.096548, mean_eps: 0.915373\n",
      "  94287/1000000: episode: 499, duration: 2.109s, episode steps: 155, steps per second:  73, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.387 [0.000, 3.000],  loss: 0.000809, mae: 0.073880, mean_q: 0.099884, mean_eps: 0.915213\n",
      "  94411/1000000: episode: 500, duration: 1.702s, episode steps: 124, steps per second:  73, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.597 [0.000, 3.000],  loss: 0.000290, mae: 0.078627, mean_q: 0.106355, mean_eps: 0.915087\n",
      "  94561/1000000: episode: 501, duration: 2.328s, episode steps: 150, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.467 [0.000, 3.000],  loss: 0.000420, mae: 0.081946, mean_q: 0.109806, mean_eps: 0.914963\n",
      "  94690/1000000: episode: 502, duration: 1.838s, episode steps: 129, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.543 [0.000, 3.000],  loss: 0.000325, mae: 0.076607, mean_q: 0.103757, mean_eps: 0.914837\n",
      "  94907/1000000: episode: 503, duration: 3.146s, episode steps: 217, steps per second:  69, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.000595, mae: 0.078075, mean_q: 0.105426, mean_eps: 0.914682\n",
      "  95114/1000000: episode: 504, duration: 3.219s, episode steps: 207, steps per second:  64, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 0.000323, mae: 0.076829, mean_q: 0.103222, mean_eps: 0.914491\n",
      "  95261/1000000: episode: 505, duration: 2.098s, episode steps: 147, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.000694, mae: 0.078951, mean_q: 0.106233, mean_eps: 0.914331\n",
      "  95456/1000000: episode: 506, duration: 2.626s, episode steps: 195, steps per second:  74, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.000454, mae: 0.077830, mean_q: 0.105069, mean_eps: 0.914178\n",
      "  95710/1000000: episode: 507, duration: 4.343s, episode steps: 254, steps per second:  58, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 0.000227, mae: 0.077130, mean_q: 0.103732, mean_eps: 0.913976\n",
      "  95890/1000000: episode: 508, duration: 2.344s, episode steps: 180, steps per second:  77, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.000209, mae: 0.076834, mean_q: 0.102923, mean_eps: 0.913780\n",
      "  96025/1000000: episode: 509, duration: 1.939s, episode steps: 135, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.563 [0.000, 3.000],  loss: 0.000127, mae: 0.075615, mean_q: 0.101500, mean_eps: 0.913638\n",
      "  96198/1000000: episode: 510, duration: 2.534s, episode steps: 173, steps per second:  68, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.335 [0.000, 3.000],  loss: 0.000294, mae: 0.075319, mean_q: 0.101354, mean_eps: 0.913499\n",
      "  96389/1000000: episode: 511, duration: 2.738s, episode steps: 191, steps per second:  70, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.366 [0.000, 3.000],  loss: 0.000204, mae: 0.076684, mean_q: 0.103187, mean_eps: 0.913335\n",
      "  96515/1000000: episode: 512, duration: 1.664s, episode steps: 126, steps per second:  76, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.587 [0.000, 3.000],  loss: 0.000141, mae: 0.078842, mean_q: 0.105888, mean_eps: 0.913193\n",
      "  96656/1000000: episode: 513, duration: 2.286s, episode steps: 141, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.716 [0.000, 3.000],  loss: 0.000235, mae: 0.087113, mean_q: 0.116280, mean_eps: 0.913074\n",
      "  96847/1000000: episode: 514, duration: 3.078s, episode steps: 191, steps per second:  62, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.414 [0.000, 3.000],  loss: 0.000310, mae: 0.072542, mean_q: 0.096319, mean_eps: 0.912925\n",
      "  97056/1000000: episode: 515, duration: 3.001s, episode steps: 209, steps per second:  70, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.435 [0.000, 3.000],  loss: 0.000125, mae: 0.073000, mean_q: 0.098247, mean_eps: 0.912745\n",
      "  97187/1000000: episode: 516, duration: 1.884s, episode steps: 131, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.382 [0.000, 3.000],  loss: 0.000399, mae: 0.076215, mean_q: 0.101862, mean_eps: 0.912592\n",
      "  97350/1000000: episode: 517, duration: 2.232s, episode steps: 163, steps per second:  73, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.000380, mae: 0.074274, mean_q: 0.100380, mean_eps: 0.912459\n",
      "  97488/1000000: episode: 518, duration: 2.006s, episode steps: 138, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.529 [0.000, 3.000],  loss: 0.000179, mae: 0.080765, mean_q: 0.108101, mean_eps: 0.912324\n",
      "  97703/1000000: episode: 519, duration: 3.503s, episode steps: 215, steps per second:  61, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.767 [0.000, 3.000],  loss: 0.000259, mae: 0.078725, mean_q: 0.105822, mean_eps: 0.912165\n",
      "  97838/1000000: episode: 520, duration: 2.209s, episode steps: 135, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.526 [0.000, 3.000],  loss: 0.000328, mae: 0.079197, mean_q: 0.106520, mean_eps: 0.912007\n",
      "  97977/1000000: episode: 521, duration: 2.007s, episode steps: 139, steps per second:  69, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.619 [0.000, 3.000],  loss: 0.000111, mae: 0.073155, mean_q: 0.098524, mean_eps: 0.911883\n",
      "  98249/1000000: episode: 522, duration: 3.673s, episode steps: 272, steps per second:  74, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 0.000232, mae: 0.074999, mean_q: 0.101053, mean_eps: 0.911697\n",
      "  98465/1000000: episode: 523, duration: 2.953s, episode steps: 216, steps per second:  73, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.449 [0.000, 3.000],  loss: 0.000284, mae: 0.081905, mean_q: 0.109759, mean_eps: 0.911478\n",
      "  98612/1000000: episode: 524, duration: 2.091s, episode steps: 147, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.000309, mae: 0.080122, mean_q: 0.107095, mean_eps: 0.911316\n",
      "  98753/1000000: episode: 525, duration: 2.023s, episode steps: 141, steps per second:  70, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.546 [0.000, 3.000],  loss: 0.000266, mae: 0.068985, mean_q: 0.092213, mean_eps: 0.911186\n",
      "  99066/1000000: episode: 526, duration: 4.703s, episode steps: 313, steps per second:  67, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: 0.000300, mae: 0.079241, mean_q: 0.106043, mean_eps: 0.910981\n",
      "  99319/1000000: episode: 527, duration: 3.394s, episode steps: 253, steps per second:  75, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.482 [0.000, 3.000],  loss: 0.000212, mae: 0.079185, mean_q: 0.105987, mean_eps: 0.910727\n",
      "  99549/1000000: episode: 528, duration: 3.407s, episode steps: 230, steps per second:  68, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000215, mae: 0.079011, mean_q: 0.105868, mean_eps: 0.910509\n",
      "  99756/1000000: episode: 529, duration: 3.662s, episode steps: 207, steps per second:  57, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.377 [0.000, 3.000],  loss: 0.000416, mae: 0.077026, mean_q: 0.103384, mean_eps: 0.910313\n",
      "  99886/1000000: episode: 530, duration: 2.488s, episode steps: 130, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.469 [0.000, 3.000],  loss: 0.000259, mae: 0.077889, mean_q: 0.104643, mean_eps: 0.910162\n",
      " 100091/1000000: episode: 531, duration: 3.697s, episode steps: 205, steps per second:  55, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 0.000855, mae: 0.076850, mean_q: 0.102543, mean_eps: 0.910011\n",
      " 100228/1000000: episode: 532, duration: 2.615s, episode steps: 137, steps per second:  52, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.628 [0.000, 3.000],  loss: 0.001749, mae: 0.094914, mean_q: 0.128768, mean_eps: 0.909858\n",
      " 100396/1000000: episode: 533, duration: 3.153s, episode steps: 168, steps per second:  53, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.001931, mae: 0.084195, mean_q: 0.112555, mean_eps: 0.909721\n",
      " 100646/1000000: episode: 534, duration: 4.624s, episode steps: 250, steps per second:  54, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 0.001330, mae: 0.089836, mean_q: 0.120619, mean_eps: 0.909532\n",
      " 100948/1000000: episode: 535, duration: 5.119s, episode steps: 302, steps per second:  59, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.001070, mae: 0.087796, mean_q: 0.117405, mean_eps: 0.909284\n",
      " 101109/1000000: episode: 536, duration: 2.256s, episode steps: 161, steps per second:  71, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.422 [0.000, 3.000],  loss: 0.000582, mae: 0.082845, mean_q: 0.112373, mean_eps: 0.909075\n",
      " 101312/1000000: episode: 537, duration: 3.145s, episode steps: 203, steps per second:  65, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 0.000804, mae: 0.085421, mean_q: 0.114469, mean_eps: 0.908911\n",
      " 101490/1000000: episode: 538, duration: 2.596s, episode steps: 178, steps per second:  69, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 0.001077, mae: 0.089261, mean_q: 0.120780, mean_eps: 0.908740\n",
      " 101625/1000000: episode: 539, duration: 1.993s, episode steps: 135, steps per second:  68, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.341 [0.000, 3.000],  loss: 0.000584, mae: 0.072609, mean_q: 0.098593, mean_eps: 0.908598\n",
      " 101764/1000000: episode: 540, duration: 2.191s, episode steps: 139, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.460 [0.000, 3.000],  loss: 0.000483, mae: 0.081837, mean_q: 0.110973, mean_eps: 0.908475\n",
      " 101912/1000000: episode: 541, duration: 2.506s, episode steps: 148, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.682 [0.000, 3.000],  loss: 0.000739, mae: 0.087447, mean_q: 0.117963, mean_eps: 0.908348\n",
      " 102047/1000000: episode: 542, duration: 2.126s, episode steps: 135, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.681 [0.000, 3.000],  loss: 0.000860, mae: 0.090449, mean_q: 0.123063, mean_eps: 0.908220\n",
      " 102276/1000000: episode: 543, duration: 2.946s, episode steps: 229, steps per second:  78, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.341 [0.000, 3.000],  loss: 0.000694, mae: 0.087616, mean_q: 0.119269, mean_eps: 0.908056\n",
      " 102538/1000000: episode: 544, duration: 3.948s, episode steps: 262, steps per second:  66, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 0.000745, mae: 0.088632, mean_q: 0.119087, mean_eps: 0.907835\n",
      " 102754/1000000: episode: 545, duration: 3.014s, episode steps: 216, steps per second:  72, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.602 [0.000, 3.000],  loss: 0.000348, mae: 0.085685, mean_q: 0.116187, mean_eps: 0.907619\n",
      " 102904/1000000: episode: 546, duration: 2.341s, episode steps: 150, steps per second:  64, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.547 [0.000, 3.000],  loss: 0.000747, mae: 0.089351, mean_q: 0.123225, mean_eps: 0.907455\n",
      " 103034/1000000: episode: 547, duration: 2.014s, episode steps: 130, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.254 [0.000, 3.000],  loss: 0.000301, mae: 0.078942, mean_q: 0.105740, mean_eps: 0.907329\n",
      " 103344/1000000: episode: 548, duration: 4.105s, episode steps: 310, steps per second:  76, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.435 [0.000, 3.000],  loss: 0.000281, mae: 0.081713, mean_q: 0.109920, mean_eps: 0.907131\n",
      " 103554/1000000: episode: 549, duration: 2.918s, episode steps: 210, steps per second:  72, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.000490, mae: 0.085619, mean_q: 0.114731, mean_eps: 0.906897\n",
      " 103736/1000000: episode: 550, duration: 2.556s, episode steps: 182, steps per second:  71, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.709 [0.000, 3.000],  loss: 0.000489, mae: 0.079795, mean_q: 0.106474, mean_eps: 0.906720\n",
      " 103921/1000000: episode: 551, duration: 2.834s, episode steps: 185, steps per second:  65, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.389 [0.000, 3.000],  loss: 0.000870, mae: 0.086435, mean_q: 0.116090, mean_eps: 0.906555\n",
      " 104049/1000000: episode: 552, duration: 1.972s, episode steps: 128, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000257, mae: 0.082523, mean_q: 0.110782, mean_eps: 0.906413\n",
      " 104207/1000000: episode: 553, duration: 2.385s, episode steps: 158, steps per second:  66, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.411 [0.000, 3.000],  loss: 0.000532, mae: 0.078982, mean_q: 0.105824, mean_eps: 0.906285\n",
      " 104356/1000000: episode: 554, duration: 2.258s, episode steps: 149, steps per second:  66, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.470 [0.000, 3.000],  loss: 0.000522, mae: 0.090590, mean_q: 0.123161, mean_eps: 0.906148\n",
      " 104541/1000000: episode: 555, duration: 2.679s, episode steps: 185, steps per second:  69, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.000256, mae: 0.087746, mean_q: 0.117891, mean_eps: 0.905997\n",
      " 104757/1000000: episode: 556, duration: 3.340s, episode steps: 216, steps per second:  65, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.292 [0.000, 3.000],  loss: 0.000306, mae: 0.082082, mean_q: 0.109971, mean_eps: 0.905815\n",
      " 104904/1000000: episode: 557, duration: 2.641s, episode steps: 147, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.537 [0.000, 3.000],  loss: 0.000653, mae: 0.088879, mean_q: 0.120232, mean_eps: 0.905653\n",
      " 105114/1000000: episode: 558, duration: 4.142s, episode steps: 210, steps per second:  51, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.000384, mae: 0.082762, mean_q: 0.111389, mean_eps: 0.905493\n",
      " 105367/1000000: episode: 559, duration: 4.705s, episode steps: 253, steps per second:  54, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.395 [0.000, 3.000],  loss: 0.000410, mae: 0.088912, mean_q: 0.118933, mean_eps: 0.905284\n",
      " 105498/1000000: episode: 560, duration: 2.419s, episode steps: 131, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.511 [0.000, 3.000],  loss: 0.000434, mae: 0.088661, mean_q: 0.118996, mean_eps: 0.905111\n",
      " 105645/1000000: episode: 561, duration: 2.742s, episode steps: 147, steps per second:  54, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: 0.000370, mae: 0.086728, mean_q: 0.115599, mean_eps: 0.904985\n",
      " 105909/1000000: episode: 562, duration: 5.027s, episode steps: 264, steps per second:  53, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.000289, mae: 0.092425, mean_q: 0.123517, mean_eps: 0.904800\n",
      " 106069/1000000: episode: 563, duration: 2.922s, episode steps: 160, steps per second:  55, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000150, mae: 0.080902, mean_q: 0.108400, mean_eps: 0.904609\n",
      " 106271/1000000: episode: 564, duration: 3.690s, episode steps: 202, steps per second:  55, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.411 [0.000, 3.000],  loss: 0.000354, mae: 0.084429, mean_q: 0.112843, mean_eps: 0.904447\n",
      " 106539/1000000: episode: 565, duration: 4.669s, episode steps: 268, steps per second:  57, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.410 [0.000, 3.000],  loss: 0.000235, mae: 0.080832, mean_q: 0.108441, mean_eps: 0.904236\n",
      " 106682/1000000: episode: 566, duration: 2.348s, episode steps: 143, steps per second:  61, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.706 [0.000, 3.000],  loss: 0.000250, mae: 0.082868, mean_q: 0.111748, mean_eps: 0.904051\n",
      " 106851/1000000: episode: 567, duration: 3.207s, episode steps: 169, steps per second:  53, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.473 [0.000, 3.000],  loss: 0.000724, mae: 0.083499, mean_q: 0.112641, mean_eps: 0.903911\n",
      " 107004/1000000: episode: 568, duration: 2.572s, episode steps: 153, steps per second:  59, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.634 [0.000, 3.000],  loss: 0.000233, mae: 0.081560, mean_q: 0.109646, mean_eps: 0.903767\n",
      " 107272/1000000: episode: 569, duration: 4.319s, episode steps: 268, steps per second:  62, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.000264, mae: 0.081519, mean_q: 0.109033, mean_eps: 0.903578\n",
      " 107460/1000000: episode: 570, duration: 3.166s, episode steps: 188, steps per second:  59, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.000427, mae: 0.092603, mean_q: 0.124337, mean_eps: 0.903372\n",
      " 107586/1000000: episode: 571, duration: 2.040s, episode steps: 126, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.690 [0.000, 3.000],  loss: 0.000605, mae: 0.090210, mean_q: 0.120925, mean_eps: 0.903230\n",
      " 107719/1000000: episode: 572, duration: 2.221s, episode steps: 133, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.602 [0.000, 3.000],  loss: 0.000296, mae: 0.083955, mean_q: 0.111328, mean_eps: 0.903113\n",
      " 107849/1000000: episode: 573, duration: 2.328s, episode steps: 130, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.331 [0.000, 3.000],  loss: 0.000168, mae: 0.086276, mean_q: 0.115833, mean_eps: 0.902994\n",
      " 107979/1000000: episode: 574, duration: 2.065s, episode steps: 130, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.715 [0.000, 3.000],  loss: 0.000317, mae: 0.074064, mean_q: 0.098897, mean_eps: 0.902877\n",
      " 108124/1000000: episode: 575, duration: 2.325s, episode steps: 145, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.572 [0.000, 3.000],  loss: 0.000227, mae: 0.087192, mean_q: 0.117910, mean_eps: 0.902755\n",
      " 108311/1000000: episode: 576, duration: 3.088s, episode steps: 187, steps per second:  61, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.299 [0.000, 3.000],  loss: 0.000292, mae: 0.080238, mean_q: 0.107666, mean_eps: 0.902606\n",
      " 108478/1000000: episode: 577, duration: 2.697s, episode steps: 167, steps per second:  62, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.629 [0.000, 3.000],  loss: 0.000226, mae: 0.090616, mean_q: 0.121848, mean_eps: 0.902445\n",
      " 108623/1000000: episode: 578, duration: 2.509s, episode steps: 145, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.517 [0.000, 3.000],  loss: 0.000208, mae: 0.081875, mean_q: 0.109930, mean_eps: 0.902305\n",
      " 108799/1000000: episode: 579, duration: 3.088s, episode steps: 176, steps per second:  57, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.000091, mae: 0.076662, mean_q: 0.102626, mean_eps: 0.902161\n",
      " 109065/1000000: episode: 580, duration: 4.516s, episode steps: 266, steps per second:  59, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.673 [0.000, 3.000],  loss: 0.000261, mae: 0.078980, mean_q: 0.105447, mean_eps: 0.901961\n",
      " 109248/1000000: episode: 581, duration: 2.714s, episode steps: 183, steps per second:  67, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 0.000311, mae: 0.088881, mean_q: 0.119037, mean_eps: 0.901760\n",
      " 109476/1000000: episode: 582, duration: 3.312s, episode steps: 228, steps per second:  69, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 0.000196, mae: 0.078806, mean_q: 0.105200, mean_eps: 0.901576\n",
      " 109658/1000000: episode: 583, duration: 2.655s, episode steps: 182, steps per second:  69, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.000167, mae: 0.082455, mean_q: 0.110565, mean_eps: 0.901391\n",
      " 109835/1000000: episode: 584, duration: 2.774s, episode steps: 177, steps per second:  64, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.475 [0.000, 3.000],  loss: 0.000138, mae: 0.084895, mean_q: 0.113962, mean_eps: 0.901229\n",
      " 110044/1000000: episode: 585, duration: 3.099s, episode steps: 209, steps per second:  67, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 0.000841, mae: 0.080413, mean_q: 0.106548, mean_eps: 0.901056\n",
      " 110203/1000000: episode: 586, duration: 2.106s, episode steps: 159, steps per second:  76, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 0.002057, mae: 0.089956, mean_q: 0.118749, mean_eps: 0.900890\n",
      " 110394/1000000: episode: 587, duration: 2.677s, episode steps: 191, steps per second:  71, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.508 [0.000, 3.000],  loss: 0.001147, mae: 0.092423, mean_q: 0.123666, mean_eps: 0.900732\n",
      " 110577/1000000: episode: 588, duration: 2.454s, episode steps: 183, steps per second:  75, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.000756, mae: 0.088946, mean_q: 0.120058, mean_eps: 0.900563\n",
      " 110711/1000000: episode: 589, duration: 1.806s, episode steps: 134, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.276 [0.000, 3.000],  loss: 0.000668, mae: 0.090279, mean_q: 0.120335, mean_eps: 0.900420\n",
      " 110950/1000000: episode: 590, duration: 4.116s, episode steps: 239, steps per second:  58, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: 0.000704, mae: 0.089127, mean_q: 0.119563, mean_eps: 0.900253\n",
      " 111252/1000000: episode: 591, duration: 4.302s, episode steps: 302, steps per second:  70, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 0.000856, mae: 0.091448, mean_q: 0.123324, mean_eps: 0.900010\n",
      " 111431/1000000: episode: 592, duration: 2.615s, episode steps: 179, steps per second:  68, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.000758, mae: 0.085177, mean_q: 0.113848, mean_eps: 0.899794\n",
      " 111615/1000000: episode: 593, duration: 3.076s, episode steps: 184, steps per second:  60, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000916, mae: 0.087485, mean_q: 0.116380, mean_eps: 0.899630\n",
      " 111778/1000000: episode: 594, duration: 2.637s, episode steps: 163, steps per second:  62, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.552 [0.000, 3.000],  loss: 0.000670, mae: 0.086696, mean_q: 0.116140, mean_eps: 0.899474\n",
      " 111914/1000000: episode: 595, duration: 2.044s, episode steps: 136, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.434 [0.000, 3.000],  loss: 0.000375, mae: 0.096454, mean_q: 0.129915, mean_eps: 0.899339\n",
      " 112238/1000000: episode: 596, duration: 4.780s, episode steps: 324, steps per second:  68, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000614, mae: 0.094867, mean_q: 0.127159, mean_eps: 0.899132\n",
      " 112370/1000000: episode: 597, duration: 2.090s, episode steps: 132, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.455 [0.000, 3.000],  loss: 0.000761, mae: 0.099289, mean_q: 0.133933, mean_eps: 0.898926\n",
      " 112579/1000000: episode: 598, duration: 3.301s, episode steps: 209, steps per second:  63, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 0.000402, mae: 0.091632, mean_q: 0.122765, mean_eps: 0.898773\n",
      " 112714/1000000: episode: 599, duration: 2.329s, episode steps: 135, steps per second:  58, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.200 [0.000, 3.000],  loss: 0.000743, mae: 0.088334, mean_q: 0.119047, mean_eps: 0.898619\n",
      " 112939/1000000: episode: 600, duration: 3.970s, episode steps: 225, steps per second:  57, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.347 [0.000, 3.000],  loss: 0.000423, mae: 0.093093, mean_q: 0.124886, mean_eps: 0.898457\n",
      " 113141/1000000: episode: 601, duration: 3.267s, episode steps: 202, steps per second:  62, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.376 [0.000, 3.000],  loss: 0.000224, mae: 0.086978, mean_q: 0.117076, mean_eps: 0.898264\n",
      " 113270/1000000: episode: 602, duration: 2.067s, episode steps: 129, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.543 [0.000, 3.000],  loss: 0.000180, mae: 0.085747, mean_q: 0.114854, mean_eps: 0.898115\n",
      " 113398/1000000: episode: 603, duration: 1.965s, episode steps: 128, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.320 [0.000, 3.000],  loss: 0.000290, mae: 0.091058, mean_q: 0.121575, mean_eps: 0.897999\n",
      " 113622/1000000: episode: 604, duration: 3.425s, episode steps: 224, steps per second:  65, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.379 [0.000, 3.000],  loss: 0.000312, mae: 0.092004, mean_q: 0.123564, mean_eps: 0.897841\n",
      " 113845/1000000: episode: 605, duration: 3.211s, episode steps: 223, steps per second:  69, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 0.000298, mae: 0.090460, mean_q: 0.121143, mean_eps: 0.897639\n",
      " 114025/1000000: episode: 606, duration: 2.699s, episode steps: 180, steps per second:  67, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.000511, mae: 0.094042, mean_q: 0.125615, mean_eps: 0.897458\n",
      " 114296/1000000: episode: 607, duration: 3.894s, episode steps: 271, steps per second:  70, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 0.000281, mae: 0.089650, mean_q: 0.120669, mean_eps: 0.897256\n",
      " 114425/1000000: episode: 608, duration: 1.809s, episode steps: 129, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.000188, mae: 0.088563, mean_q: 0.119243, mean_eps: 0.897076\n",
      " 114662/1000000: episode: 609, duration: 3.224s, episode steps: 237, steps per second:  74, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.363 [0.000, 3.000],  loss: 0.000171, mae: 0.089696, mean_q: 0.119935, mean_eps: 0.896910\n",
      " 114853/1000000: episode: 610, duration: 2.669s, episode steps: 191, steps per second:  72, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.000205, mae: 0.088600, mean_q: 0.118528, mean_eps: 0.896718\n",
      " 115069/1000000: episode: 611, duration: 3.183s, episode steps: 216, steps per second:  68, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.000295, mae: 0.087780, mean_q: 0.117651, mean_eps: 0.896534\n",
      " 115248/1000000: episode: 612, duration: 2.431s, episode steps: 179, steps per second:  74, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 0.000509, mae: 0.090670, mean_q: 0.121194, mean_eps: 0.896358\n",
      " 115491/1000000: episode: 613, duration: 3.561s, episode steps: 243, steps per second:  68, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 0.000211, mae: 0.092179, mean_q: 0.123508, mean_eps: 0.896169\n",
      " 115714/1000000: episode: 614, duration: 3.122s, episode steps: 223, steps per second:  71, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.000359, mae: 0.092533, mean_q: 0.124342, mean_eps: 0.895958\n",
      " 115843/1000000: episode: 615, duration: 1.930s, episode steps: 129, steps per second:  67, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.333 [0.000, 3.000],  loss: 0.000296, mae: 0.097933, mean_q: 0.131691, mean_eps: 0.895800\n",
      " 115990/1000000: episode: 616, duration: 2.056s, episode steps: 147, steps per second:  71, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.415 [0.000, 3.000],  loss: 0.000275, mae: 0.083851, mean_q: 0.113204, mean_eps: 0.895676\n",
      " 116250/1000000: episode: 617, duration: 4.547s, episode steps: 260, steps per second:  57, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.404 [0.000, 3.000],  loss: 0.000530, mae: 0.096844, mean_q: 0.131395, mean_eps: 0.895492\n",
      " 116380/1000000: episode: 618, duration: 2.303s, episode steps: 130, steps per second:  56, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.415 [0.000, 3.000],  loss: 0.000196, mae: 0.085101, mean_q: 0.114388, mean_eps: 0.895317\n",
      " 116518/1000000: episode: 619, duration: 2.240s, episode steps: 138, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.594 [0.000, 3.000],  loss: 0.000197, mae: 0.095086, mean_q: 0.127771, mean_eps: 0.895197\n",
      " 116747/1000000: episode: 620, duration: 3.631s, episode steps: 229, steps per second:  63, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: 0.000285, mae: 0.083212, mean_q: 0.111865, mean_eps: 0.895031\n",
      " 116980/1000000: episode: 621, duration: 4.333s, episode steps: 233, steps per second:  54, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.000604, mae: 0.102228, mean_q: 0.137869, mean_eps: 0.894824\n",
      " 117115/1000000: episode: 622, duration: 2.548s, episode steps: 135, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.378 [0.000, 3.000],  loss: 0.000264, mae: 0.087647, mean_q: 0.117045, mean_eps: 0.894659\n",
      " 117309/1000000: episode: 623, duration: 3.204s, episode steps: 194, steps per second:  61, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: 0.000256, mae: 0.088745, mean_q: 0.118528, mean_eps: 0.894509\n",
      " 117447/1000000: episode: 624, duration: 2.192s, episode steps: 138, steps per second:  63, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.000376, mae: 0.093499, mean_q: 0.124731, mean_eps: 0.894360\n",
      " 117614/1000000: episode: 625, duration: 2.644s, episode steps: 167, steps per second:  63, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: 0.000218, mae: 0.088089, mean_q: 0.118595, mean_eps: 0.894223\n",
      " 117849/1000000: episode: 626, duration: 3.780s, episode steps: 235, steps per second:  62, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 0.000229, mae: 0.083764, mean_q: 0.112698, mean_eps: 0.894041\n",
      " 118002/1000000: episode: 627, duration: 2.606s, episode steps: 153, steps per second:  59, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.359 [0.000, 3.000],  loss: 0.000437, mae: 0.090311, mean_q: 0.121085, mean_eps: 0.893867\n",
      " 118206/1000000: episode: 628, duration: 3.849s, episode steps: 204, steps per second:  53, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.000174, mae: 0.078652, mean_q: 0.105655, mean_eps: 0.893706\n",
      " 118348/1000000: episode: 629, duration: 2.694s, episode steps: 142, steps per second:  53, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.514 [0.000, 3.000],  loss: 0.000182, mae: 0.091788, mean_q: 0.123215, mean_eps: 0.893552\n",
      " 118529/1000000: episode: 630, duration: 2.979s, episode steps: 181, steps per second:  61, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.370 [0.000, 3.000],  loss: 0.000244, mae: 0.093665, mean_q: 0.125150, mean_eps: 0.893406\n",
      " 118796/1000000: episode: 631, duration: 5.006s, episode steps: 267, steps per second:  53, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: 0.000267, mae: 0.084837, mean_q: 0.113314, mean_eps: 0.893204\n",
      " 118933/1000000: episode: 632, duration: 2.737s, episode steps: 137, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.000145, mae: 0.084821, mean_q: 0.114413, mean_eps: 0.893022\n",
      " 119191/1000000: episode: 633, duration: 5.701s, episode steps: 258, steps per second:  45, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: 0.000207, mae: 0.087794, mean_q: 0.117943, mean_eps: 0.892844\n",
      " 119338/1000000: episode: 634, duration: 3.222s, episode steps: 147, steps per second:  46, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.524 [0.000, 3.000],  loss: 0.000217, mae: 0.090883, mean_q: 0.121519, mean_eps: 0.892662\n",
      " 119548/1000000: episode: 635, duration: 4.483s, episode steps: 210, steps per second:  47, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.438 [0.000, 3.000],  loss: 0.000334, mae: 0.093386, mean_q: 0.124474, mean_eps: 0.892502\n",
      " 119719/1000000: episode: 636, duration: 3.475s, episode steps: 171, steps per second:  49, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 0.000321, mae: 0.088770, mean_q: 0.118736, mean_eps: 0.892331\n",
      " 119847/1000000: episode: 637, duration: 2.121s, episode steps: 128, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.711 [0.000, 3.000],  loss: 0.000240, mae: 0.088116, mean_q: 0.119471, mean_eps: 0.892196\n",
      " 120014/1000000: episode: 638, duration: 2.761s, episode steps: 167, steps per second:  60, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.431 [0.000, 3.000],  loss: 0.000159, mae: 0.083709, mean_q: 0.112272, mean_eps: 0.892063\n",
      " 120245/1000000: episode: 639, duration: 3.637s, episode steps: 231, steps per second:  64, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 0.002133, mae: 0.096033, mean_q: 0.129472, mean_eps: 0.891883\n",
      " 120543/1000000: episode: 640, duration: 4.716s, episode steps: 298, steps per second:  63, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.000770, mae: 0.092689, mean_q: 0.124273, mean_eps: 0.891645\n",
      " 120815/1000000: episode: 641, duration: 5.277s, episode steps: 272, steps per second:  52, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 0.000905, mae: 0.095577, mean_q: 0.129122, mean_eps: 0.891390\n",
      " 120941/1000000: episode: 642, duration: 2.507s, episode steps: 126, steps per second:  50, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.460 [0.000, 3.000],  loss: 0.000593, mae: 0.091380, mean_q: 0.123739, mean_eps: 0.891210\n",
      " 121178/1000000: episode: 643, duration: 4.091s, episode steps: 237, steps per second:  58, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.000576, mae: 0.094794, mean_q: 0.127333, mean_eps: 0.891046\n",
      " 121505/1000000: episode: 644, duration: 5.685s, episode steps: 327, steps per second:  58, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.413 [0.000, 3.000],  loss: 0.000695, mae: 0.092806, mean_q: 0.123797, mean_eps: 0.890792\n",
      " 121807/1000000: episode: 645, duration: 4.930s, episode steps: 302, steps per second:  61, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 0.000380, mae: 0.094909, mean_q: 0.127939, mean_eps: 0.890510\n",
      " 122010/1000000: episode: 646, duration: 3.177s, episode steps: 203, steps per second:  64, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.000319, mae: 0.089418, mean_q: 0.119578, mean_eps: 0.890283\n",
      " 122189/1000000: episode: 647, duration: 2.793s, episode steps: 179, steps per second:  64, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.000643, mae: 0.095840, mean_q: 0.127706, mean_eps: 0.890110\n",
      " 122366/1000000: episode: 648, duration: 2.879s, episode steps: 177, steps per second:  61, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.638 [0.000, 3.000],  loss: 0.000447, mae: 0.094631, mean_q: 0.126601, mean_eps: 0.889950\n",
      " 122634/1000000: episode: 649, duration: 4.888s, episode steps: 268, steps per second:  55, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.414 [0.000, 3.000],  loss: 0.000461, mae: 0.096268, mean_q: 0.128264, mean_eps: 0.889750\n",
      " 122771/1000000: episode: 650, duration: 2.271s, episode steps: 137, steps per second:  60, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.467 [0.000, 3.000],  loss: 0.000732, mae: 0.104149, mean_q: 0.139660, mean_eps: 0.889568\n",
      " 122922/1000000: episode: 651, duration: 2.433s, episode steps: 151, steps per second:  62, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: 0.000269, mae: 0.086579, mean_q: 0.116405, mean_eps: 0.889439\n",
      " 123104/1000000: episode: 652, duration: 2.947s, episode steps: 182, steps per second:  62, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.407 [0.000, 3.000],  loss: 0.000333, mae: 0.098433, mean_q: 0.131937, mean_eps: 0.889289\n",
      " 123316/1000000: episode: 653, duration: 3.486s, episode steps: 212, steps per second:  61, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.363 [0.000, 3.000],  loss: 0.000264, mae: 0.095331, mean_q: 0.127926, mean_eps: 0.889113\n",
      " 123496/1000000: episode: 654, duration: 3.000s, episode steps: 180, steps per second:  60, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.528 [0.000, 3.000],  loss: 0.000160, mae: 0.095091, mean_q: 0.127829, mean_eps: 0.888936\n",
      " 123721/1000000: episode: 655, duration: 2.588s, episode steps: 225, steps per second:  87, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.000260, mae: 0.093328, mean_q: 0.125611, mean_eps: 0.888753\n",
      " 123951/1000000: episode: 656, duration: 2.451s, episode steps: 230, steps per second:  94, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.409 [0.000, 3.000],  loss: 0.000403, mae: 0.097565, mean_q: 0.131691, mean_eps: 0.888548\n",
      " 124156/1000000: episode: 657, duration: 2.108s, episode steps: 205, steps per second:  97, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.473 [0.000, 3.000],  loss: 0.000269, mae: 0.095972, mean_q: 0.130097, mean_eps: 0.888353\n",
      " 124319/1000000: episode: 658, duration: 1.664s, episode steps: 163, steps per second:  98, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 0.000227, mae: 0.092115, mean_q: 0.124150, mean_eps: 0.888188\n",
      " 124550/1000000: episode: 659, duration: 2.323s, episode steps: 231, steps per second:  99, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.429 [0.000, 3.000],  loss: 0.000259, mae: 0.096622, mean_q: 0.129457, mean_eps: 0.888009\n",
      " 124712/1000000: episode: 660, duration: 1.927s, episode steps: 162, steps per second:  84, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 0.000209, mae: 0.088502, mean_q: 0.118392, mean_eps: 0.887833\n",
      " 124869/1000000: episode: 661, duration: 1.847s, episode steps: 157, steps per second:  85, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 0.000237, mae: 0.096908, mean_q: 0.129880, mean_eps: 0.887689\n",
      " 125014/1000000: episode: 662, duration: 1.517s, episode steps: 145, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.483 [0.000, 3.000],  loss: 0.000185, mae: 0.084124, mean_q: 0.112906, mean_eps: 0.887552\n",
      " 125316/1000000: episode: 663, duration: 3.187s, episode steps: 302, steps per second:  95, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: 0.000205, mae: 0.093422, mean_q: 0.124491, mean_eps: 0.887352\n",
      " 125528/1000000: episode: 664, duration: 2.186s, episode steps: 212, steps per second:  97, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.528 [0.000, 3.000],  loss: 0.000188, mae: 0.092898, mean_q: 0.124127, mean_eps: 0.887122\n",
      " 125694/1000000: episode: 665, duration: 1.742s, episode steps: 166, steps per second:  95, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.000192, mae: 0.095157, mean_q: 0.127324, mean_eps: 0.886951\n",
      " 125848/1000000: episode: 666, duration: 1.545s, episode steps: 154, steps per second: 100, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.526 [0.000, 3.000],  loss: 0.000205, mae: 0.103464, mean_q: 0.138794, mean_eps: 0.886807\n",
      " 126053/1000000: episode: 667, duration: 2.134s, episode steps: 205, steps per second:  96, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.395 [0.000, 3.000],  loss: 0.000324, mae: 0.099173, mean_q: 0.133806, mean_eps: 0.886645\n",
      " 126276/1000000: episode: 668, duration: 2.234s, episode steps: 223, steps per second: 100, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.336 [0.000, 3.000],  loss: 0.000215, mae: 0.096514, mean_q: 0.129762, mean_eps: 0.886452\n",
      " 126520/1000000: episode: 669, duration: 2.391s, episode steps: 244, steps per second: 102, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 0.000221, mae: 0.087492, mean_q: 0.116765, mean_eps: 0.886244\n",
      " 126681/1000000: episode: 670, duration: 1.531s, episode steps: 161, steps per second: 105, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.385 [0.000, 3.000],  loss: 0.000208, mae: 0.088745, mean_q: 0.118535, mean_eps: 0.886060\n",
      " 126865/1000000: episode: 671, duration: 1.714s, episode steps: 184, steps per second: 107, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: 0.000192, mae: 0.098567, mean_q: 0.131978, mean_eps: 0.885903\n",
      " 127032/1000000: episode: 672, duration: 1.598s, episode steps: 167, steps per second: 104, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.569 [0.000, 3.000],  loss: 0.000414, mae: 0.098482, mean_q: 0.131565, mean_eps: 0.885747\n",
      " 127233/1000000: episode: 673, duration: 1.896s, episode steps: 201, steps per second: 106, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 0.000202, mae: 0.102733, mean_q: 0.137339, mean_eps: 0.885581\n",
      " 127463/1000000: episode: 674, duration: 2.152s, episode steps: 230, steps per second: 107, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.596 [0.000, 3.000],  loss: 0.000271, mae: 0.100038, mean_q: 0.133873, mean_eps: 0.885387\n",
      " 127609/1000000: episode: 675, duration: 1.402s, episode steps: 146, steps per second: 104, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.418 [0.000, 3.000],  loss: 0.000143, mae: 0.093303, mean_q: 0.125054, mean_eps: 0.885218\n",
      " 127831/1000000: episode: 676, duration: 2.106s, episode steps: 222, steps per second: 105, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 0.000269, mae: 0.092795, mean_q: 0.125325, mean_eps: 0.885052\n",
      " 128015/1000000: episode: 677, duration: 1.841s, episode steps: 184, steps per second: 100, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.647 [0.000, 3.000],  loss: 0.000200, mae: 0.090819, mean_q: 0.121349, mean_eps: 0.884870\n",
      " 128236/1000000: episode: 678, duration: 2.141s, episode steps: 221, steps per second: 103, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 0.000118, mae: 0.088693, mean_q: 0.118680, mean_eps: 0.884688\n",
      " 128524/1000000: episode: 679, duration: 2.775s, episode steps: 288, steps per second: 104, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 0.000195, mae: 0.089212, mean_q: 0.119123, mean_eps: 0.884460\n",
      " 128704/1000000: episode: 680, duration: 1.792s, episode steps: 180, steps per second: 100, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 0.000289, mae: 0.098400, mean_q: 0.131983, mean_eps: 0.884249\n",
      " 128901/1000000: episode: 681, duration: 1.930s, episode steps: 197, steps per second: 102, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.360 [0.000, 3.000],  loss: 0.000297, mae: 0.096587, mean_q: 0.128993, mean_eps: 0.884078\n",
      " 129076/1000000: episode: 682, duration: 1.695s, episode steps: 175, steps per second: 103, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 0.000147, mae: 0.092927, mean_q: 0.124976, mean_eps: 0.883911\n",
      " 129332/1000000: episode: 683, duration: 2.462s, episode steps: 256, steps per second: 104, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 0.000167, mae: 0.089941, mean_q: 0.120107, mean_eps: 0.883718\n",
      " 129582/1000000: episode: 684, duration: 2.511s, episode steps: 250, steps per second: 100, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.468 [0.000, 3.000],  loss: 0.000257, mae: 0.097003, mean_q: 0.129536, mean_eps: 0.883490\n",
      " 129837/1000000: episode: 685, duration: 2.484s, episode steps: 255, steps per second: 103, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: 0.000153, mae: 0.092521, mean_q: 0.123816, mean_eps: 0.883261\n",
      " 129969/1000000: episode: 686, duration: 1.236s, episode steps: 132, steps per second: 107, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.417 [0.000, 3.000],  loss: 0.000396, mae: 0.095334, mean_q: 0.128069, mean_eps: 0.883086\n",
      " 130189/1000000: episode: 687, duration: 2.212s, episode steps: 220, steps per second:  99, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 0.001113, mae: 0.099172, mean_q: 0.133541, mean_eps: 0.882928\n",
      " 130419/1000000: episode: 688, duration: 2.240s, episode steps: 230, steps per second: 103, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: 0.001491, mae: 0.105010, mean_q: 0.141242, mean_eps: 0.882726\n",
      " 130554/1000000: episode: 689, duration: 1.473s, episode steps: 135, steps per second:  92, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.281 [0.000, 3.000],  loss: 0.001950, mae: 0.100067, mean_q: 0.132082, mean_eps: 0.882563\n",
      " 130730/1000000: episode: 690, duration: 1.778s, episode steps: 176, steps per second:  99, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.659 [0.000, 3.000],  loss: 0.000877, mae: 0.103256, mean_q: 0.136682, mean_eps: 0.882422\n",
      " 130911/1000000: episode: 691, duration: 1.704s, episode steps: 181, steps per second: 106, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.001058, mae: 0.104663, mean_q: 0.141759, mean_eps: 0.882262\n",
      " 131046/1000000: episode: 692, duration: 1.407s, episode steps: 135, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.459 [0.000, 3.000],  loss: 0.001343, mae: 0.106853, mean_q: 0.143179, mean_eps: 0.882120\n",
      " 131200/1000000: episode: 693, duration: 1.687s, episode steps: 154, steps per second:  91, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.526 [0.000, 3.000],  loss: 0.000610, mae: 0.094957, mean_q: 0.127955, mean_eps: 0.881990\n",
      " 131336/1000000: episode: 694, duration: 1.391s, episode steps: 136, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.449 [0.000, 3.000],  loss: 0.000856, mae: 0.095716, mean_q: 0.128170, mean_eps: 0.881861\n",
      " 131464/1000000: episode: 695, duration: 1.233s, episode steps: 128, steps per second: 104, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.617 [0.000, 3.000],  loss: 0.000326, mae: 0.096874, mean_q: 0.129739, mean_eps: 0.881742\n",
      " 131644/1000000: episode: 696, duration: 1.775s, episode steps: 180, steps per second: 101, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.472 [0.000, 3.000],  loss: 0.000549, mae: 0.098690, mean_q: 0.131704, mean_eps: 0.881603\n",
      " 131770/1000000: episode: 697, duration: 1.208s, episode steps: 126, steps per second: 104, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.643 [0.000, 3.000],  loss: 0.000526, mae: 0.108707, mean_q: 0.144996, mean_eps: 0.881465\n",
      " 131926/1000000: episode: 698, duration: 1.494s, episode steps: 156, steps per second: 104, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: 0.000361, mae: 0.094704, mean_q: 0.127226, mean_eps: 0.881337\n",
      " 132130/1000000: episode: 699, duration: 1.920s, episode steps: 204, steps per second: 106, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.412 [0.000, 3.000],  loss: 0.000410, mae: 0.095947, mean_q: 0.128737, mean_eps: 0.881175\n",
      " 132344/1000000: episode: 700, duration: 2.041s, episode steps: 214, steps per second: 105, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.308 [0.000, 3.000],  loss: 0.000347, mae: 0.101870, mean_q: 0.136650, mean_eps: 0.880988\n",
      " 132511/1000000: episode: 701, duration: 1.594s, episode steps: 167, steps per second: 105, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.515 [0.000, 3.000],  loss: 0.000283, mae: 0.101670, mean_q: 0.135679, mean_eps: 0.880817\n",
      " 132647/1000000: episode: 702, duration: 1.319s, episode steps: 136, steps per second: 103, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.382 [0.000, 3.000],  loss: 0.000867, mae: 0.098659, mean_q: 0.132723, mean_eps: 0.880680\n",
      " 132775/1000000: episode: 703, duration: 1.278s, episode steps: 128, steps per second: 100, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.711 [0.000, 3.000],  loss: 0.000574, mae: 0.103703, mean_q: 0.137854, mean_eps: 0.880561\n",
      " 133077/1000000: episode: 704, duration: 2.908s, episode steps: 302, steps per second: 104, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.417 [0.000, 3.000],  loss: 0.000350, mae: 0.097318, mean_q: 0.130361, mean_eps: 0.880367\n",
      " 133293/1000000: episode: 705, duration: 2.032s, episode steps: 216, steps per second: 106, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.440 [0.000, 3.000],  loss: 0.000265, mae: 0.093818, mean_q: 0.125650, mean_eps: 0.880133\n",
      " 133417/1000000: episode: 706, duration: 1.198s, episode steps: 124, steps per second: 103, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.516 [0.000, 3.000],  loss: 0.000179, mae: 0.086653, mean_q: 0.115587, mean_eps: 0.879980\n",
      " 133598/1000000: episode: 707, duration: 1.928s, episode steps: 181, steps per second:  94, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.674 [0.000, 3.000],  loss: 0.000331, mae: 0.098778, mean_q: 0.131318, mean_eps: 0.879843\n",
      " 133754/1000000: episode: 708, duration: 1.851s, episode steps: 156, steps per second:  84, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 0.000207, mae: 0.095343, mean_q: 0.127669, mean_eps: 0.879692\n",
      " 133971/1000000: episode: 709, duration: 2.603s, episode steps: 217, steps per second:  83, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.000364, mae: 0.094348, mean_q: 0.126615, mean_eps: 0.879524\n",
      " 134144/1000000: episode: 710, duration: 2.493s, episode steps: 173, steps per second:  69, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.347 [0.000, 3.000],  loss: 0.000272, mae: 0.099492, mean_q: 0.134438, mean_eps: 0.879350\n",
      " 134475/1000000: episode: 711, duration: 4.217s, episode steps: 331, steps per second:  78, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.480 [0.000, 3.000],  loss: 0.000197, mae: 0.095126, mean_q: 0.127935, mean_eps: 0.879123\n",
      " 134621/1000000: episode: 712, duration: 1.600s, episode steps: 146, steps per second:  91, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.603 [0.000, 3.000],  loss: 0.000303, mae: 0.114023, mean_q: 0.152761, mean_eps: 0.878907\n",
      " 134858/1000000: episode: 713, duration: 2.564s, episode steps: 237, steps per second:  92, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.000209, mae: 0.101210, mean_q: 0.135423, mean_eps: 0.878734\n",
      " 135078/1000000: episode: 714, duration: 2.271s, episode steps: 220, steps per second:  97, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.586 [0.000, 3.000],  loss: 0.000282, mae: 0.102266, mean_q: 0.137040, mean_eps: 0.878529\n",
      " 135380/1000000: episode: 715, duration: 3.172s, episode steps: 302, steps per second:  95, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: 0.000206, mae: 0.098871, mean_q: 0.132248, mean_eps: 0.878295\n",
      " 135588/1000000: episode: 716, duration: 2.700s, episode steps: 208, steps per second:  77, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.428 [0.000, 3.000],  loss: 0.000176, mae: 0.105656, mean_q: 0.141136, mean_eps: 0.878066\n",
      " 135728/1000000: episode: 717, duration: 1.897s, episode steps: 140, steps per second:  74, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.407 [0.000, 3.000],  loss: 0.000190, mae: 0.092512, mean_q: 0.124049, mean_eps: 0.877910\n",
      " 135945/1000000: episode: 718, duration: 2.836s, episode steps: 217, steps per second:  77, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: 0.000204, mae: 0.109064, mean_q: 0.146421, mean_eps: 0.877748\n",
      " 136126/1000000: episode: 719, duration: 2.197s, episode steps: 181, steps per second:  82, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 0.000634, mae: 0.102994, mean_q: 0.138333, mean_eps: 0.877568\n",
      " 136252/1000000: episode: 720, duration: 1.938s, episode steps: 126, steps per second:  65, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.365 [0.000, 3.000],  loss: 0.000187, mae: 0.089979, mean_q: 0.121784, mean_eps: 0.877431\n",
      " 136384/1000000: episode: 721, duration: 1.575s, episode steps: 132, steps per second:  84, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.583 [0.000, 3.000],  loss: 0.000116, mae: 0.097512, mean_q: 0.130346, mean_eps: 0.877316\n",
      " 136593/1000000: episode: 722, duration: 2.226s, episode steps: 209, steps per second:  94, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.569 [0.000, 3.000],  loss: 0.000250, mae: 0.100223, mean_q: 0.133939, mean_eps: 0.877161\n",
      " 136798/1000000: episode: 723, duration: 2.053s, episode steps: 205, steps per second: 100, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.366 [0.000, 3.000],  loss: 0.000159, mae: 0.092794, mean_q: 0.123669, mean_eps: 0.876974\n",
      " 136933/1000000: episode: 724, duration: 1.424s, episode steps: 135, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.615 [0.000, 3.000],  loss: 0.000347, mae: 0.100954, mean_q: 0.135617, mean_eps: 0.876821\n",
      " 137155/1000000: episode: 725, duration: 2.508s, episode steps: 222, steps per second:  89, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: 0.000215, mae: 0.097706, mean_q: 0.131052, mean_eps: 0.876660\n",
      " 137299/1000000: episode: 726, duration: 1.482s, episode steps: 144, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.549 [0.000, 3.000],  loss: 0.000290, mae: 0.092148, mean_q: 0.123830, mean_eps: 0.876497\n",
      " 137426/1000000: episode: 727, duration: 1.300s, episode steps: 127, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.567 [0.000, 3.000],  loss: 0.000114, mae: 0.089849, mean_q: 0.119763, mean_eps: 0.876374\n",
      " 137603/1000000: episode: 728, duration: 1.715s, episode steps: 177, steps per second: 103, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.000140, mae: 0.089318, mean_q: 0.118960, mean_eps: 0.876237\n",
      " 137863/1000000: episode: 729, duration: 2.534s, episode steps: 260, steps per second: 103, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.450 [0.000, 3.000],  loss: 0.000188, mae: 0.102286, mean_q: 0.136800, mean_eps: 0.876041\n",
      " 138068/1000000: episode: 730, duration: 2.135s, episode steps: 205, steps per second:  96, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.429 [0.000, 3.000],  loss: 0.000481, mae: 0.102966, mean_q: 0.137842, mean_eps: 0.875832\n",
      " 138294/1000000: episode: 731, duration: 2.385s, episode steps: 226, steps per second:  95, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.650 [0.000, 3.000],  loss: 0.000272, mae: 0.104665, mean_q: 0.139362, mean_eps: 0.875638\n",
      " 138433/1000000: episode: 732, duration: 1.472s, episode steps: 139, steps per second:  94, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.482 [0.000, 3.000],  loss: 0.000146, mae: 0.100284, mean_q: 0.134688, mean_eps: 0.875472\n",
      " 138654/1000000: episode: 733, duration: 2.243s, episode steps: 221, steps per second:  99, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.376 [0.000, 3.000],  loss: 0.000240, mae: 0.100820, mean_q: 0.135182, mean_eps: 0.875310\n",
      " 138781/1000000: episode: 734, duration: 1.262s, episode steps: 127, steps per second: 101, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.748 [0.000, 3.000],  loss: 0.000222, mae: 0.093159, mean_q: 0.125332, mean_eps: 0.875154\n",
      " 139030/1000000: episode: 735, duration: 2.474s, episode steps: 249, steps per second: 101, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.000193, mae: 0.101529, mean_q: 0.135659, mean_eps: 0.874985\n",
      " 139156/1000000: episode: 736, duration: 1.256s, episode steps: 126, steps per second: 100, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.286 [0.000, 3.000],  loss: 0.000200, mae: 0.102213, mean_q: 0.136794, mean_eps: 0.874817\n",
      " 139299/1000000: episode: 737, duration: 1.443s, episode steps: 143, steps per second:  99, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.664 [0.000, 3.000],  loss: 0.000122, mae: 0.096172, mean_q: 0.129177, mean_eps: 0.874697\n",
      " 139527/1000000: episode: 738, duration: 2.301s, episode steps: 228, steps per second:  99, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.000207, mae: 0.107687, mean_q: 0.143983, mean_eps: 0.874529\n",
      " 139666/1000000: episode: 739, duration: 1.427s, episode steps: 139, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.338 [0.000, 3.000],  loss: 0.000390, mae: 0.093799, mean_q: 0.125904, mean_eps: 0.874364\n",
      " 139847/1000000: episode: 740, duration: 1.783s, episode steps: 181, steps per second: 102, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.398 [0.000, 3.000],  loss: 0.000170, mae: 0.098545, mean_q: 0.132903, mean_eps: 0.874220\n",
      " 140072/1000000: episode: 741, duration: 2.270s, episode steps: 225, steps per second:  99, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.596 [0.000, 3.000],  loss: 0.000744, mae: 0.099666, mean_q: 0.132623, mean_eps: 0.874038\n",
      " 140217/1000000: episode: 742, duration: 1.516s, episode steps: 145, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.310 [0.000, 3.000],  loss: 0.002036, mae: 0.103783, mean_q: 0.139858, mean_eps: 0.873870\n",
      " 140444/1000000: episode: 743, duration: 2.263s, episode steps: 227, steps per second: 100, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.361 [0.000, 3.000],  loss: 0.000996, mae: 0.098456, mean_q: 0.131022, mean_eps: 0.873703\n",
      " 140726/1000000: episode: 744, duration: 2.779s, episode steps: 282, steps per second: 101, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.440 [0.000, 3.000],  loss: 0.000832, mae: 0.103920, mean_q: 0.142398, mean_eps: 0.873474\n",
      " 140859/1000000: episode: 745, duration: 1.322s, episode steps: 133, steps per second: 101, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.421 [0.000, 3.000],  loss: 0.000885, mae: 0.099737, mean_q: 0.133702, mean_eps: 0.873287\n",
      " 141082/1000000: episode: 746, duration: 2.211s, episode steps: 223, steps per second: 101, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: 0.000706, mae: 0.105976, mean_q: 0.141444, mean_eps: 0.873127\n",
      " 141258/1000000: episode: 747, duration: 1.748s, episode steps: 176, steps per second: 101, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.000512, mae: 0.100178, mean_q: 0.133040, mean_eps: 0.872947\n",
      " 141423/1000000: episode: 748, duration: 1.643s, episode steps: 165, steps per second: 100, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 0.000551, mae: 0.106878, mean_q: 0.143307, mean_eps: 0.872794\n",
      " 141656/1000000: episode: 749, duration: 2.348s, episode steps: 233, steps per second:  99, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 0.000360, mae: 0.105125, mean_q: 0.142180, mean_eps: 0.872616\n",
      " 141918/1000000: episode: 750, duration: 2.682s, episode steps: 262, steps per second:  98, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.000770, mae: 0.097794, mean_q: 0.131965, mean_eps: 0.872393\n",
      " 142078/1000000: episode: 751, duration: 1.570s, episode steps: 160, steps per second: 102, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.387 [0.000, 3.000],  loss: 0.000448, mae: 0.100555, mean_q: 0.134992, mean_eps: 0.872202\n",
      " 142288/1000000: episode: 752, duration: 2.122s, episode steps: 210, steps per second:  99, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 0.000634, mae: 0.100461, mean_q: 0.134740, mean_eps: 0.872036\n",
      " 142530/1000000: episode: 753, duration: 2.388s, episode steps: 242, steps per second: 101, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.533 [0.000, 3.000],  loss: 0.000326, mae: 0.098435, mean_q: 0.131807, mean_eps: 0.871833\n",
      " 142788/1000000: episode: 754, duration: 2.587s, episode steps: 258, steps per second: 100, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.426 [0.000, 3.000],  loss: 0.000310, mae: 0.094110, mean_q: 0.125467, mean_eps: 0.871608\n",
      " 143016/1000000: episode: 755, duration: 2.268s, episode steps: 228, steps per second: 101, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.000491, mae: 0.101167, mean_q: 0.136127, mean_eps: 0.871390\n",
      " 143160/1000000: episode: 756, duration: 1.486s, episode steps: 144, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.639 [0.000, 3.000],  loss: 0.000474, mae: 0.102232, mean_q: 0.136747, mean_eps: 0.871223\n",
      " 143332/1000000: episode: 757, duration: 1.802s, episode steps: 172, steps per second:  95, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.552 [0.000, 3.000],  loss: 0.000409, mae: 0.104129, mean_q: 0.139963, mean_eps: 0.871080\n",
      " 143481/1000000: episode: 758, duration: 1.523s, episode steps: 149, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.685 [0.000, 3.000],  loss: 0.000295, mae: 0.096465, mean_q: 0.129519, mean_eps: 0.870935\n",
      " 143620/1000000: episode: 759, duration: 1.409s, episode steps: 139, steps per second:  99, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.813 [0.000, 3.000],  loss: 0.000332, mae: 0.100790, mean_q: 0.135393, mean_eps: 0.870805\n",
      " 143808/1000000: episode: 760, duration: 1.887s, episode steps: 188, steps per second: 100, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.000196, mae: 0.099329, mean_q: 0.133096, mean_eps: 0.870659\n",
      " 144039/1000000: episode: 761, duration: 2.291s, episode steps: 231, steps per second: 101, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.468 [0.000, 3.000],  loss: 0.000423, mae: 0.098698, mean_q: 0.132067, mean_eps: 0.870470\n",
      " 144236/1000000: episode: 762, duration: 2.043s, episode steps: 197, steps per second:  96, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.335 [0.000, 3.000],  loss: 0.000267, mae: 0.098606, mean_q: 0.133175, mean_eps: 0.870278\n",
      " 144491/1000000: episode: 763, duration: 2.641s, episode steps: 255, steps per second:  97, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.447 [0.000, 3.000],  loss: 0.000269, mae: 0.095824, mean_q: 0.128377, mean_eps: 0.870074\n",
      " 144706/1000000: episode: 764, duration: 2.131s, episode steps: 215, steps per second: 101, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.428 [0.000, 3.000],  loss: 0.000240, mae: 0.103507, mean_q: 0.139966, mean_eps: 0.869862\n",
      " 144944/1000000: episode: 765, duration: 2.439s, episode steps: 238, steps per second:  98, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.000281, mae: 0.094422, mean_q: 0.126018, mean_eps: 0.869658\n",
      " 145089/1000000: episode: 766, duration: 1.514s, episode steps: 145, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.434 [0.000, 3.000],  loss: 0.000291, mae: 0.100067, mean_q: 0.134730, mean_eps: 0.869486\n",
      " 145218/1000000: episode: 767, duration: 1.328s, episode steps: 129, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.465 [0.000, 3.000],  loss: 0.000203, mae: 0.098985, mean_q: 0.133047, mean_eps: 0.869361\n",
      " 145351/1000000: episode: 768, duration: 1.374s, episode steps: 133, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.624 [0.000, 3.000],  loss: 0.000170, mae: 0.099165, mean_q: 0.133766, mean_eps: 0.869244\n",
      " 145516/1000000: episode: 769, duration: 1.711s, episode steps: 165, steps per second:  96, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 0.000359, mae: 0.107183, mean_q: 0.143988, mean_eps: 0.869111\n",
      " 145772/1000000: episode: 770, duration: 2.725s, episode steps: 256, steps per second:  94, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.582 [0.000, 3.000],  loss: 0.000210, mae: 0.096718, mean_q: 0.129295, mean_eps: 0.868922\n",
      " 145953/1000000: episode: 771, duration: 1.969s, episode steps: 181, steps per second:  92, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.348 [0.000, 3.000],  loss: 0.000174, mae: 0.102828, mean_q: 0.137618, mean_eps: 0.868724\n",
      " 146115/1000000: episode: 772, duration: 1.610s, episode steps: 162, steps per second: 101, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.370 [0.000, 3.000],  loss: 0.000208, mae: 0.089658, mean_q: 0.120487, mean_eps: 0.868569\n",
      " 146344/1000000: episode: 773, duration: 2.302s, episode steps: 229, steps per second:  99, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: 0.000129, mae: 0.103493, mean_q: 0.138887, mean_eps: 0.868395\n",
      " 146475/1000000: episode: 774, duration: 1.369s, episode steps: 131, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.344 [0.000, 3.000],  loss: 0.000203, mae: 0.093184, mean_q: 0.124540, mean_eps: 0.868233\n",
      " 146620/1000000: episode: 775, duration: 1.503s, episode steps: 145, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.434 [0.000, 3.000],  loss: 0.000373, mae: 0.103756, mean_q: 0.138599, mean_eps: 0.868109\n",
      " 146785/1000000: episode: 776, duration: 1.714s, episode steps: 165, steps per second:  96, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000212, mae: 0.101723, mean_q: 0.136177, mean_eps: 0.867968\n",
      " 147048/1000000: episode: 777, duration: 2.658s, episode steps: 263, steps per second:  99, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.000230, mae: 0.100106, mean_q: 0.133853, mean_eps: 0.867776\n",
      " 147220/1000000: episode: 778, duration: 1.799s, episode steps: 172, steps per second:  96, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.419 [0.000, 3.000],  loss: 0.000201, mae: 0.099351, mean_q: 0.133339, mean_eps: 0.867581\n",
      " 147454/1000000: episode: 779, duration: 2.462s, episode steps: 234, steps per second:  95, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: 0.000233, mae: 0.103075, mean_q: 0.137861, mean_eps: 0.867398\n",
      " 147583/1000000: episode: 780, duration: 1.364s, episode steps: 129, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.403 [0.000, 3.000],  loss: 0.000325, mae: 0.109268, mean_q: 0.145679, mean_eps: 0.867234\n",
      " 147858/1000000: episode: 781, duration: 2.804s, episode steps: 275, steps per second:  98, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.415 [0.000, 3.000],  loss: 0.000187, mae: 0.099920, mean_q: 0.134336, mean_eps: 0.867052\n",
      " 148046/1000000: episode: 782, duration: 1.974s, episode steps: 188, steps per second:  95, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.644 [0.000, 3.000],  loss: 0.000370, mae: 0.098453, mean_q: 0.131780, mean_eps: 0.866843\n",
      " 148253/1000000: episode: 783, duration: 2.094s, episode steps: 207, steps per second:  99, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.454 [0.000, 3.000],  loss: 0.000209, mae: 0.105221, mean_q: 0.140420, mean_eps: 0.866665\n",
      " 148487/1000000: episode: 784, duration: 2.285s, episode steps: 234, steps per second: 102, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 0.000168, mae: 0.096865, mean_q: 0.129630, mean_eps: 0.866467\n",
      " 148639/1000000: episode: 785, duration: 1.507s, episode steps: 152, steps per second: 101, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: 0.000191, mae: 0.109630, mean_q: 0.146544, mean_eps: 0.866294\n",
      " 148772/1000000: episode: 786, duration: 1.338s, episode steps: 133, steps per second:  99, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.353 [0.000, 3.000],  loss: 0.000146, mae: 0.099075, mean_q: 0.132666, mean_eps: 0.866166\n",
      " 148996/1000000: episode: 787, duration: 2.329s, episode steps: 224, steps per second:  96, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.000172, mae: 0.099762, mean_q: 0.133653, mean_eps: 0.866006\n",
      " 149288/1000000: episode: 788, duration: 2.990s, episode steps: 292, steps per second:  98, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: 0.000131, mae: 0.095148, mean_q: 0.127030, mean_eps: 0.865774\n",
      " 149554/1000000: episode: 789, duration: 2.675s, episode steps: 266, steps per second:  99, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 0.000155, mae: 0.100239, mean_q: 0.134552, mean_eps: 0.865522\n",
      " 149749/1000000: episode: 790, duration: 2.016s, episode steps: 195, steps per second:  97, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.000178, mae: 0.100758, mean_q: 0.134820, mean_eps: 0.865313\n",
      " 149923/1000000: episode: 791, duration: 1.750s, episode steps: 174, steps per second:  99, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.316 [0.000, 3.000],  loss: 0.000162, mae: 0.099914, mean_q: 0.133800, mean_eps: 0.865148\n",
      " 150065/1000000: episode: 792, duration: 1.488s, episode steps: 142, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.001531, mae: 0.108305, mean_q: 0.143745, mean_eps: 0.865005\n",
      " 150273/1000000: episode: 793, duration: 2.121s, episode steps: 208, steps per second:  98, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001782, mae: 0.112258, mean_q: 0.151772, mean_eps: 0.864847\n",
      " 150459/1000000: episode: 794, duration: 1.907s, episode steps: 186, steps per second:  98, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.000745, mae: 0.103603, mean_q: 0.139468, mean_eps: 0.864671\n",
      " 150618/1000000: episode: 795, duration: 1.625s, episode steps: 159, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.535 [0.000, 3.000],  loss: 0.000978, mae: 0.108825, mean_q: 0.145974, mean_eps: 0.864516\n",
      " 150885/1000000: episode: 796, duration: 2.655s, episode steps: 267, steps per second: 101, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.539 [0.000, 3.000],  loss: 0.001250, mae: 0.111536, mean_q: 0.149167, mean_eps: 0.864323\n",
      " 151151/1000000: episode: 797, duration: 2.690s, episode steps: 266, steps per second:  99, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 0.000701, mae: 0.111530, mean_q: 0.147740, mean_eps: 0.864084\n",
      " 151326/1000000: episode: 798, duration: 1.773s, episode steps: 175, steps per second:  99, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.354 [0.000, 3.000],  loss: 0.000503, mae: 0.110257, mean_q: 0.148302, mean_eps: 0.863886\n",
      " 151528/1000000: episode: 799, duration: 2.023s, episode steps: 202, steps per second: 100, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.000657, mae: 0.117630, mean_q: 0.158603, mean_eps: 0.863717\n",
      " 151723/1000000: episode: 800, duration: 1.927s, episode steps: 195, steps per second: 101, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.354 [0.000, 3.000],  loss: 0.000443, mae: 0.112481, mean_q: 0.149005, mean_eps: 0.863538\n",
      " 151931/1000000: episode: 801, duration: 2.038s, episode steps: 208, steps per second: 102, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.000545, mae: 0.113863, mean_q: 0.152248, mean_eps: 0.863357\n",
      " 152059/1000000: episode: 802, duration: 1.253s, episode steps: 128, steps per second: 102, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.398 [0.000, 3.000],  loss: 0.000395, mae: 0.125472, mean_q: 0.167770, mean_eps: 0.863205\n",
      " 152191/1000000: episode: 803, duration: 1.339s, episode steps: 132, steps per second:  99, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.462 [0.000, 3.000],  loss: 0.000383, mae: 0.115962, mean_q: 0.153634, mean_eps: 0.863088\n",
      " 152371/1000000: episode: 804, duration: 1.779s, episode steps: 180, steps per second: 101, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 0.000527, mae: 0.105515, mean_q: 0.140114, mean_eps: 0.862948\n",
      " 152558/1000000: episode: 805, duration: 1.900s, episode steps: 187, steps per second:  98, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.406 [0.000, 3.000],  loss: 0.000395, mae: 0.115862, mean_q: 0.154069, mean_eps: 0.862782\n",
      " 152742/1000000: episode: 806, duration: 1.950s, episode steps: 184, steps per second:  94, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: 0.000561, mae: 0.106740, mean_q: 0.143060, mean_eps: 0.862615\n",
      " 152976/1000000: episode: 807, duration: 2.397s, episode steps: 234, steps per second:  98, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.346 [0.000, 3.000],  loss: 0.000488, mae: 0.106227, mean_q: 0.142277, mean_eps: 0.862428\n",
      " 153256/1000000: episode: 808, duration: 2.784s, episode steps: 280, steps per second: 101, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.411 [0.000, 3.000],  loss: 0.000326, mae: 0.109647, mean_q: 0.147320, mean_eps: 0.862197\n",
      " 153433/1000000: episode: 809, duration: 1.792s, episode steps: 177, steps per second:  99, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.740 [0.000, 3.000],  loss: 0.000309, mae: 0.108956, mean_q: 0.147183, mean_eps: 0.861990\n",
      " 153701/1000000: episode: 810, duration: 2.644s, episode steps: 268, steps per second: 101, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.000248, mae: 0.108628, mean_q: 0.144920, mean_eps: 0.861789\n",
      " 153857/1000000: episode: 811, duration: 1.580s, episode steps: 156, steps per second:  99, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 0.000295, mae: 0.115388, mean_q: 0.153500, mean_eps: 0.861598\n",
      " 154033/1000000: episode: 812, duration: 1.797s, episode steps: 176, steps per second:  98, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.472 [0.000, 3.000],  loss: 0.000180, mae: 0.108528, mean_q: 0.145660, mean_eps: 0.861449\n",
      " 154217/1000000: episode: 813, duration: 1.855s, episode steps: 184, steps per second:  99, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 0.000142, mae: 0.100326, mean_q: 0.134066, mean_eps: 0.861287\n",
      " 154372/1000000: episode: 814, duration: 1.610s, episode steps: 155, steps per second:  96, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 0.000233, mae: 0.102901, mean_q: 0.137611, mean_eps: 0.861135\n",
      " 154582/1000000: episode: 815, duration: 2.087s, episode steps: 210, steps per second: 101, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.410 [0.000, 3.000],  loss: 0.000204, mae: 0.107229, mean_q: 0.143406, mean_eps: 0.860972\n",
      " 154846/1000000: episode: 816, duration: 2.671s, episode steps: 264, steps per second:  99, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: 0.000194, mae: 0.106087, mean_q: 0.142178, mean_eps: 0.860757\n",
      " 155054/1000000: episode: 817, duration: 2.110s, episode steps: 208, steps per second:  99, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.678 [0.000, 3.000],  loss: 0.000196, mae: 0.104128, mean_q: 0.139869, mean_eps: 0.860545\n",
      " 155234/1000000: episode: 818, duration: 1.802s, episode steps: 180, steps per second: 100, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 0.000222, mae: 0.103973, mean_q: 0.138353, mean_eps: 0.860370\n",
      " 155435/1000000: episode: 819, duration: 2.021s, episode steps: 201, steps per second:  99, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 0.000224, mae: 0.105156, mean_q: 0.141083, mean_eps: 0.860199\n",
      " 155658/1000000: episode: 820, duration: 2.252s, episode steps: 223, steps per second:  99, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.601 [0.000, 3.000],  loss: 0.000448, mae: 0.108347, mean_q: 0.146023, mean_eps: 0.860009\n",
      " 155790/1000000: episode: 821, duration: 1.344s, episode steps: 132, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.477 [0.000, 3.000],  loss: 0.000238, mae: 0.108151, mean_q: 0.145674, mean_eps: 0.859848\n",
      " 156077/1000000: episode: 822, duration: 3.067s, episode steps: 287, steps per second:  94, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.000164, mae: 0.105116, mean_q: 0.140950, mean_eps: 0.859659\n",
      " 156267/1000000: episode: 823, duration: 1.918s, episode steps: 190, steps per second:  99, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.311 [0.000, 3.000],  loss: 0.000244, mae: 0.116091, mean_q: 0.155925, mean_eps: 0.859445\n",
      " 156455/1000000: episode: 824, duration: 1.885s, episode steps: 188, steps per second: 100, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: 0.000353, mae: 0.106678, mean_q: 0.142465, mean_eps: 0.859276\n",
      " 156717/1000000: episode: 825, duration: 2.681s, episode steps: 262, steps per second:  98, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.000238, mae: 0.108134, mean_q: 0.145159, mean_eps: 0.859073\n",
      " 156975/1000000: episode: 826, duration: 2.563s, episode steps: 258, steps per second: 101, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.395 [0.000, 3.000],  loss: 0.000373, mae: 0.105457, mean_q: 0.141103, mean_eps: 0.858839\n",
      " 157172/1000000: episode: 827, duration: 2.008s, episode steps: 197, steps per second:  98, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 0.000245, mae: 0.116376, mean_q: 0.154924, mean_eps: 0.858635\n",
      " 157338/1000000: episode: 828, duration: 1.715s, episode steps: 166, steps per second:  97, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.548 [0.000, 3.000],  loss: 0.000240, mae: 0.117589, mean_q: 0.157640, mean_eps: 0.858471\n",
      " 157530/1000000: episode: 829, duration: 2.014s, episode steps: 192, steps per second:  95, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: 0.000282, mae: 0.112649, mean_q: 0.150064, mean_eps: 0.858309\n",
      " 157736/1000000: episode: 830, duration: 2.093s, episode steps: 206, steps per second:  98, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 0.000123, mae: 0.096326, mean_q: 0.129112, mean_eps: 0.858131\n",
      " 157994/1000000: episode: 831, duration: 2.602s, episode steps: 258, steps per second:  99, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 0.000240, mae: 0.120201, mean_q: 0.160736, mean_eps: 0.857922\n",
      " 158149/1000000: episode: 832, duration: 1.588s, episode steps: 155, steps per second:  98, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 0.000198, mae: 0.108140, mean_q: 0.144691, mean_eps: 0.857735\n",
      " 158326/1000000: episode: 833, duration: 1.815s, episode steps: 177, steps per second:  97, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.429 [0.000, 3.000],  loss: 0.000319, mae: 0.108224, mean_q: 0.144626, mean_eps: 0.857586\n",
      " 158531/1000000: episode: 834, duration: 2.204s, episode steps: 205, steps per second:  93, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: 0.000173, mae: 0.104470, mean_q: 0.139774, mean_eps: 0.857415\n",
      " 158764/1000000: episode: 835, duration: 2.438s, episode steps: 233, steps per second:  96, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.421 [0.000, 3.000],  loss: 0.000148, mae: 0.106090, mean_q: 0.142056, mean_eps: 0.857219\n",
      " 158902/1000000: episode: 836, duration: 1.472s, episode steps: 138, steps per second:  94, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.464 [0.000, 3.000],  loss: 0.000238, mae: 0.110956, mean_q: 0.147755, mean_eps: 0.857051\n",
      " 159109/1000000: episode: 837, duration: 2.165s, episode steps: 207, steps per second:  96, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.454 [0.000, 3.000],  loss: 0.000215, mae: 0.107512, mean_q: 0.143381, mean_eps: 0.856895\n",
      " 159342/1000000: episode: 838, duration: 2.323s, episode steps: 233, steps per second: 100, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.365 [0.000, 3.000],  loss: 0.000178, mae: 0.108476, mean_q: 0.145912, mean_eps: 0.856697\n",
      " 159483/1000000: episode: 839, duration: 1.400s, episode steps: 141, steps per second: 101, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.695 [0.000, 3.000],  loss: 0.000499, mae: 0.101166, mean_q: 0.136183, mean_eps: 0.856529\n",
      " 159758/1000000: episode: 840, duration: 2.770s, episode steps: 275, steps per second:  99, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.000248, mae: 0.114633, mean_q: 0.153037, mean_eps: 0.856342\n",
      " 160103/1000000: episode: 841, duration: 3.441s, episode steps: 345, steps per second: 100, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.461 [0.000, 3.000],  loss: 0.000901, mae: 0.108894, mean_q: 0.145612, mean_eps: 0.856063\n",
      " 160290/1000000: episode: 842, duration: 1.867s, episode steps: 187, steps per second: 100, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.326 [0.000, 3.000],  loss: 0.001728, mae: 0.110964, mean_q: 0.150837, mean_eps: 0.855824\n",
      " 160441/1000000: episode: 843, duration: 1.636s, episode steps: 151, steps per second:  92, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.430 [0.000, 3.000],  loss: 0.000829, mae: 0.111793, mean_q: 0.149971, mean_eps: 0.855671\n",
      " 160581/1000000: episode: 844, duration: 1.516s, episode steps: 140, steps per second:  92, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.379 [0.000, 3.000],  loss: 0.000653, mae: 0.120472, mean_q: 0.160935, mean_eps: 0.855539\n",
      " 160849/1000000: episode: 845, duration: 2.867s, episode steps: 268, steps per second:  93, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 0.000945, mae: 0.116873, mean_q: 0.154625, mean_eps: 0.855356\n",
      " 160985/1000000: episode: 846, duration: 1.483s, episode steps: 136, steps per second:  92, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.390 [0.000, 3.000],  loss: 0.000527, mae: 0.121251, mean_q: 0.161501, mean_eps: 0.855174\n",
      " 161222/1000000: episode: 847, duration: 2.315s, episode steps: 237, steps per second: 102, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 0.000589, mae: 0.121126, mean_q: 0.161344, mean_eps: 0.855006\n",
      " 161359/1000000: episode: 848, duration: 1.337s, episode steps: 137, steps per second: 102, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.314 [0.000, 3.000],  loss: 0.000306, mae: 0.112763, mean_q: 0.149539, mean_eps: 0.854839\n",
      " 161508/1000000: episode: 849, duration: 1.645s, episode steps: 149, steps per second:  91, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.456 [0.000, 3.000],  loss: 0.000414, mae: 0.114836, mean_q: 0.152652, mean_eps: 0.854711\n",
      " 161693/1000000: episode: 850, duration: 1.869s, episode steps: 185, steps per second:  99, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.573 [0.000, 3.000],  loss: 0.000422, mae: 0.122319, mean_q: 0.162485, mean_eps: 0.854560\n",
      " 161821/1000000: episode: 851, duration: 1.292s, episode steps: 128, steps per second:  99, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.445 [0.000, 3.000],  loss: 0.000269, mae: 0.106099, mean_q: 0.142008, mean_eps: 0.854418\n",
      " 161960/1000000: episode: 852, duration: 1.419s, episode steps: 139, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.604 [0.000, 3.000],  loss: 0.000546, mae: 0.129007, mean_q: 0.171764, mean_eps: 0.854299\n",
      " 162105/1000000: episode: 853, duration: 1.516s, episode steps: 145, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.593 [0.000, 3.000],  loss: 0.000238, mae: 0.120645, mean_q: 0.161023, mean_eps: 0.854171\n",
      " 162246/1000000: episode: 854, duration: 1.471s, episode steps: 141, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.468 [0.000, 3.000],  loss: 0.000236, mae: 0.110858, mean_q: 0.147609, mean_eps: 0.854042\n",
      " 162423/1000000: episode: 855, duration: 1.735s, episode steps: 177, steps per second: 102, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.655 [0.000, 3.000],  loss: 0.000282, mae: 0.112085, mean_q: 0.149677, mean_eps: 0.853899\n",
      " 162556/1000000: episode: 856, duration: 1.338s, episode steps: 133, steps per second:  99, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.511 [0.000, 3.000],  loss: 0.000336, mae: 0.107990, mean_q: 0.144346, mean_eps: 0.853761\n",
      " 162763/1000000: episode: 857, duration: 2.114s, episode steps: 207, steps per second:  98, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 0.000213, mae: 0.106057, mean_q: 0.141798, mean_eps: 0.853608\n",
      " 162966/1000000: episode: 858, duration: 2.006s, episode steps: 203, steps per second: 101, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.690 [0.000, 3.000],  loss: 0.000183, mae: 0.104034, mean_q: 0.139020, mean_eps: 0.853422\n",
      " 163149/1000000: episode: 859, duration: 1.778s, episode steps: 183, steps per second: 103, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.257 [0.000, 3.000],  loss: 0.000449, mae: 0.113479, mean_q: 0.153068, mean_eps: 0.853248\n",
      " 163379/1000000: episode: 860, duration: 2.253s, episode steps: 230, steps per second: 102, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.465 [0.000, 3.000],  loss: 0.000380, mae: 0.116817, mean_q: 0.155758, mean_eps: 0.853062\n",
      " 163517/1000000: episode: 861, duration: 1.403s, episode steps: 138, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.478 [0.000, 3.000],  loss: 0.000230, mae: 0.119605, mean_q: 0.160079, mean_eps: 0.852897\n",
      " 163760/1000000: episode: 862, duration: 2.653s, episode steps: 243, steps per second:  92, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.000256, mae: 0.114904, mean_q: 0.153635, mean_eps: 0.852726\n",
      " 163902/1000000: episode: 863, duration: 1.526s, episode steps: 142, steps per second:  93, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.486 [0.000, 3.000],  loss: 0.000206, mae: 0.121016, mean_q: 0.161591, mean_eps: 0.852553\n",
      " 164110/1000000: episode: 864, duration: 2.138s, episode steps: 208, steps per second:  97, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.317 [0.000, 3.000],  loss: 0.000224, mae: 0.121147, mean_q: 0.160895, mean_eps: 0.852395\n",
      " 164328/1000000: episode: 865, duration: 2.254s, episode steps: 218, steps per second:  97, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.000208, mae: 0.114366, mean_q: 0.153179, mean_eps: 0.852204\n",
      " 164518/1000000: episode: 866, duration: 1.977s, episode steps: 190, steps per second:  96, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.789 [0.000, 3.000],  loss: 0.000201, mae: 0.105949, mean_q: 0.141285, mean_eps: 0.852020\n",
      " 164704/1000000: episode: 867, duration: 2.120s, episode steps: 186, steps per second:  88, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.392 [0.000, 3.000],  loss: 0.000346, mae: 0.116254, mean_q: 0.155600, mean_eps: 0.851851\n",
      " 164877/1000000: episode: 868, duration: 1.901s, episode steps: 173, steps per second:  91, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.422 [0.000, 3.000],  loss: 0.000290, mae: 0.115417, mean_q: 0.154528, mean_eps: 0.851689\n",
      " 165050/1000000: episode: 869, duration: 1.814s, episode steps: 173, steps per second:  95, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.000135, mae: 0.107955, mean_q: 0.145124, mean_eps: 0.851532\n",
      " 165273/1000000: episode: 870, duration: 2.470s, episode steps: 223, steps per second:  90, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.408 [0.000, 3.000],  loss: 0.000149, mae: 0.115503, mean_q: 0.154168, mean_eps: 0.851354\n",
      " 165427/1000000: episode: 871, duration: 1.595s, episode steps: 154, steps per second:  97, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.435 [0.000, 3.000],  loss: 0.000174, mae: 0.120056, mean_q: 0.161192, mean_eps: 0.851185\n",
      " 165593/1000000: episode: 872, duration: 1.776s, episode steps: 166, steps per second:  93, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.000233, mae: 0.108510, mean_q: 0.145405, mean_eps: 0.851041\n",
      " 165729/1000000: episode: 873, duration: 1.420s, episode steps: 136, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.331 [0.000, 3.000],  loss: 0.000223, mae: 0.106043, mean_q: 0.141705, mean_eps: 0.850904\n",
      " 165981/1000000: episode: 874, duration: 2.635s, episode steps: 252, steps per second:  96, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000119, mae: 0.108693, mean_q: 0.145846, mean_eps: 0.850730\n",
      " 166326/1000000: episode: 875, duration: 3.555s, episode steps: 345, steps per second:  97, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.000174, mae: 0.114026, mean_q: 0.152818, mean_eps: 0.850461\n",
      " 166466/1000000: episode: 876, duration: 1.447s, episode steps: 140, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.564 [0.000, 3.000],  loss: 0.000353, mae: 0.111243, mean_q: 0.149610, mean_eps: 0.850244\n",
      " 166701/1000000: episode: 877, duration: 2.387s, episode steps: 235, steps per second:  98, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 0.000182, mae: 0.121391, mean_q: 0.163025, mean_eps: 0.850074\n",
      " 166836/1000000: episode: 878, duration: 1.485s, episode steps: 135, steps per second:  91, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.585 [0.000, 3.000],  loss: 0.000442, mae: 0.121169, mean_q: 0.161372, mean_eps: 0.849909\n",
      " 167052/1000000: episode: 879, duration: 2.302s, episode steps: 216, steps per second:  94, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.597 [0.000, 3.000],  loss: 0.000196, mae: 0.118302, mean_q: 0.157702, mean_eps: 0.849752\n",
      " 167355/1000000: episode: 880, duration: 3.214s, episode steps: 303, steps per second:  94, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.508 [0.000, 3.000],  loss: 0.000185, mae: 0.111799, mean_q: 0.149224, mean_eps: 0.849518\n",
      " 167665/1000000: episode: 881, duration: 3.203s, episode steps: 310, steps per second:  97, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.000162, mae: 0.106100, mean_q: 0.141917, mean_eps: 0.849241\n",
      " 167857/1000000: episode: 882, duration: 1.928s, episode steps: 192, steps per second: 100, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.594 [0.000, 3.000],  loss: 0.000190, mae: 0.124076, mean_q: 0.166794, mean_eps: 0.849014\n",
      " 168068/1000000: episode: 883, duration: 2.122s, episode steps: 211, steps per second:  99, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 0.000466, mae: 0.119147, mean_q: 0.159924, mean_eps: 0.848834\n",
      " 168201/1000000: episode: 884, duration: 1.430s, episode steps: 133, steps per second:  93, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.549 [0.000, 3.000],  loss: 0.000211, mae: 0.104553, mean_q: 0.140660, mean_eps: 0.848679\n",
      " 168362/1000000: episode: 885, duration: 1.676s, episode steps: 161, steps per second:  96, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.000227, mae: 0.124808, mean_q: 0.167404, mean_eps: 0.848546\n",
      " 168590/1000000: episode: 886, duration: 2.242s, episode steps: 228, steps per second: 102, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 0.000221, mae: 0.110697, mean_q: 0.147928, mean_eps: 0.848372\n",
      " 168883/1000000: episode: 887, duration: 2.878s, episode steps: 293, steps per second: 102, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.000213, mae: 0.112087, mean_q: 0.150019, mean_eps: 0.848138\n",
      " 169020/1000000: episode: 888, duration: 1.378s, episode steps: 137, steps per second:  99, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.292 [0.000, 3.000],  loss: 0.000179, mae: 0.111868, mean_q: 0.149682, mean_eps: 0.847945\n",
      " 169232/1000000: episode: 889, duration: 2.123s, episode steps: 212, steps per second: 100, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 0.000225, mae: 0.124122, mean_q: 0.166053, mean_eps: 0.847788\n",
      " 169413/1000000: episode: 890, duration: 1.804s, episode steps: 181, steps per second: 100, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.497 [0.000, 3.000],  loss: 0.000204, mae: 0.107626, mean_q: 0.143767, mean_eps: 0.847610\n",
      " 169665/1000000: episode: 891, duration: 2.524s, episode steps: 252, steps per second: 100, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.492 [0.000, 3.000],  loss: 0.000399, mae: 0.117344, mean_q: 0.158633, mean_eps: 0.847414\n",
      " 169895/1000000: episode: 892, duration: 2.390s, episode steps: 230, steps per second:  96, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 0.000168, mae: 0.113533, mean_q: 0.151741, mean_eps: 0.847198\n",
      " 170028/1000000: episode: 893, duration: 1.403s, episode steps: 133, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.233 [0.000, 3.000],  loss: 0.000666, mae: 0.113894, mean_q: 0.151398, mean_eps: 0.847036\n",
      " 170165/1000000: episode: 894, duration: 1.423s, episode steps: 137, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.591 [0.000, 3.000],  loss: 0.000922, mae: 0.118681, mean_q: 0.160826, mean_eps: 0.846914\n",
      " 170371/1000000: episode: 895, duration: 2.109s, episode steps: 206, steps per second:  98, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.461 [0.000, 3.000],  loss: 0.001740, mae: 0.125916, mean_q: 0.170476, mean_eps: 0.846759\n",
      " 170597/1000000: episode: 896, duration: 2.336s, episode steps: 226, steps per second:  97, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.000871, mae: 0.125544, mean_q: 0.167137, mean_eps: 0.846564\n",
      " 170733/1000000: episode: 897, duration: 1.410s, episode steps: 136, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.463 [0.000, 3.000],  loss: 0.000880, mae: 0.113070, mean_q: 0.151613, mean_eps: 0.846401\n",
      " 170955/1000000: episode: 898, duration: 2.302s, episode steps: 222, steps per second:  96, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: 0.000927, mae: 0.126088, mean_q: 0.168205, mean_eps: 0.846240\n",
      " 171150/1000000: episode: 899, duration: 2.047s, episode steps: 195, steps per second:  95, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.000992, mae: 0.120752, mean_q: 0.161652, mean_eps: 0.846053\n",
      " 171365/1000000: episode: 900, duration: 2.293s, episode steps: 215, steps per second:  94, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.572 [0.000, 3.000],  loss: 0.000479, mae: 0.121684, mean_q: 0.163923, mean_eps: 0.845868\n",
      " 171523/1000000: episode: 901, duration: 1.673s, episode steps: 158, steps per second:  94, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.690 [0.000, 3.000],  loss: 0.000610, mae: 0.126179, mean_q: 0.169111, mean_eps: 0.845700\n",
      " 171709/1000000: episode: 902, duration: 1.969s, episode steps: 186, steps per second:  94, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.349 [0.000, 3.000],  loss: 0.000571, mae: 0.123706, mean_q: 0.165399, mean_eps: 0.845546\n",
      " 171925/1000000: episode: 903, duration: 2.210s, episode steps: 216, steps per second:  98, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: 0.000412, mae: 0.117510, mean_q: 0.157742, mean_eps: 0.845364\n",
      " 172059/1000000: episode: 904, duration: 1.342s, episode steps: 134, steps per second: 100, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.545 [0.000, 3.000],  loss: 0.000897, mae: 0.120927, mean_q: 0.162342, mean_eps: 0.845207\n",
      " 172238/1000000: episode: 905, duration: 1.845s, episode steps: 179, steps per second:  97, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.453 [0.000, 3.000],  loss: 0.000388, mae: 0.121228, mean_q: 0.162438, mean_eps: 0.845067\n",
      " 172410/1000000: episode: 906, duration: 1.760s, episode steps: 172, steps per second:  98, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.750 [0.000, 3.000],  loss: 0.000665, mae: 0.112729, mean_q: 0.152186, mean_eps: 0.844908\n",
      " 172592/1000000: episode: 907, duration: 1.869s, episode steps: 182, steps per second:  97, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: 0.000506, mae: 0.121212, mean_q: 0.163044, mean_eps: 0.844750\n",
      " 172874/1000000: episode: 908, duration: 2.987s, episode steps: 282, steps per second:  94, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.674 [0.000, 3.000],  loss: 0.000449, mae: 0.126404, mean_q: 0.169130, mean_eps: 0.844541\n",
      " 173004/1000000: episode: 909, duration: 1.344s, episode steps: 130, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.000406, mae: 0.121022, mean_q: 0.160965, mean_eps: 0.844356\n",
      " 173171/1000000: episode: 910, duration: 1.719s, episode steps: 167, steps per second:  97, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 0.000250, mae: 0.116559, mean_q: 0.155926, mean_eps: 0.844223\n",
      " 173298/1000000: episode: 911, duration: 1.273s, episode steps: 127, steps per second: 100, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.551 [0.000, 3.000],  loss: 0.000259, mae: 0.127908, mean_q: 0.170298, mean_eps: 0.844089\n",
      " 173603/1000000: episode: 912, duration: 2.978s, episode steps: 305, steps per second: 102, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.656 [0.000, 3.000],  loss: 0.000323, mae: 0.123843, mean_q: 0.166795, mean_eps: 0.843895\n",
      " 173791/1000000: episode: 913, duration: 1.912s, episode steps: 188, steps per second:  98, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.691 [0.000, 3.000],  loss: 0.000245, mae: 0.120361, mean_q: 0.160544, mean_eps: 0.843674\n",
      " 173947/1000000: episode: 914, duration: 1.587s, episode steps: 156, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.468 [0.000, 3.000],  loss: 0.000310, mae: 0.126288, mean_q: 0.167995, mean_eps: 0.843519\n",
      " 174218/1000000: episode: 915, duration: 2.647s, episode steps: 271, steps per second: 102, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: 0.000202, mae: 0.117696, mean_q: 0.157706, mean_eps: 0.843326\n",
      " 174443/1000000: episode: 916, duration: 2.286s, episode steps: 225, steps per second:  98, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.391 [0.000, 3.000],  loss: 0.000251, mae: 0.116882, mean_q: 0.157074, mean_eps: 0.843103\n",
      " 174677/1000000: episode: 917, duration: 2.630s, episode steps: 234, steps per second:  89, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 0.000220, mae: 0.122862, mean_q: 0.164783, mean_eps: 0.842896\n",
      " 174986/1000000: episode: 918, duration: 3.069s, episode steps: 309, steps per second: 101, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.631 [0.000, 3.000],  loss: 0.000247, mae: 0.117468, mean_q: 0.156879, mean_eps: 0.842651\n",
      " 175181/1000000: episode: 919, duration: 1.889s, episode steps: 195, steps per second: 103, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.000202, mae: 0.127039, mean_q: 0.170019, mean_eps: 0.842424\n",
      " 175420/1000000: episode: 920, duration: 2.391s, episode steps: 239, steps per second: 100, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.000196, mae: 0.124509, mean_q: 0.166594, mean_eps: 0.842230\n",
      " 175686/1000000: episode: 921, duration: 2.627s, episode steps: 266, steps per second: 101, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.383 [0.000, 3.000],  loss: 0.000320, mae: 0.121977, mean_q: 0.162181, mean_eps: 0.842003\n",
      " 175872/1000000: episode: 922, duration: 1.795s, episode steps: 186, steps per second: 104, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.387 [0.000, 3.000],  loss: 0.000221, mae: 0.122289, mean_q: 0.162799, mean_eps: 0.841800\n",
      " 176016/1000000: episode: 923, duration: 1.520s, episode steps: 144, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.472 [0.000, 3.000],  loss: 0.000612, mae: 0.121840, mean_q: 0.162575, mean_eps: 0.841652\n",
      " 176157/1000000: episode: 924, duration: 1.527s, episode steps: 141, steps per second:  92, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.546 [0.000, 3.000],  loss: 0.000115, mae: 0.110525, mean_q: 0.149319, mean_eps: 0.841523\n",
      " 176355/1000000: episode: 925, duration: 2.303s, episode steps: 198, steps per second:  86, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.000445, mae: 0.124092, mean_q: 0.165343, mean_eps: 0.841370\n",
      " 176642/1000000: episode: 926, duration: 3.097s, episode steps: 287, steps per second:  93, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: 0.000376, mae: 0.117408, mean_q: 0.157161, mean_eps: 0.841152\n",
      " 176898/1000000: episode: 927, duration: 2.569s, episode steps: 256, steps per second: 100, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.441 [0.000, 3.000],  loss: 0.000229, mae: 0.118065, mean_q: 0.158071, mean_eps: 0.840907\n",
      " 177048/1000000: episode: 928, duration: 1.483s, episode steps: 150, steps per second: 101, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.627 [0.000, 3.000],  loss: 0.000249, mae: 0.132395, mean_q: 0.177763, mean_eps: 0.840725\n",
      " 177350/1000000: episode: 929, duration: 2.999s, episode steps: 302, steps per second: 101, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: 0.000236, mae: 0.128432, mean_q: 0.171908, mean_eps: 0.840522\n",
      " 177602/1000000: episode: 930, duration: 2.583s, episode steps: 252, steps per second:  98, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.385 [0.000, 3.000],  loss: 0.000243, mae: 0.124621, mean_q: 0.165923, mean_eps: 0.840272\n",
      " 177758/1000000: episode: 931, duration: 1.629s, episode steps: 156, steps per second:  96, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.487 [0.000, 3.000],  loss: 0.000231, mae: 0.114796, mean_q: 0.153240, mean_eps: 0.840088\n",
      " 177997/1000000: episode: 932, duration: 2.364s, episode steps: 239, steps per second: 101, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.397 [0.000, 3.000],  loss: 0.000187, mae: 0.124959, mean_q: 0.166908, mean_eps: 0.839910\n",
      " 178138/1000000: episode: 933, duration: 1.410s, episode steps: 141, steps per second: 100, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.482 [0.000, 3.000],  loss: 0.000174, mae: 0.118757, mean_q: 0.158983, mean_eps: 0.839739\n",
      " 178482/1000000: episode: 934, duration: 3.381s, episode steps: 344, steps per second: 102, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.387 [0.000, 3.000],  loss: 0.000153, mae: 0.114092, mean_q: 0.152638, mean_eps: 0.839521\n",
      " 178691/1000000: episode: 935, duration: 2.027s, episode steps: 209, steps per second: 103, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 0.000161, mae: 0.129150, mean_q: 0.172842, mean_eps: 0.839273\n",
      " 178975/1000000: episode: 936, duration: 2.832s, episode steps: 284, steps per second: 100, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: 0.000191, mae: 0.120898, mean_q: 0.161999, mean_eps: 0.839051\n",
      " 179182/1000000: episode: 937, duration: 2.131s, episode steps: 207, steps per second:  97, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.348 [0.000, 3.000],  loss: 0.000176, mae: 0.112509, mean_q: 0.150487, mean_eps: 0.838830\n",
      " 179413/1000000: episode: 938, duration: 2.366s, episode steps: 231, steps per second:  98, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.450 [0.000, 3.000],  loss: 0.000344, mae: 0.127607, mean_q: 0.170355, mean_eps: 0.838632\n",
      " 179701/1000000: episode: 939, duration: 2.802s, episode steps: 288, steps per second: 103, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 0.000219, mae: 0.128477, mean_q: 0.171211, mean_eps: 0.838398\n",
      " 179866/1000000: episode: 940, duration: 1.607s, episode steps: 165, steps per second: 103, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.358 [0.000, 3.000],  loss: 0.000172, mae: 0.115724, mean_q: 0.154904, mean_eps: 0.838194\n",
      " 180040/1000000: episode: 941, duration: 1.763s, episode steps: 174, steps per second:  99, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.316 [0.000, 3.000],  loss: 0.001113, mae: 0.123964, mean_q: 0.164475, mean_eps: 0.838043\n",
      " 180215/1000000: episode: 942, duration: 2.075s, episode steps: 175, steps per second:  84, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.406 [0.000, 3.000],  loss: 0.001585, mae: 0.125010, mean_q: 0.168630, mean_eps: 0.837887\n",
      " 180424/1000000: episode: 943, duration: 2.329s, episode steps: 209, steps per second:  90, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.001485, mae: 0.132981, mean_q: 0.180086, mean_eps: 0.837714\n",
      " 180570/1000000: episode: 944, duration: 1.689s, episode steps: 146, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.582 [0.000, 3.000],  loss: 0.000883, mae: 0.130507, mean_q: 0.173672, mean_eps: 0.837554\n",
      " 180747/1000000: episode: 945, duration: 1.827s, episode steps: 177, steps per second:  97, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.001019, mae: 0.120865, mean_q: 0.160125, mean_eps: 0.837408\n",
      " 180950/1000000: episode: 946, duration: 2.062s, episode steps: 203, steps per second:  98, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.414 [0.000, 3.000],  loss: 0.000568, mae: 0.132317, mean_q: 0.176642, mean_eps: 0.837237\n",
      " 181080/1000000: episode: 947, duration: 1.311s, episode steps: 130, steps per second:  99, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.485 [0.000, 3.000],  loss: 0.000677, mae: 0.128102, mean_q: 0.171071, mean_eps: 0.837087\n",
      " 181217/1000000: episode: 948, duration: 1.392s, episode steps: 137, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.438 [0.000, 3.000],  loss: 0.000465, mae: 0.132055, mean_q: 0.177195, mean_eps: 0.836967\n",
      " 181364/1000000: episode: 949, duration: 1.514s, episode steps: 147, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.537 [0.000, 3.000],  loss: 0.000498, mae: 0.127813, mean_q: 0.169696, mean_eps: 0.836839\n",
      " 181520/1000000: episode: 950, duration: 1.563s, episode steps: 156, steps per second: 100, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.660 [0.000, 3.000],  loss: 0.000398, mae: 0.128296, mean_q: 0.170504, mean_eps: 0.836704\n",
      " 181660/1000000: episode: 951, duration: 1.417s, episode steps: 140, steps per second:  99, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.543 [0.000, 3.000],  loss: 0.000872, mae: 0.125854, mean_q: 0.168365, mean_eps: 0.836571\n",
      " 181801/1000000: episode: 952, duration: 1.432s, episode steps: 141, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.447 [0.000, 3.000],  loss: 0.000286, mae: 0.130359, mean_q: 0.174400, mean_eps: 0.836443\n",
      " 181932/1000000: episode: 953, duration: 1.336s, episode steps: 131, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.481 [0.000, 3.000],  loss: 0.000694, mae: 0.127743, mean_q: 0.171527, mean_eps: 0.836321\n",
      " 182141/1000000: episode: 954, duration: 2.073s, episode steps: 209, steps per second: 101, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.411 [0.000, 3.000],  loss: 0.000301, mae: 0.132674, mean_q: 0.176805, mean_eps: 0.836168\n",
      " 182349/1000000: episode: 955, duration: 2.168s, episode steps: 208, steps per second:  96, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 0.000417, mae: 0.131073, mean_q: 0.174420, mean_eps: 0.835979\n",
      " 182530/1000000: episode: 956, duration: 1.803s, episode steps: 181, steps per second: 100, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.000271, mae: 0.129181, mean_q: 0.172758, mean_eps: 0.835804\n",
      " 182761/1000000: episode: 957, duration: 2.242s, episode steps: 231, steps per second: 103, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.290 [0.000, 3.000],  loss: 0.000318, mae: 0.130676, mean_q: 0.173722, mean_eps: 0.835619\n",
      " 182996/1000000: episode: 958, duration: 2.332s, episode steps: 235, steps per second: 101, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.000294, mae: 0.117951, mean_q: 0.157362, mean_eps: 0.835410\n",
      " 183127/1000000: episode: 959, duration: 1.331s, episode steps: 131, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.542 [0.000, 3.000],  loss: 0.000184, mae: 0.128330, mean_q: 0.171526, mean_eps: 0.835246\n",
      " 183344/1000000: episode: 960, duration: 2.226s, episode steps: 217, steps per second:  98, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.387 [0.000, 3.000],  loss: 0.000323, mae: 0.120659, mean_q: 0.162071, mean_eps: 0.835089\n",
      " 183472/1000000: episode: 961, duration: 1.293s, episode steps: 128, steps per second:  99, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.398 [0.000, 3.000],  loss: 0.000163, mae: 0.124073, mean_q: 0.165315, mean_eps: 0.834935\n",
      " 183606/1000000: episode: 962, duration: 1.378s, episode steps: 134, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.396 [0.000, 3.000],  loss: 0.000604, mae: 0.127331, mean_q: 0.170689, mean_eps: 0.834816\n",
      " 183751/1000000: episode: 963, duration: 1.398s, episode steps: 145, steps per second: 104, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.607 [0.000, 3.000],  loss: 0.000250, mae: 0.123278, mean_q: 0.164479, mean_eps: 0.834690\n",
      " 183960/1000000: episode: 964, duration: 2.225s, episode steps: 209, steps per second:  94, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.368 [0.000, 3.000],  loss: 0.000298, mae: 0.120647, mean_q: 0.161367, mean_eps: 0.834531\n",
      " 184182/1000000: episode: 965, duration: 2.334s, episode steps: 222, steps per second:  95, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.396 [0.000, 3.000],  loss: 0.000177, mae: 0.129090, mean_q: 0.172361, mean_eps: 0.834337\n",
      " 184362/1000000: episode: 966, duration: 1.837s, episode steps: 180, steps per second:  98, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 0.000313, mae: 0.127911, mean_q: 0.170354, mean_eps: 0.834155\n",
      " 184517/1000000: episode: 967, duration: 1.597s, episode steps: 155, steps per second:  97, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.000294, mae: 0.127999, mean_q: 0.170661, mean_eps: 0.834004\n",
      " 184668/1000000: episode: 968, duration: 1.694s, episode steps: 151, steps per second:  89, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.503 [0.000, 3.000],  loss: 0.000293, mae: 0.124419, mean_q: 0.166392, mean_eps: 0.833867\n",
      " 184819/1000000: episode: 969, duration: 1.516s, episode steps: 151, steps per second: 100, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.616 [0.000, 3.000],  loss: 0.000229, mae: 0.128212, mean_q: 0.172840, mean_eps: 0.833732\n",
      " 184975/1000000: episode: 970, duration: 1.584s, episode steps: 156, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.519 [0.000, 3.000],  loss: 0.000224, mae: 0.127228, mean_q: 0.169786, mean_eps: 0.833594\n",
      " 185105/1000000: episode: 971, duration: 1.337s, episode steps: 130, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.454 [0.000, 3.000],  loss: 0.000373, mae: 0.127975, mean_q: 0.171629, mean_eps: 0.833464\n",
      " 185288/1000000: episode: 972, duration: 1.872s, episode steps: 183, steps per second:  98, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.000176, mae: 0.130817, mean_q: 0.174339, mean_eps: 0.833324\n",
      " 185489/1000000: episode: 973, duration: 2.055s, episode steps: 201, steps per second:  98, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 0.000297, mae: 0.131087, mean_q: 0.174619, mean_eps: 0.833151\n",
      " 185704/1000000: episode: 974, duration: 2.252s, episode steps: 215, steps per second:  95, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.000287, mae: 0.127991, mean_q: 0.170425, mean_eps: 0.832964\n",
      " 185963/1000000: episode: 975, duration: 2.542s, episode steps: 259, steps per second: 102, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.687 [0.000, 3.000],  loss: 0.000221, mae: 0.128740, mean_q: 0.172152, mean_eps: 0.832751\n",
      " 186135/1000000: episode: 976, duration: 1.724s, episode steps: 172, steps per second: 100, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.407 [0.000, 3.000],  loss: 0.000193, mae: 0.126387, mean_q: 0.168237, mean_eps: 0.832557\n",
      " 186340/1000000: episode: 977, duration: 2.043s, episode steps: 205, steps per second: 100, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.424 [0.000, 3.000],  loss: 0.000276, mae: 0.126629, mean_q: 0.169743, mean_eps: 0.832388\n",
      " 186485/1000000: episode: 978, duration: 1.616s, episode steps: 145, steps per second:  90, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.586 [0.000, 3.000],  loss: 0.000177, mae: 0.128710, mean_q: 0.171446, mean_eps: 0.832229\n",
      " 186613/1000000: episode: 979, duration: 1.336s, episode steps: 128, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.352 [0.000, 3.000],  loss: 0.000172, mae: 0.129704, mean_q: 0.173196, mean_eps: 0.832105\n",
      " 186771/1000000: episode: 980, duration: 1.619s, episode steps: 158, steps per second:  98, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 0.000215, mae: 0.134483, mean_q: 0.179886, mean_eps: 0.831977\n",
      " 186944/1000000: episode: 981, duration: 1.792s, episode steps: 173, steps per second:  97, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.416 [0.000, 3.000],  loss: 0.000244, mae: 0.132807, mean_q: 0.177626, mean_eps: 0.831830\n",
      " 187187/1000000: episode: 982, duration: 2.547s, episode steps: 243, steps per second:  95, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 0.000362, mae: 0.126343, mean_q: 0.168753, mean_eps: 0.831642\n",
      " 187360/1000000: episode: 983, duration: 1.807s, episode steps: 173, steps per second:  96, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.671 [0.000, 3.000],  loss: 0.000198, mae: 0.117138, mean_q: 0.156089, mean_eps: 0.831455\n",
      " 187503/1000000: episode: 984, duration: 1.469s, episode steps: 143, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.601 [0.000, 3.000],  loss: 0.000182, mae: 0.126751, mean_q: 0.169372, mean_eps: 0.831313\n",
      " 187664/1000000: episode: 985, duration: 1.700s, episode steps: 161, steps per second:  95, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.472 [0.000, 3.000],  loss: 0.000285, mae: 0.129507, mean_q: 0.173729, mean_eps: 0.831176\n",
      " 187936/1000000: episode: 986, duration: 2.882s, episode steps: 272, steps per second:  94, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 0.000355, mae: 0.119002, mean_q: 0.158746, mean_eps: 0.830982\n",
      " 188198/1000000: episode: 987, duration: 2.747s, episode steps: 262, steps per second:  95, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 0.000166, mae: 0.123175, mean_q: 0.164191, mean_eps: 0.830741\n",
      " 188395/1000000: episode: 988, duration: 2.029s, episode steps: 197, steps per second:  97, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.340 [0.000, 3.000],  loss: 0.000167, mae: 0.127493, mean_q: 0.170936, mean_eps: 0.830534\n",
      " 188575/1000000: episode: 989, duration: 2.087s, episode steps: 180, steps per second:  86, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 0.000605, mae: 0.121030, mean_q: 0.163361, mean_eps: 0.830364\n",
      " 188749/1000000: episode: 990, duration: 1.803s, episode steps: 174, steps per second:  97, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.471 [0.000, 3.000],  loss: 0.000240, mae: 0.122783, mean_q: 0.164557, mean_eps: 0.830204\n",
      " 188960/1000000: episode: 991, duration: 2.209s, episode steps: 211, steps per second:  96, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.431 [0.000, 3.000],  loss: 0.000186, mae: 0.134777, mean_q: 0.180341, mean_eps: 0.830031\n",
      " 189125/1000000: episode: 992, duration: 1.789s, episode steps: 165, steps per second:  92, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.479 [0.000, 3.000],  loss: 0.000206, mae: 0.125512, mean_q: 0.167171, mean_eps: 0.829862\n",
      " 189258/1000000: episode: 993, duration: 1.409s, episode steps: 133, steps per second:  94, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.406 [0.000, 3.000],  loss: 0.000279, mae: 0.120790, mean_q: 0.160792, mean_eps: 0.829727\n",
      " 189394/1000000: episode: 994, duration: 1.594s, episode steps: 136, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.507 [0.000, 3.000],  loss: 0.000122, mae: 0.130515, mean_q: 0.173996, mean_eps: 0.829607\n",
      " 189650/1000000: episode: 995, duration: 3.285s, episode steps: 256, steps per second:  78, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: 0.000233, mae: 0.129872, mean_q: 0.173803, mean_eps: 0.829430\n",
      " 189840/1000000: episode: 996, duration: 2.497s, episode steps: 190, steps per second:  76, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.553 [0.000, 3.000],  loss: 0.000228, mae: 0.130153, mean_q: 0.174712, mean_eps: 0.829230\n",
      " 190092/1000000: episode: 997, duration: 2.853s, episode steps: 252, steps per second:  88, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.440 [0.000, 3.000],  loss: 0.000439, mae: 0.129243, mean_q: 0.172717, mean_eps: 0.829032\n",
      " 190364/1000000: episode: 998, duration: 2.993s, episode steps: 272, steps per second:  91, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.002124, mae: 0.148396, mean_q: 0.200487, mean_eps: 0.828797\n",
      " 190584/1000000: episode: 999, duration: 2.223s, episode steps: 220, steps per second:  99, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 0.000948, mae: 0.136261, mean_q: 0.182983, mean_eps: 0.828575\n",
      " 190709/1000000: episode: 1000, duration: 1.335s, episode steps: 125, steps per second:  94, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.368 [0.000, 3.000],  loss: 0.001755, mae: 0.138289, mean_q: 0.183208, mean_eps: 0.828419\n",
      " 190966/1000000: episode: 1001, duration: 2.580s, episode steps: 257, steps per second: 100, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.432 [0.000, 3.000],  loss: 0.000880, mae: 0.132635, mean_q: 0.177079, mean_eps: 0.828246\n",
      " 191108/1000000: episode: 1002, duration: 1.435s, episode steps: 142, steps per second:  99, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.585 [0.000, 3.000],  loss: 0.000880, mae: 0.147170, mean_q: 0.197141, mean_eps: 0.828068\n",
      " 191309/1000000: episode: 1003, duration: 1.998s, episode steps: 201, steps per second: 101, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.388 [0.000, 3.000],  loss: 0.000642, mae: 0.127160, mean_q: 0.170209, mean_eps: 0.827913\n",
      " 191468/1000000: episode: 1004, duration: 1.582s, episode steps: 159, steps per second: 100, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.327 [0.000, 3.000],  loss: 0.001100, mae: 0.133592, mean_q: 0.177251, mean_eps: 0.827751\n",
      " 191611/1000000: episode: 1005, duration: 1.450s, episode steps: 143, steps per second:  99, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.420 [0.000, 3.000],  loss: 0.000823, mae: 0.135445, mean_q: 0.181149, mean_eps: 0.827616\n",
      " 191849/1000000: episode: 1006, duration: 2.533s, episode steps: 238, steps per second:  94, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 0.000640, mae: 0.140739, mean_q: 0.189389, mean_eps: 0.827443\n",
      " 192039/1000000: episode: 1007, duration: 1.906s, episode steps: 190, steps per second: 100, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.600 [0.000, 3.000],  loss: 0.001146, mae: 0.140827, mean_q: 0.188651, mean_eps: 0.827250\n",
      " 192269/1000000: episode: 1008, duration: 2.348s, episode steps: 230, steps per second:  98, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.352 [0.000, 3.000],  loss: 0.000472, mae: 0.127536, mean_q: 0.169537, mean_eps: 0.827061\n",
      " 192453/1000000: episode: 1009, duration: 1.864s, episode steps: 184, steps per second:  99, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.000419, mae: 0.135456, mean_q: 0.180329, mean_eps: 0.826874\n",
      " 192580/1000000: episode: 1010, duration: 1.291s, episode steps: 127, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.315 [0.000, 3.000],  loss: 0.000508, mae: 0.130780, mean_q: 0.173730, mean_eps: 0.826736\n",
      " 192864/1000000: episode: 1011, duration: 2.804s, episode steps: 284, steps per second: 101, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.479 [0.000, 3.000],  loss: 0.000376, mae: 0.134473, mean_q: 0.178623, mean_eps: 0.826552\n",
      " 193118/1000000: episode: 1012, duration: 2.628s, episode steps: 254, steps per second:  97, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.681 [0.000, 3.000],  loss: 0.000481, mae: 0.139464, mean_q: 0.186095, mean_eps: 0.826309\n",
      " 193296/1000000: episode: 1013, duration: 1.831s, episode steps: 178, steps per second:  97, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.747 [0.000, 3.000],  loss: 0.000646, mae: 0.143112, mean_q: 0.191702, mean_eps: 0.826115\n",
      " 193451/1000000: episode: 1014, duration: 1.582s, episode steps: 155, steps per second:  98, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: 0.000267, mae: 0.134361, mean_q: 0.180159, mean_eps: 0.825965\n",
      " 193626/1000000: episode: 1015, duration: 1.722s, episode steps: 175, steps per second: 102, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.497 [0.000, 3.000],  loss: 0.000285, mae: 0.137356, mean_q: 0.183627, mean_eps: 0.825816\n",
      " 193807/1000000: episode: 1016, duration: 1.888s, episode steps: 181, steps per second:  96, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.613 [0.000, 3.000],  loss: 0.000359, mae: 0.131574, mean_q: 0.176294, mean_eps: 0.825656\n",
      " 193957/1000000: episode: 1017, duration: 1.548s, episode steps: 150, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000394, mae: 0.141678, mean_q: 0.188304, mean_eps: 0.825506\n",
      " 194103/1000000: episode: 1018, duration: 1.428s, episode steps: 146, steps per second: 102, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.329 [0.000, 3.000],  loss: 0.000640, mae: 0.155340, mean_q: 0.207420, mean_eps: 0.825373\n",
      " 194288/1000000: episode: 1019, duration: 1.898s, episode steps: 185, steps per second:  97, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.497 [0.000, 3.000],  loss: 0.000313, mae: 0.134797, mean_q: 0.181074, mean_eps: 0.825225\n",
      " 194547/1000000: episode: 1020, duration: 2.658s, episode steps: 259, steps per second:  97, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.456 [0.000, 3.000],  loss: 0.000290, mae: 0.137175, mean_q: 0.183163, mean_eps: 0.825026\n",
      " 194700/1000000: episode: 1021, duration: 1.668s, episode steps: 153, steps per second:  92, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.340 [0.000, 3.000],  loss: 0.000288, mae: 0.134899, mean_q: 0.179791, mean_eps: 0.824840\n",
      " 194948/1000000: episode: 1022, duration: 2.507s, episode steps: 248, steps per second:  99, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.351 [0.000, 3.000],  loss: 0.000448, mae: 0.136027, mean_q: 0.181484, mean_eps: 0.824660\n",
      " 195199/1000000: episode: 1023, duration: 2.589s, episode steps: 251, steps per second:  97, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.311 [0.000, 3.000],  loss: 0.000552, mae: 0.138419, mean_q: 0.185770, mean_eps: 0.824435\n",
      " 195337/1000000: episode: 1024, duration: 1.426s, episode steps: 138, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.674 [0.000, 3.000],  loss: 0.000491, mae: 0.133444, mean_q: 0.177402, mean_eps: 0.824259\n",
      " 195536/1000000: episode: 1025, duration: 2.047s, episode steps: 199, steps per second:  97, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.613 [0.000, 3.000],  loss: 0.000272, mae: 0.145730, mean_q: 0.195463, mean_eps: 0.824108\n",
      " 195681/1000000: episode: 1026, duration: 1.526s, episode steps: 145, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.448 [0.000, 3.000],  loss: 0.000280, mae: 0.131517, mean_q: 0.175425, mean_eps: 0.823953\n",
      " 195884/1000000: episode: 1027, duration: 2.486s, episode steps: 203, steps per second:  82, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.650 [0.000, 3.000],  loss: 0.000278, mae: 0.136669, mean_q: 0.182743, mean_eps: 0.823796\n",
      " 196129/1000000: episode: 1028, duration: 2.762s, episode steps: 245, steps per second:  89, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.000248, mae: 0.132997, mean_q: 0.177857, mean_eps: 0.823595\n",
      " 196420/1000000: episode: 1029, duration: 3.089s, episode steps: 291, steps per second:  94, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: 0.000250, mae: 0.135019, mean_q: 0.180924, mean_eps: 0.823353\n",
      " 196549/1000000: episode: 1030, duration: 1.319s, episode steps: 129, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.457 [0.000, 3.000],  loss: 0.000246, mae: 0.133240, mean_q: 0.177710, mean_eps: 0.823164\n",
      " 196681/1000000: episode: 1031, duration: 1.339s, episode steps: 132, steps per second:  99, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.386 [0.000, 3.000],  loss: 0.000221, mae: 0.137568, mean_q: 0.183568, mean_eps: 0.823046\n",
      " 196940/1000000: episode: 1032, duration: 2.569s, episode steps: 259, steps per second: 101, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 0.000227, mae: 0.134848, mean_q: 0.180226, mean_eps: 0.822871\n",
      " 197144/1000000: episode: 1033, duration: 2.016s, episode steps: 204, steps per second: 101, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.426 [0.000, 3.000],  loss: 0.000246, mae: 0.132465, mean_q: 0.177058, mean_eps: 0.822664\n",
      " 197282/1000000: episode: 1034, duration: 1.374s, episode steps: 138, steps per second: 100, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.522 [0.000, 3.000],  loss: 0.000178, mae: 0.126774, mean_q: 0.170161, mean_eps: 0.822509\n",
      " 197419/1000000: episode: 1035, duration: 1.350s, episode steps: 137, steps per second: 101, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.431 [0.000, 3.000],  loss: 0.000217, mae: 0.132211, mean_q: 0.176067, mean_eps: 0.822385\n",
      " 197557/1000000: episode: 1036, duration: 1.410s, episode steps: 138, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.630 [0.000, 3.000],  loss: 0.000367, mae: 0.130451, mean_q: 0.174958, mean_eps: 0.822261\n",
      " 197766/1000000: episode: 1037, duration: 2.151s, episode steps: 209, steps per second:  97, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.000258, mae: 0.139458, mean_q: 0.186308, mean_eps: 0.822104\n",
      " 198009/1000000: episode: 1038, duration: 2.369s, episode steps: 243, steps per second: 103, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.000253, mae: 0.138616, mean_q: 0.185554, mean_eps: 0.821901\n",
      " 198187/1000000: episode: 1039, duration: 1.729s, episode steps: 178, steps per second: 103, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.421 [0.000, 3.000],  loss: 0.000218, mae: 0.142225, mean_q: 0.190345, mean_eps: 0.821712\n",
      " 198403/1000000: episode: 1040, duration: 2.109s, episode steps: 216, steps per second: 102, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.458 [0.000, 3.000],  loss: 0.000128, mae: 0.136541, mean_q: 0.182488, mean_eps: 0.821535\n",
      " 198629/1000000: episode: 1041, duration: 2.276s, episode steps: 226, steps per second:  99, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.363 [0.000, 3.000],  loss: 0.000142, mae: 0.128159, mean_q: 0.171669, mean_eps: 0.821336\n",
      " 198780/1000000: episode: 1042, duration: 1.524s, episode steps: 151, steps per second:  99, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.662 [0.000, 3.000],  loss: 0.000274, mae: 0.152858, mean_q: 0.205070, mean_eps: 0.821166\n",
      " 199090/1000000: episode: 1043, duration: 3.070s, episode steps: 310, steps per second: 101, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.329 [0.000, 3.000],  loss: 0.000258, mae: 0.136115, mean_q: 0.182110, mean_eps: 0.820959\n",
      " 199278/1000000: episode: 1044, duration: 1.930s, episode steps: 188, steps per second:  97, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.473 [0.000, 3.000],  loss: 0.000129, mae: 0.131998, mean_q: 0.176752, mean_eps: 0.820734\n",
      " 199431/1000000: episode: 1045, duration: 1.605s, episode steps: 153, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.405 [0.000, 3.000],  loss: 0.000308, mae: 0.137919, mean_q: 0.183775, mean_eps: 0.820581\n",
      " 199667/1000000: episode: 1046, duration: 2.377s, episode steps: 236, steps per second:  99, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.000262, mae: 0.144321, mean_q: 0.192738, mean_eps: 0.820407\n",
      " 199854/1000000: episode: 1047, duration: 1.860s, episode steps: 187, steps per second: 101, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.000126, mae: 0.146351, mean_q: 0.195727, mean_eps: 0.820216\n",
      " 200117/1000000: episode: 1048, duration: 2.637s, episode steps: 263, steps per second: 100, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.422 [0.000, 3.000],  loss: 0.000650, mae: 0.144052, mean_q: 0.191926, mean_eps: 0.820013\n",
      " 200292/1000000: episode: 1049, duration: 1.760s, episode steps: 175, steps per second:  99, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.394 [0.000, 3.000],  loss: 0.002292, mae: 0.147874, mean_q: 0.198185, mean_eps: 0.819816\n",
      " 200425/1000000: episode: 1050, duration: 1.359s, episode steps: 133, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.436 [0.000, 3.000],  loss: 0.001322, mae: 0.140621, mean_q: 0.187666, mean_eps: 0.819678\n",
      " 200566/1000000: episode: 1051, duration: 1.417s, episode steps: 141, steps per second:  99, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.411 [0.000, 3.000],  loss: 0.001595, mae: 0.141782, mean_q: 0.191338, mean_eps: 0.819554\n",
      " 200694/1000000: episode: 1052, duration: 1.341s, episode steps: 128, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.453 [0.000, 3.000],  loss: 0.001128, mae: 0.139372, mean_q: 0.187740, mean_eps: 0.819433\n",
      " 200945/1000000: episode: 1053, duration: 2.633s, episode steps: 251, steps per second:  95, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: 0.000877, mae: 0.152610, mean_q: 0.202386, mean_eps: 0.819262\n",
      " 201078/1000000: episode: 1054, duration: 1.362s, episode steps: 133, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.662 [0.000, 3.000],  loss: 0.001027, mae: 0.146586, mean_q: 0.195296, mean_eps: 0.819089\n",
      " 201209/1000000: episode: 1055, duration: 1.347s, episode steps: 131, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.382 [0.000, 3.000],  loss: 0.000414, mae: 0.144914, mean_q: 0.193952, mean_eps: 0.818970\n",
      " 201339/1000000: episode: 1056, duration: 1.337s, episode steps: 130, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.392 [0.000, 3.000],  loss: 0.001295, mae: 0.145891, mean_q: 0.194686, mean_eps: 0.818853\n",
      " 201607/1000000: episode: 1057, duration: 2.687s, episode steps: 268, steps per second: 100, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 0.000901, mae: 0.148978, mean_q: 0.198732, mean_eps: 0.818675\n",
      " 201765/1000000: episode: 1058, duration: 1.598s, episode steps: 158, steps per second:  99, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: 0.000836, mae: 0.151838, mean_q: 0.203958, mean_eps: 0.818483\n",
      " 202027/1000000: episode: 1059, duration: 2.608s, episode steps: 262, steps per second: 100, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 0.000793, mae: 0.148594, mean_q: 0.197660, mean_eps: 0.818294\n",
      " 202193/1000000: episode: 1060, duration: 1.723s, episode steps: 166, steps per second:  96, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.000744, mae: 0.148497, mean_q: 0.197681, mean_eps: 0.818101\n",
      " 202356/1000000: episode: 1061, duration: 1.704s, episode steps: 163, steps per second:  96, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.411 [0.000, 3.000],  loss: 0.000722, mae: 0.159115, mean_q: 0.210714, mean_eps: 0.817953\n",
      " 202551/1000000: episode: 1062, duration: 2.058s, episode steps: 195, steps per second:  95, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.697 [0.000, 3.000],  loss: 0.000895, mae: 0.143594, mean_q: 0.192586, mean_eps: 0.817793\n",
      " 202832/1000000: episode: 1063, duration: 2.933s, episode steps: 281, steps per second:  96, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 0.000666, mae: 0.147988, mean_q: 0.198070, mean_eps: 0.817579\n",
      " 202981/1000000: episode: 1064, duration: 1.545s, episode steps: 149, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.530 [0.000, 3.000],  loss: 0.000638, mae: 0.142426, mean_q: 0.189853, mean_eps: 0.817385\n",
      " 203111/1000000: episode: 1065, duration: 1.280s, episode steps: 130, steps per second: 102, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.400 [0.000, 3.000],  loss: 0.000380, mae: 0.155212, mean_q: 0.205930, mean_eps: 0.817259\n",
      " 203287/1000000: episode: 1066, duration: 1.761s, episode steps: 176, steps per second: 100, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.602 [0.000, 3.000],  loss: 0.000374, mae: 0.139950, mean_q: 0.185991, mean_eps: 0.817122\n",
      " 203498/1000000: episode: 1067, duration: 2.126s, episode steps: 211, steps per second:  99, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: 0.000547, mae: 0.139141, mean_q: 0.186797, mean_eps: 0.816947\n",
      " 203703/1000000: episode: 1068, duration: 2.082s, episode steps: 205, steps per second:  98, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.332 [0.000, 3.000],  loss: 0.000663, mae: 0.147450, mean_q: 0.197800, mean_eps: 0.816760\n",
      " 203958/1000000: episode: 1069, duration: 3.041s, episode steps: 255, steps per second:  84, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.000907, mae: 0.145197, mean_q: 0.195613, mean_eps: 0.816553\n",
      " 204280/1000000: episode: 1070, duration: 3.669s, episode steps: 322, steps per second:  88, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.575 [0.000, 3.000],  loss: 0.000824, mae: 0.151835, mean_q: 0.204396, mean_eps: 0.816294\n",
      " 204497/1000000: episode: 1071, duration: 2.377s, episode steps: 217, steps per second:  91, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.429 [0.000, 3.000],  loss: 0.000313, mae: 0.154874, mean_q: 0.207405, mean_eps: 0.816051\n",
      " 204659/1000000: episode: 1072, duration: 1.649s, episode steps: 162, steps per second:  98, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.611 [0.000, 3.000],  loss: 0.000540, mae: 0.148745, mean_q: 0.199554, mean_eps: 0.815880\n",
      " 204846/1000000: episode: 1073, duration: 2.098s, episode steps: 187, steps per second:  89, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 0.000321, mae: 0.149769, mean_q: 0.200664, mean_eps: 0.815723\n",
      " 204984/1000000: episode: 1074, duration: 1.517s, episode steps: 138, steps per second:  91, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.377 [0.000, 3.000],  loss: 0.001083, mae: 0.144166, mean_q: 0.192975, mean_eps: 0.815577\n",
      " 205111/1000000: episode: 1075, duration: 1.317s, episode steps: 127, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.661 [0.000, 3.000],  loss: 0.000352, mae: 0.152957, mean_q: 0.204024, mean_eps: 0.815459\n",
      " 205253/1000000: episode: 1076, duration: 1.499s, episode steps: 142, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.472 [0.000, 3.000],  loss: 0.000228, mae: 0.145615, mean_q: 0.193900, mean_eps: 0.815336\n",
      " 205566/1000000: episode: 1077, duration: 3.304s, episode steps: 313, steps per second:  95, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.687 [0.000, 3.000],  loss: 0.000547, mae: 0.144584, mean_q: 0.192704, mean_eps: 0.815131\n",
      " 205693/1000000: episode: 1078, duration: 1.311s, episode steps: 127, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.512 [0.000, 3.000],  loss: 0.000540, mae: 0.147477, mean_q: 0.196769, mean_eps: 0.814933\n",
      " 205820/1000000: episode: 1079, duration: 1.279s, episode steps: 127, steps per second:  99, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.276 [0.000, 3.000],  loss: 0.000521, mae: 0.144521, mean_q: 0.192492, mean_eps: 0.814820\n",
      " 205955/1000000: episode: 1080, duration: 1.403s, episode steps: 135, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.511 [0.000, 3.000],  loss: 0.000270, mae: 0.146086, mean_q: 0.195161, mean_eps: 0.814703\n",
      " 206088/1000000: episode: 1081, duration: 1.390s, episode steps: 133, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.519 [0.000, 3.000],  loss: 0.000265, mae: 0.150624, mean_q: 0.199897, mean_eps: 0.814582\n",
      " 206357/1000000: episode: 1082, duration: 2.799s, episode steps: 269, steps per second:  96, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: 0.000241, mae: 0.148561, mean_q: 0.198347, mean_eps: 0.814400\n",
      " 206490/1000000: episode: 1083, duration: 1.348s, episode steps: 133, steps per second:  99, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.000217, mae: 0.142737, mean_q: 0.190516, mean_eps: 0.814218\n",
      " 206627/1000000: episode: 1084, duration: 1.463s, episode steps: 137, steps per second:  94, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.474 [0.000, 3.000],  loss: 0.000663, mae: 0.150128, mean_q: 0.199783, mean_eps: 0.814098\n",
      " 206788/1000000: episode: 1085, duration: 1.679s, episode steps: 161, steps per second:  96, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.422 [0.000, 3.000],  loss: 0.000240, mae: 0.147765, mean_q: 0.197007, mean_eps: 0.813965\n",
      " 207091/1000000: episode: 1086, duration: 3.160s, episode steps: 303, steps per second:  96, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 0.000211, mae: 0.143495, mean_q: 0.191771, mean_eps: 0.813756\n",
      " 207304/1000000: episode: 1087, duration: 2.180s, episode steps: 213, steps per second:  98, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.338 [0.000, 3.000],  loss: 0.000607, mae: 0.152016, mean_q: 0.202961, mean_eps: 0.813524\n",
      " 207486/1000000: episode: 1088, duration: 1.856s, episode steps: 182, steps per second:  98, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.330 [0.000, 3.000],  loss: 0.000176, mae: 0.137747, mean_q: 0.185170, mean_eps: 0.813345\n",
      " 207609/1000000: episode: 1089, duration: 1.250s, episode steps: 123, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.520 [0.000, 3.000],  loss: 0.000649, mae: 0.145904, mean_q: 0.195853, mean_eps: 0.813207\n",
      " 207787/1000000: episode: 1090, duration: 1.941s, episode steps: 178, steps per second:  92, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 0.000189, mae: 0.146084, mean_q: 0.194900, mean_eps: 0.813072\n",
      " 207998/1000000: episode: 1091, duration: 2.346s, episode steps: 211, steps per second:  90, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.408 [0.000, 3.000],  loss: 0.000198, mae: 0.153900, mean_q: 0.205410, mean_eps: 0.812897\n",
      " 208202/1000000: episode: 1092, duration: 2.113s, episode steps: 204, steps per second:  97, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: 0.001012, mae: 0.135379, mean_q: 0.181455, mean_eps: 0.812710\n",
      " 208439/1000000: episode: 1093, duration: 2.522s, episode steps: 237, steps per second:  94, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: 0.000318, mae: 0.137707, mean_q: 0.183696, mean_eps: 0.812512\n",
      " 208587/1000000: episode: 1094, duration: 1.685s, episode steps: 148, steps per second:  88, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.318 [0.000, 3.000],  loss: 0.000272, mae: 0.154658, mean_q: 0.207027, mean_eps: 0.812339\n",
      " 208784/1000000: episode: 1095, duration: 2.193s, episode steps: 197, steps per second:  90, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.751 [0.000, 3.000],  loss: 0.000181, mae: 0.149203, mean_q: 0.199580, mean_eps: 0.812184\n",
      " 208912/1000000: episode: 1096, duration: 1.435s, episode steps: 128, steps per second:  89, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.531 [0.000, 3.000],  loss: 0.000274, mae: 0.141587, mean_q: 0.189418, mean_eps: 0.812039\n",
      " 209039/1000000: episode: 1097, duration: 1.427s, episode steps: 127, steps per second:  89, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.346 [0.000, 3.000],  loss: 0.000444, mae: 0.140111, mean_q: 0.187037, mean_eps: 0.811923\n",
      " 209314/1000000: episode: 1098, duration: 2.889s, episode steps: 275, steps per second:  95, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.516 [0.000, 3.000],  loss: 0.000333, mae: 0.143942, mean_q: 0.192313, mean_eps: 0.811742\n",
      " 209515/1000000: episode: 1099, duration: 2.054s, episode steps: 201, steps per second:  98, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 0.000566, mae: 0.145817, mean_q: 0.194669, mean_eps: 0.811527\n",
      " 209669/1000000: episode: 1100, duration: 1.587s, episode steps: 154, steps per second:  97, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.468 [0.000, 3.000],  loss: 0.000728, mae: 0.145884, mean_q: 0.194238, mean_eps: 0.811367\n",
      " 209899/1000000: episode: 1101, duration: 2.355s, episode steps: 230, steps per second:  98, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.000257, mae: 0.141805, mean_q: 0.189656, mean_eps: 0.811194\n",
      " 210043/1000000: episode: 1102, duration: 1.567s, episode steps: 144, steps per second:  92, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.743 [0.000, 3.000],  loss: 0.000383, mae: 0.142909, mean_q: 0.190837, mean_eps: 0.811027\n",
      " 210199/1000000: episode: 1103, duration: 1.548s, episode steps: 156, steps per second: 101, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.647 [0.000, 3.000],  loss: 0.002191, mae: 0.154322, mean_q: 0.206139, mean_eps: 0.810892\n",
      " 210436/1000000: episode: 1104, duration: 2.383s, episode steps: 237, steps per second:  99, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.354 [0.000, 3.000],  loss: 0.001446, mae: 0.147086, mean_q: 0.197255, mean_eps: 0.810716\n",
      " 210746/1000000: episode: 1105, duration: 3.125s, episode steps: 310, steps per second:  99, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 0.000963, mae: 0.159484, mean_q: 0.213061, mean_eps: 0.810469\n",
      " 210939/1000000: episode: 1106, duration: 1.974s, episode steps: 193, steps per second:  98, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.415 [0.000, 3.000],  loss: 0.000592, mae: 0.162569, mean_q: 0.217280, mean_eps: 0.810242\n",
      " 211127/1000000: episode: 1107, duration: 1.948s, episode steps: 188, steps per second:  96, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.000877, mae: 0.154841, mean_q: 0.205945, mean_eps: 0.810071\n",
      " 211253/1000000: episode: 1108, duration: 1.303s, episode steps: 126, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.437 [0.000, 3.000],  loss: 0.000852, mae: 0.153783, mean_q: 0.204504, mean_eps: 0.809929\n",
      " 211528/1000000: episode: 1109, duration: 2.884s, episode steps: 275, steps per second:  95, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.582 [0.000, 3.000],  loss: 0.000459, mae: 0.147253, mean_q: 0.196026, mean_eps: 0.809749\n",
      " 211699/1000000: episode: 1110, duration: 1.765s, episode steps: 171, steps per second:  97, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.643 [0.000, 3.000],  loss: 0.000630, mae: 0.159921, mean_q: 0.213498, mean_eps: 0.809549\n",
      " 211840/1000000: episode: 1111, duration: 1.450s, episode steps: 141, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.312 [0.000, 3.000],  loss: 0.000529, mae: 0.168198, mean_q: 0.224133, mean_eps: 0.809409\n",
      " 212051/1000000: episode: 1112, duration: 2.138s, episode steps: 211, steps per second:  99, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.720 [0.000, 3.000],  loss: 0.000526, mae: 0.156027, mean_q: 0.209413, mean_eps: 0.809250\n",
      " 212309/1000000: episode: 1113, duration: 2.607s, episode steps: 258, steps per second:  99, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.601 [0.000, 3.000],  loss: 0.000936, mae: 0.145732, mean_q: 0.193942, mean_eps: 0.809038\n",
      " 212449/1000000: episode: 1114, duration: 1.444s, episode steps: 140, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.457 [0.000, 3.000],  loss: 0.000367, mae: 0.161747, mean_q: 0.216458, mean_eps: 0.808858\n",
      " 212574/1000000: episode: 1115, duration: 1.236s, episode steps: 125, steps per second: 101, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.432 [0.000, 3.000],  loss: 0.000429, mae: 0.171462, mean_q: 0.229401, mean_eps: 0.808739\n",
      " 212715/1000000: episode: 1116, duration: 1.404s, episode steps: 141, steps per second: 100, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.496 [0.000, 3.000],  loss: 0.000895, mae: 0.161785, mean_q: 0.215470, mean_eps: 0.808620\n",
      " 212934/1000000: episode: 1117, duration: 2.187s, episode steps: 219, steps per second: 100, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.292 [0.000, 3.000],  loss: 0.000618, mae: 0.154512, mean_q: 0.206545, mean_eps: 0.808458\n",
      " 213194/1000000: episode: 1118, duration: 2.712s, episode steps: 260, steps per second:  96, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 0.000405, mae: 0.143966, mean_q: 0.192472, mean_eps: 0.808242\n",
      " 213407/1000000: episode: 1119, duration: 2.112s, episode steps: 213, steps per second: 101, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 0.001241, mae: 0.149091, mean_q: 0.200233, mean_eps: 0.808030\n",
      " 213663/1000000: episode: 1120, duration: 2.552s, episode steps: 256, steps per second: 100, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.273 [0.000, 3.000],  loss: 0.000485, mae: 0.147080, mean_q: 0.197059, mean_eps: 0.807819\n",
      " 213854/1000000: episode: 1121, duration: 1.934s, episode steps: 191, steps per second:  99, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 0.000549, mae: 0.150219, mean_q: 0.202140, mean_eps: 0.807618\n",
      " 214060/1000000: episode: 1122, duration: 2.064s, episode steps: 206, steps per second: 100, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: 0.001159, mae: 0.153130, mean_q: 0.202977, mean_eps: 0.807440\n",
      " 214219/1000000: episode: 1123, duration: 1.617s, episode steps: 159, steps per second:  98, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.371 [0.000, 3.000],  loss: 0.000685, mae: 0.149876, mean_q: 0.200428, mean_eps: 0.807276\n",
      " 214429/1000000: episode: 1124, duration: 2.127s, episode steps: 210, steps per second:  99, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.000571, mae: 0.146118, mean_q: 0.195870, mean_eps: 0.807108\n",
      " 214595/1000000: episode: 1125, duration: 1.859s, episode steps: 166, steps per second:  89, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.000587, mae: 0.152031, mean_q: 0.202320, mean_eps: 0.806939\n",
      " 214791/1000000: episode: 1126, duration: 2.128s, episode steps: 196, steps per second:  92, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.668 [0.000, 3.000],  loss: 0.000542, mae: 0.154620, mean_q: 0.206564, mean_eps: 0.806777\n",
      " 215019/1000000: episode: 1127, duration: 2.261s, episode steps: 228, steps per second: 101, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.404 [0.000, 3.000],  loss: 0.000624, mae: 0.157092, mean_q: 0.210257, mean_eps: 0.806586\n",
      " 215261/1000000: episode: 1128, duration: 2.452s, episode steps: 242, steps per second:  99, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.174 [0.000, 3.000],  loss: 0.000510, mae: 0.158313, mean_q: 0.212052, mean_eps: 0.806374\n",
      " 215393/1000000: episode: 1129, duration: 1.317s, episode steps: 132, steps per second: 100, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.379 [0.000, 3.000],  loss: 0.000744, mae: 0.153066, mean_q: 0.205374, mean_eps: 0.806205\n",
      " 215555/1000000: episode: 1130, duration: 1.627s, episode steps: 162, steps per second: 100, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.722 [0.000, 3.000],  loss: 0.000630, mae: 0.155540, mean_q: 0.208437, mean_eps: 0.806073\n",
      " 215756/1000000: episode: 1131, duration: 2.039s, episode steps: 201, steps per second:  99, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 0.000909, mae: 0.155313, mean_q: 0.206239, mean_eps: 0.805911\n",
      " 215910/1000000: episode: 1132, duration: 1.535s, episode steps: 154, steps per second: 100, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.364 [0.000, 3.000],  loss: 0.000624, mae: 0.153628, mean_q: 0.205784, mean_eps: 0.805751\n",
      " 216143/1000000: episode: 1133, duration: 2.332s, episode steps: 233, steps per second: 100, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.000956, mae: 0.153680, mean_q: 0.205851, mean_eps: 0.805577\n",
      " 216342/1000000: episode: 1134, duration: 2.166s, episode steps: 199, steps per second:  92, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: 0.000512, mae: 0.146618, mean_q: 0.196540, mean_eps: 0.805382\n",
      " 216566/1000000: episode: 1135, duration: 2.249s, episode steps: 224, steps per second: 100, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.442 [0.000, 3.000],  loss: 0.000401, mae: 0.149960, mean_q: 0.200841, mean_eps: 0.805191\n",
      " 216759/1000000: episode: 1136, duration: 2.016s, episode steps: 193, steps per second:  96, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 0.000326, mae: 0.145497, mean_q: 0.193954, mean_eps: 0.805004\n",
      " 216936/1000000: episode: 1137, duration: 1.839s, episode steps: 177, steps per second:  96, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.000904, mae: 0.155181, mean_q: 0.209224, mean_eps: 0.804839\n",
      " 217101/1000000: episode: 1138, duration: 1.707s, episode steps: 165, steps per second:  97, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.467 [0.000, 3.000],  loss: 0.000255, mae: 0.153712, mean_q: 0.206164, mean_eps: 0.804684\n",
      " 217306/1000000: episode: 1139, duration: 2.115s, episode steps: 205, steps per second:  97, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.644 [0.000, 3.000],  loss: 0.000557, mae: 0.146431, mean_q: 0.195326, mean_eps: 0.804516\n",
      " 217478/1000000: episode: 1140, duration: 1.751s, episode steps: 172, steps per second:  98, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.674 [0.000, 3.000],  loss: 0.000485, mae: 0.153312, mean_q: 0.204647, mean_eps: 0.804347\n",
      " 217775/1000000: episode: 1141, duration: 3.085s, episode steps: 297, steps per second:  96, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.000539, mae: 0.144194, mean_q: 0.193454, mean_eps: 0.804137\n",
      " 217942/1000000: episode: 1142, duration: 1.853s, episode steps: 167, steps per second:  90, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.533 [0.000, 3.000],  loss: 0.000367, mae: 0.146058, mean_q: 0.194570, mean_eps: 0.803928\n",
      " 218074/1000000: episode: 1143, duration: 1.370s, episode steps: 132, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.402 [0.000, 3.000],  loss: 0.000431, mae: 0.158191, mean_q: 0.210689, mean_eps: 0.803793\n",
      " 218321/1000000: episode: 1144, duration: 2.482s, episode steps: 247, steps per second: 100, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.397 [0.000, 3.000],  loss: 0.000290, mae: 0.149180, mean_q: 0.199856, mean_eps: 0.803622\n",
      " 218571/1000000: episode: 1145, duration: 2.523s, episode steps: 250, steps per second:  99, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.000194, mae: 0.151568, mean_q: 0.202988, mean_eps: 0.803399\n",
      " 218796/1000000: episode: 1146, duration: 2.302s, episode steps: 225, steps per second:  98, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 0.000501, mae: 0.151748, mean_q: 0.202352, mean_eps: 0.803186\n",
      " 218969/1000000: episode: 1147, duration: 1.771s, episode steps: 173, steps per second:  98, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.659 [0.000, 3.000],  loss: 0.000768, mae: 0.139550, mean_q: 0.186719, mean_eps: 0.803006\n",
      " 219097/1000000: episode: 1148, duration: 1.266s, episode steps: 128, steps per second: 101, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.383 [0.000, 3.000],  loss: 0.000323, mae: 0.153633, mean_q: 0.204924, mean_eps: 0.802869\n",
      " 219316/1000000: episode: 1149, duration: 2.254s, episode steps: 219, steps per second:  97, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.603 [0.000, 3.000],  loss: 0.000475, mae: 0.149682, mean_q: 0.199223, mean_eps: 0.802715\n",
      " 219538/1000000: episode: 1150, duration: 2.557s, episode steps: 222, steps per second:  87, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: 0.000800, mae: 0.150969, mean_q: 0.201937, mean_eps: 0.802517\n",
      " 219824/1000000: episode: 1151, duration: 2.969s, episode steps: 286, steps per second:  96, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.441 [0.000, 3.000],  loss: 0.000516, mae: 0.146824, mean_q: 0.195604, mean_eps: 0.802288\n",
      " 220029/1000000: episode: 1152, duration: 2.272s, episode steps: 205, steps per second:  90, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.600 [0.000, 3.000],  loss: 0.000546, mae: 0.158265, mean_q: 0.212184, mean_eps: 0.802067\n",
      " 220348/1000000: episode: 1153, duration: 3.614s, episode steps: 319, steps per second:  88, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.702 [0.000, 3.000],  loss: 0.002252, mae: 0.167122, mean_q: 0.225273, mean_eps: 0.801831\n",
      " 220548/1000000: episode: 1154, duration: 2.326s, episode steps: 200, steps per second:  86, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: 0.001959, mae: 0.158394, mean_q: 0.211767, mean_eps: 0.801599\n",
      " 220674/1000000: episode: 1155, duration: 1.436s, episode steps: 126, steps per second:  88, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.444 [0.000, 3.000],  loss: 0.000836, mae: 0.152649, mean_q: 0.204376, mean_eps: 0.801451\n",
      " 220926/1000000: episode: 1156, duration: 2.948s, episode steps: 252, steps per second:  85, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.000983, mae: 0.158002, mean_q: 0.210044, mean_eps: 0.801280\n",
      " 221071/1000000: episode: 1157, duration: 1.529s, episode steps: 145, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.414 [0.000, 3.000],  loss: 0.000889, mae: 0.166178, mean_q: 0.222461, mean_eps: 0.801102\n",
      " 221238/1000000: episode: 1158, duration: 1.738s, episode steps: 167, steps per second:  96, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: 0.001104, mae: 0.163466, mean_q: 0.218096, mean_eps: 0.800961\n",
      " 221507/1000000: episode: 1159, duration: 2.729s, episode steps: 269, steps per second:  99, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.706 [0.000, 3.000],  loss: 0.001069, mae: 0.161949, mean_q: 0.215931, mean_eps: 0.800765\n",
      " 221716/1000000: episode: 1160, duration: 2.140s, episode steps: 209, steps per second:  98, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.244 [0.000, 3.000],  loss: 0.001538, mae: 0.154383, mean_q: 0.205831, mean_eps: 0.800551\n",
      " 221863/1000000: episode: 1161, duration: 1.481s, episode steps: 147, steps per second:  99, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.810 [0.000, 3.000],  loss: 0.000672, mae: 0.169072, mean_q: 0.227130, mean_eps: 0.800391\n",
      " 222082/1000000: episode: 1162, duration: 2.227s, episode steps: 219, steps per second:  98, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 0.000973, mae: 0.163216, mean_q: 0.216247, mean_eps: 0.800225\n",
      " 222290/1000000: episode: 1163, duration: 2.167s, episode steps: 208, steps per second:  96, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.001054, mae: 0.162008, mean_q: 0.216440, mean_eps: 0.800033\n",
      " 222471/1000000: episode: 1164, duration: 2.190s, episode steps: 181, steps per second:  83, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.001363, mae: 0.163647, mean_q: 0.218074, mean_eps: 0.799858\n",
      " 222798/1000000: episode: 1165, duration: 3.623s, episode steps: 327, steps per second:  90, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.000650, mae: 0.155773, mean_q: 0.209042, mean_eps: 0.799629\n",
      " 223088/1000000: episode: 1166, duration: 3.168s, episode steps: 290, steps per second:  92, episode reward:  5.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 0.000602, mae: 0.162546, mean_q: 0.217149, mean_eps: 0.799352\n",
      " 223294/1000000: episode: 1167, duration: 2.296s, episode steps: 206, steps per second:  90, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.655 [0.000, 3.000],  loss: 0.000928, mae: 0.160232, mean_q: 0.213693, mean_eps: 0.799129\n",
      " 223487/1000000: episode: 1168, duration: 2.130s, episode steps: 193, steps per second:  91, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.798 [0.000, 3.000],  loss: 0.000963, mae: 0.173244, mean_q: 0.233527, mean_eps: 0.798949\n",
      " 223663/1000000: episode: 1169, duration: 1.947s, episode steps: 176, steps per second:  90, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 0.000688, mae: 0.165571, mean_q: 0.222838, mean_eps: 0.798783\n",
      " 223834/1000000: episode: 1170, duration: 1.907s, episode steps: 171, steps per second:  90, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.766 [0.000, 3.000],  loss: 0.001185, mae: 0.167359, mean_q: 0.225637, mean_eps: 0.798627\n",
      " 223961/1000000: episode: 1171, duration: 1.419s, episode steps: 127, steps per second:  90, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.787 [0.000, 3.000],  loss: 0.001317, mae: 0.163914, mean_q: 0.220209, mean_eps: 0.798492\n",
      " 224182/1000000: episode: 1172, duration: 2.298s, episode steps: 221, steps per second:  96, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: 0.000682, mae: 0.157534, mean_q: 0.211946, mean_eps: 0.798335\n",
      " 224408/1000000: episode: 1173, duration: 2.307s, episode steps: 226, steps per second:  98, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.496 [0.000, 3.000],  loss: 0.000960, mae: 0.159320, mean_q: 0.212891, mean_eps: 0.798135\n",
      " 224651/1000000: episode: 1174, duration: 2.510s, episode steps: 243, steps per second:  97, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.654 [0.000, 3.000],  loss: 0.000742, mae: 0.163736, mean_q: 0.219016, mean_eps: 0.797925\n",
      " 224838/1000000: episode: 1175, duration: 1.904s, episode steps: 187, steps per second:  98, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: 0.001411, mae: 0.157046, mean_q: 0.213652, mean_eps: 0.797730\n",
      " 225067/1000000: episode: 1176, duration: 2.467s, episode steps: 229, steps per second:  93, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.000563, mae: 0.152845, mean_q: 0.203752, mean_eps: 0.797543\n",
      " 225248/1000000: episode: 1177, duration: 1.947s, episode steps: 181, steps per second:  93, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 0.001081, mae: 0.170626, mean_q: 0.228837, mean_eps: 0.797360\n",
      " 225456/1000000: episode: 1178, duration: 2.230s, episode steps: 208, steps per second:  93, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 0.000335, mae: 0.163928, mean_q: 0.219673, mean_eps: 0.797185\n",
      " 225663/1000000: episode: 1179, duration: 2.110s, episode steps: 207, steps per second:  98, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 0.000926, mae: 0.165052, mean_q: 0.222465, mean_eps: 0.796998\n",
      " 225787/1000000: episode: 1180, duration: 1.292s, episode steps: 124, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.661 [0.000, 3.000],  loss: 0.000692, mae: 0.152064, mean_q: 0.206396, mean_eps: 0.796848\n",
      " 226077/1000000: episode: 1181, duration: 2.980s, episode steps: 290, steps per second:  97, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.459 [0.000, 3.000],  loss: 0.000689, mae: 0.175035, mean_q: 0.233867, mean_eps: 0.796661\n",
      " 226241/1000000: episode: 1182, duration: 1.678s, episode steps: 164, steps per second:  98, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 0.000442, mae: 0.154589, mean_q: 0.206233, mean_eps: 0.796456\n",
      " 226395/1000000: episode: 1183, duration: 1.563s, episode steps: 154, steps per second:  99, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.695 [0.000, 3.000],  loss: 0.000916, mae: 0.173873, mean_q: 0.231217, mean_eps: 0.796314\n",
      " 226585/1000000: episode: 1184, duration: 2.014s, episode steps: 190, steps per second:  94, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.537 [0.000, 3.000],  loss: 0.000699, mae: 0.150037, mean_q: 0.202025, mean_eps: 0.796159\n",
      " 226722/1000000: episode: 1185, duration: 1.435s, episode steps: 137, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.343 [0.000, 3.000],  loss: 0.000495, mae: 0.151878, mean_q: 0.204799, mean_eps: 0.796011\n",
      " 226949/1000000: episode: 1186, duration: 2.413s, episode steps: 227, steps per second:  94, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.000542, mae: 0.160309, mean_q: 0.215171, mean_eps: 0.795848\n",
      " 227250/1000000: episode: 1187, duration: 3.075s, episode steps: 301, steps per second:  98, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.641 [0.000, 3.000],  loss: 0.000842, mae: 0.160750, mean_q: 0.216485, mean_eps: 0.795610\n",
      " 227438/1000000: episode: 1188, duration: 1.872s, episode steps: 188, steps per second: 100, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.447 [0.000, 3.000],  loss: 0.000916, mae: 0.167605, mean_q: 0.224713, mean_eps: 0.795390\n",
      " 227648/1000000: episode: 1189, duration: 2.152s, episode steps: 210, steps per second:  98, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.414 [0.000, 3.000],  loss: 0.000614, mae: 0.158303, mean_q: 0.211734, mean_eps: 0.795212\n",
      " 227807/1000000: episode: 1190, duration: 1.637s, episode steps: 159, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.572 [0.000, 3.000],  loss: 0.000797, mae: 0.155617, mean_q: 0.209701, mean_eps: 0.795047\n",
      " 227974/1000000: episode: 1191, duration: 1.708s, episode steps: 167, steps per second:  98, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 0.000882, mae: 0.163725, mean_q: 0.220141, mean_eps: 0.794899\n",
      " 228258/1000000: episode: 1192, duration: 2.974s, episode steps: 284, steps per second:  95, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.676 [0.000, 3.000],  loss: 0.000846, mae: 0.167579, mean_q: 0.224670, mean_eps: 0.794696\n",
      " 228471/1000000: episode: 1193, duration: 2.297s, episode steps: 213, steps per second:  93, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.695 [0.000, 3.000],  loss: 0.000641, mae: 0.158788, mean_q: 0.213282, mean_eps: 0.794472\n",
      " 228649/1000000: episode: 1194, duration: 1.878s, episode steps: 178, steps per second:  95, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.000383, mae: 0.169517, mean_q: 0.228311, mean_eps: 0.794296\n",
      " 228834/1000000: episode: 1195, duration: 1.860s, episode steps: 185, steps per second:  99, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.681 [0.000, 3.000],  loss: 0.000437, mae: 0.166986, mean_q: 0.224046, mean_eps: 0.794132\n",
      " 228980/1000000: episode: 1196, duration: 1.510s, episode steps: 146, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.473 [0.000, 3.000],  loss: 0.000609, mae: 0.173704, mean_q: 0.232629, mean_eps: 0.793985\n",
      " 229230/1000000: episode: 1197, duration: 2.614s, episode steps: 250, steps per second:  96, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.468 [0.000, 3.000],  loss: 0.000841, mae: 0.161494, mean_q: 0.217211, mean_eps: 0.793806\n",
      " 229382/1000000: episode: 1198, duration: 1.550s, episode steps: 152, steps per second:  98, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.474 [0.000, 3.000],  loss: 0.000481, mae: 0.157670, mean_q: 0.211802, mean_eps: 0.793625\n",
      " 229616/1000000: episode: 1199, duration: 2.414s, episode steps: 234, steps per second:  97, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.603 [0.000, 3.000],  loss: 0.000478, mae: 0.160377, mean_q: 0.215276, mean_eps: 0.793452\n",
      " 229780/1000000: episode: 1200, duration: 1.707s, episode steps: 164, steps per second:  96, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.726 [0.000, 3.000],  loss: 0.000786, mae: 0.154088, mean_q: 0.206754, mean_eps: 0.793274\n",
      " 229915/1000000: episode: 1201, duration: 1.485s, episode steps: 135, steps per second:  91, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.511 [0.000, 3.000],  loss: 0.000779, mae: 0.159344, mean_q: 0.214773, mean_eps: 0.793139\n",
      " 230052/1000000: episode: 1202, duration: 1.483s, episode steps: 137, steps per second:  92, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.533 [0.000, 3.000],  loss: 0.001157, mae: 0.168077, mean_q: 0.226081, mean_eps: 0.793016\n",
      " 230178/1000000: episode: 1203, duration: 1.314s, episode steps: 126, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.421 [0.000, 3.000],  loss: 0.002295, mae: 0.181663, mean_q: 0.244689, mean_eps: 0.792897\n",
      " 230370/1000000: episode: 1204, duration: 1.967s, episode steps: 192, steps per second:  98, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 0.002375, mae: 0.182511, mean_q: 0.242797, mean_eps: 0.792753\n",
      " 230500/1000000: episode: 1205, duration: 1.332s, episode steps: 130, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.001984, mae: 0.175883, mean_q: 0.234428, mean_eps: 0.792609\n",
      " 230716/1000000: episode: 1206, duration: 2.216s, episode steps: 216, steps per second:  97, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: 0.001367, mae: 0.163574, mean_q: 0.219484, mean_eps: 0.792455\n",
      " 231079/1000000: episode: 1207, duration: 3.691s, episode steps: 363, steps per second:  98, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.573 [0.000, 3.000],  loss: 0.001137, mae: 0.180954, mean_q: 0.242601, mean_eps: 0.792194\n",
      " 231206/1000000: episode: 1208, duration: 1.342s, episode steps: 127, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.732 [0.000, 3.000],  loss: 0.001321, mae: 0.171883, mean_q: 0.230247, mean_eps: 0.791972\n",
      " 231348/1000000: episode: 1209, duration: 1.496s, episode steps: 142, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.599 [0.000, 3.000],  loss: 0.001658, mae: 0.190590, mean_q: 0.258982, mean_eps: 0.791852\n",
      " 231493/1000000: episode: 1210, duration: 1.603s, episode steps: 145, steps per second:  90, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.510 [0.000, 3.000],  loss: 0.001302, mae: 0.167436, mean_q: 0.225853, mean_eps: 0.791722\n",
      " 231707/1000000: episode: 1211, duration: 2.198s, episode steps: 214, steps per second:  97, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.000980, mae: 0.164866, mean_q: 0.223035, mean_eps: 0.791560\n",
      " 231981/1000000: episode: 1212, duration: 2.847s, episode steps: 274, steps per second:  96, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.001370, mae: 0.183831, mean_q: 0.247521, mean_eps: 0.791340\n",
      " 232127/1000000: episode: 1213, duration: 1.534s, episode steps: 146, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.438 [0.000, 3.000],  loss: 0.001066, mae: 0.182477, mean_q: 0.243629, mean_eps: 0.791151\n",
      " 232270/1000000: episode: 1214, duration: 1.514s, episode steps: 143, steps per second:  94, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.804 [0.000, 3.000],  loss: 0.000748, mae: 0.176068, mean_q: 0.236564, mean_eps: 0.791022\n",
      " 232436/1000000: episode: 1215, duration: 1.721s, episode steps: 166, steps per second:  96, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.458 [0.000, 3.000],  loss: 0.001246, mae: 0.183217, mean_q: 0.244969, mean_eps: 0.790883\n",
      " 232658/1000000: episode: 1216, duration: 2.316s, episode steps: 222, steps per second:  96, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.000944, mae: 0.177418, mean_q: 0.239348, mean_eps: 0.790709\n",
      " 232925/1000000: episode: 1217, duration: 2.783s, episode steps: 267, steps per second:  96, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.697 [0.000, 3.000],  loss: 0.000881, mae: 0.177074, mean_q: 0.238721, mean_eps: 0.790487\n",
      " 233097/1000000: episode: 1218, duration: 1.840s, episode steps: 172, steps per second:  94, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.756 [0.000, 3.000],  loss: 0.001076, mae: 0.171434, mean_q: 0.230554, mean_eps: 0.790289\n",
      " 233306/1000000: episode: 1219, duration: 2.164s, episode steps: 209, steps per second:  97, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: 0.001025, mae: 0.174913, mean_q: 0.235138, mean_eps: 0.790118\n",
      " 233465/1000000: episode: 1220, duration: 1.651s, episode steps: 159, steps per second:  96, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.440 [0.000, 3.000],  loss: 0.000933, mae: 0.164173, mean_q: 0.221044, mean_eps: 0.789953\n",
      " 233632/1000000: episode: 1221, duration: 1.705s, episode steps: 167, steps per second:  98, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.743 [0.000, 3.000],  loss: 0.001305, mae: 0.159769, mean_q: 0.212679, mean_eps: 0.789807\n",
      " 233773/1000000: episode: 1222, duration: 1.471s, episode steps: 141, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.525 [0.000, 3.000],  loss: 0.000981, mae: 0.172222, mean_q: 0.231243, mean_eps: 0.789668\n",
      " 234039/1000000: episode: 1223, duration: 2.655s, episode steps: 266, steps per second: 100, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.613 [0.000, 3.000],  loss: 0.001415, mae: 0.181054, mean_q: 0.244053, mean_eps: 0.789485\n",
      " 234168/1000000: episode: 1224, duration: 1.339s, episode steps: 129, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.651 [0.000, 3.000],  loss: 0.000852, mae: 0.170209, mean_q: 0.230482, mean_eps: 0.789308\n",
      " 234335/1000000: episode: 1225, duration: 1.741s, episode steps: 167, steps per second:  96, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.659 [0.000, 3.000],  loss: 0.000993, mae: 0.172762, mean_q: 0.232017, mean_eps: 0.789175\n",
      " 234573/1000000: episode: 1226, duration: 2.563s, episode steps: 238, steps per second:  93, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.643 [0.000, 3.000],  loss: 0.001362, mae: 0.177201, mean_q: 0.238990, mean_eps: 0.788991\n",
      " 234867/1000000: episode: 1227, duration: 3.039s, episode steps: 294, steps per second:  97, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.000805, mae: 0.183387, mean_q: 0.247226, mean_eps: 0.788752\n",
      " 235103/1000000: episode: 1228, duration: 2.485s, episode steps: 236, steps per second:  95, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.674 [0.000, 3.000],  loss: 0.001132, mae: 0.162352, mean_q: 0.219070, mean_eps: 0.788514\n",
      " 235322/1000000: episode: 1229, duration: 2.373s, episode steps: 219, steps per second:  92, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.379 [0.000, 3.000],  loss: 0.001798, mae: 0.178311, mean_q: 0.240110, mean_eps: 0.788309\n",
      " 235486/1000000: episode: 1230, duration: 1.672s, episode steps: 164, steps per second:  98, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.683 [0.000, 3.000],  loss: 0.001620, mae: 0.178968, mean_q: 0.241809, mean_eps: 0.788136\n",
      " 235805/1000000: episode: 1231, duration: 3.307s, episode steps: 319, steps per second:  96, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: 0.001462, mae: 0.176804, mean_q: 0.241576, mean_eps: 0.787919\n",
      " 236009/1000000: episode: 1232, duration: 2.184s, episode steps: 204, steps per second:  93, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.652 [0.000, 3.000],  loss: 0.001273, mae: 0.167713, mean_q: 0.226532, mean_eps: 0.787683\n",
      " 236241/1000000: episode: 1233, duration: 2.470s, episode steps: 232, steps per second:  94, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 0.001592, mae: 0.182291, mean_q: 0.245790, mean_eps: 0.787487\n",
      " 236507/1000000: episode: 1234, duration: 2.694s, episode steps: 266, steps per second:  99, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.001411, mae: 0.176572, mean_q: 0.238688, mean_eps: 0.787263\n",
      " 236642/1000000: episode: 1235, duration: 1.372s, episode steps: 135, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.681 [0.000, 3.000],  loss: 0.001513, mae: 0.192943, mean_q: 0.259838, mean_eps: 0.787083\n",
      " 236779/1000000: episode: 1236, duration: 1.465s, episode steps: 137, steps per second:  93, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.453 [0.000, 3.000],  loss: 0.000702, mae: 0.179549, mean_q: 0.240096, mean_eps: 0.786961\n",
      " 236966/1000000: episode: 1237, duration: 1.921s, episode steps: 187, steps per second:  97, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.754 [0.000, 3.000],  loss: 0.001168, mae: 0.184803, mean_q: 0.249963, mean_eps: 0.786815\n",
      " 237101/1000000: episode: 1238, duration: 1.376s, episode steps: 135, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.889 [0.000, 3.000],  loss: 0.001158, mae: 0.188852, mean_q: 0.256783, mean_eps: 0.786669\n",
      " 237233/1000000: episode: 1239, duration: 1.342s, episode steps: 132, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.364 [0.000, 3.000],  loss: 0.000809, mae: 0.182674, mean_q: 0.245794, mean_eps: 0.786549\n",
      " 237369/1000000: episode: 1240, duration: 1.387s, episode steps: 136, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.596 [0.000, 3.000],  loss: 0.000776, mae: 0.168424, mean_q: 0.228480, mean_eps: 0.786428\n",
      " 237581/1000000: episode: 1241, duration: 2.224s, episode steps: 212, steps per second:  95, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 0.001168, mae: 0.183505, mean_q: 0.245871, mean_eps: 0.786272\n",
      " 237820/1000000: episode: 1242, duration: 2.513s, episode steps: 239, steps per second:  95, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.674 [0.000, 3.000],  loss: 0.000935, mae: 0.180282, mean_q: 0.244065, mean_eps: 0.786070\n",
      " 237983/1000000: episode: 1243, duration: 1.713s, episode steps: 163, steps per second:  95, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: 0.000923, mae: 0.172601, mean_q: 0.232726, mean_eps: 0.785890\n",
      " 238190/1000000: episode: 1244, duration: 2.077s, episode steps: 207, steps per second: 100, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.401 [0.000, 3.000],  loss: 0.000917, mae: 0.175578, mean_q: 0.236729, mean_eps: 0.785723\n",
      " 238368/1000000: episode: 1245, duration: 1.809s, episode steps: 178, steps per second:  98, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.000479, mae: 0.175012, mean_q: 0.236232, mean_eps: 0.785550\n",
      " 238590/1000000: episode: 1246, duration: 2.219s, episode steps: 222, steps per second: 100, episode reward:  3.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.802 [0.000, 3.000],  loss: 0.001738, mae: 0.178592, mean_q: 0.243561, mean_eps: 0.785370\n",
      " 238847/1000000: episode: 1247, duration: 2.575s, episode steps: 257, steps per second: 100, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.001169, mae: 0.180771, mean_q: 0.245231, mean_eps: 0.785154\n",
      " 239011/1000000: episode: 1248, duration: 1.682s, episode steps: 164, steps per second:  97, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.001136, mae: 0.174751, mean_q: 0.236921, mean_eps: 0.784965\n",
      " 239278/1000000: episode: 1249, duration: 2.798s, episode steps: 267, steps per second:  95, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.914 [0.000, 3.000],  loss: 0.001317, mae: 0.187929, mean_q: 0.255188, mean_eps: 0.784770\n",
      " 239418/1000000: episode: 1250, duration: 1.483s, episode steps: 140, steps per second:  94, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.643 [0.000, 3.000],  loss: 0.000842, mae: 0.178609, mean_q: 0.240608, mean_eps: 0.784587\n",
      " 239543/1000000: episode: 1251, duration: 1.412s, episode steps: 125, steps per second:  89, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.448 [0.000, 3.000],  loss: 0.001461, mae: 0.169134, mean_q: 0.230891, mean_eps: 0.784468\n",
      " 239826/1000000: episode: 1252, duration: 3.156s, episode steps: 283, steps per second:  90, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.604 [0.000, 3.000],  loss: 0.000672, mae: 0.169432, mean_q: 0.229012, mean_eps: 0.784284\n",
      " 240081/1000000: episode: 1253, duration: 2.745s, episode steps: 255, steps per second:  93, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.002183, mae: 0.176499, mean_q: 0.237370, mean_eps: 0.784041\n",
      " 240246/1000000: episode: 1254, duration: 1.712s, episode steps: 165, steps per second:  96, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.673 [0.000, 3.000],  loss: 0.002565, mae: 0.193493, mean_q: 0.267098, mean_eps: 0.783852\n",
      " 240514/1000000: episode: 1255, duration: 2.713s, episode steps: 268, steps per second:  99, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 0.002814, mae: 0.194827, mean_q: 0.262118, mean_eps: 0.783658\n",
      " 240642/1000000: episode: 1256, duration: 1.443s, episode steps: 128, steps per second:  89, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.656 [0.000, 3.000],  loss: 0.001717, mae: 0.194867, mean_q: 0.262720, mean_eps: 0.783480\n",
      " 240843/1000000: episode: 1257, duration: 2.182s, episode steps: 201, steps per second:  92, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.796 [0.000, 3.000],  loss: 0.000987, mae: 0.183029, mean_q: 0.247670, mean_eps: 0.783332\n",
      " 241127/1000000: episode: 1258, duration: 2.888s, episode steps: 284, steps per second:  98, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 0.001554, mae: 0.195158, mean_q: 0.264916, mean_eps: 0.783114\n",
      " 241304/1000000: episode: 1259, duration: 1.857s, episode steps: 177, steps per second:  95, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.001126, mae: 0.193977, mean_q: 0.262140, mean_eps: 0.782907\n",
      " 241564/1000000: episode: 1260, duration: 2.798s, episode steps: 260, steps per second:  93, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.542 [0.000, 3.000],  loss: 0.001481, mae: 0.187586, mean_q: 0.253149, mean_eps: 0.782711\n",
      " 241701/1000000: episode: 1261, duration: 1.485s, episode steps: 137, steps per second:  92, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.270 [0.000, 3.000],  loss: 0.000943, mae: 0.187523, mean_q: 0.252049, mean_eps: 0.782531\n",
      " 241827/1000000: episode: 1262, duration: 1.346s, episode steps: 126, steps per second:  94, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.476 [0.000, 3.000],  loss: 0.001646, mae: 0.199352, mean_q: 0.271868, mean_eps: 0.782412\n",
      " 241981/1000000: episode: 1263, duration: 1.746s, episode steps: 154, steps per second:  88, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.292 [0.000, 3.000],  loss: 0.001094, mae: 0.193915, mean_q: 0.260949, mean_eps: 0.782286\n",
      " 242115/1000000: episode: 1264, duration: 1.542s, episode steps: 134, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.858 [0.000, 3.000],  loss: 0.001110, mae: 0.198505, mean_q: 0.269080, mean_eps: 0.782157\n",
      " 242335/1000000: episode: 1265, duration: 2.494s, episode steps: 220, steps per second:  88, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.777 [0.000, 3.000],  loss: 0.002007, mae: 0.189687, mean_q: 0.258766, mean_eps: 0.781998\n",
      " 242623/1000000: episode: 1266, duration: 2.950s, episode steps: 288, steps per second:  98, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.001361, mae: 0.191349, mean_q: 0.258238, mean_eps: 0.781770\n",
      " 242857/1000000: episode: 1267, duration: 2.466s, episode steps: 234, steps per second:  95, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.590 [0.000, 3.000],  loss: 0.001782, mae: 0.200836, mean_q: 0.271031, mean_eps: 0.781534\n",
      " 243185/1000000: episode: 1268, duration: 3.509s, episode steps: 328, steps per second:  93, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.001220, mae: 0.183014, mean_q: 0.247847, mean_eps: 0.781280\n",
      " 243431/1000000: episode: 1269, duration: 2.658s, episode steps: 246, steps per second:  93, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.423 [0.000, 3.000],  loss: 0.001744, mae: 0.195132, mean_q: 0.266443, mean_eps: 0.781023\n",
      " 243740/1000000: episode: 1270, duration: 3.551s, episode steps: 309, steps per second:  87, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.001220, mae: 0.195093, mean_q: 0.261989, mean_eps: 0.780774\n",
      " 244003/1000000: episode: 1271, duration: 2.948s, episode steps: 263, steps per second:  89, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.696 [0.000, 3.000],  loss: 0.001461, mae: 0.196702, mean_q: 0.265790, mean_eps: 0.780517\n",
      " 244220/1000000: episode: 1272, duration: 2.437s, episode steps: 217, steps per second:  89, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.631 [0.000, 3.000],  loss: 0.001179, mae: 0.180334, mean_q: 0.243817, mean_eps: 0.780301\n",
      " 244353/1000000: episode: 1273, duration: 1.481s, episode steps: 133, steps per second:  90, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.504 [0.000, 3.000],  loss: 0.002422, mae: 0.186991, mean_q: 0.251269, mean_eps: 0.780143\n",
      " 244592/1000000: episode: 1274, duration: 2.620s, episode steps: 239, steps per second:  91, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: 0.000963, mae: 0.196403, mean_q: 0.264885, mean_eps: 0.779975\n",
      " 244882/1000000: episode: 1275, duration: 3.293s, episode steps: 290, steps per second:  88, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 0.001408, mae: 0.193282, mean_q: 0.260956, mean_eps: 0.779738\n",
      " 245046/1000000: episode: 1276, duration: 1.892s, episode steps: 164, steps per second:  87, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.652 [0.000, 3.000],  loss: 0.001254, mae: 0.176287, mean_q: 0.240163, mean_eps: 0.779532\n",
      " 245213/1000000: episode: 1277, duration: 1.944s, episode steps: 167, steps per second:  86, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 0.001124, mae: 0.197254, mean_q: 0.268117, mean_eps: 0.779383\n",
      " 245467/1000000: episode: 1278, duration: 2.606s, episode steps: 254, steps per second:  97, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.472 [0.000, 3.000],  loss: 0.000726, mae: 0.183999, mean_q: 0.246322, mean_eps: 0.779194\n",
      " 245714/1000000: episode: 1279, duration: 2.511s, episode steps: 247, steps per second:  98, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.611 [0.000, 3.000],  loss: 0.001215, mae: 0.191846, mean_q: 0.258103, mean_eps: 0.778969\n",
      " 245936/1000000: episode: 1280, duration: 2.283s, episode steps: 222, steps per second:  97, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: 0.001247, mae: 0.204247, mean_q: 0.276815, mean_eps: 0.778758\n",
      " 246092/1000000: episode: 1281, duration: 1.640s, episode steps: 156, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.692 [0.000, 3.000],  loss: 0.000783, mae: 0.196964, mean_q: 0.266427, mean_eps: 0.778589\n",
      " 246297/1000000: episode: 1282, duration: 2.089s, episode steps: 205, steps per second:  98, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.001066, mae: 0.194849, mean_q: 0.265584, mean_eps: 0.778425\n",
      " 246523/1000000: episode: 1283, duration: 2.408s, episode steps: 226, steps per second:  94, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.699 [0.000, 3.000],  loss: 0.000903, mae: 0.188861, mean_q: 0.256137, mean_eps: 0.778231\n",
      " 246776/1000000: episode: 1284, duration: 2.817s, episode steps: 253, steps per second:  90, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.451 [0.000, 3.000],  loss: 0.000972, mae: 0.195629, mean_q: 0.264493, mean_eps: 0.778017\n",
      " 246976/1000000: episode: 1285, duration: 2.066s, episode steps: 200, steps per second:  97, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.000804, mae: 0.191432, mean_q: 0.260735, mean_eps: 0.777813\n",
      " 247121/1000000: episode: 1286, duration: 1.496s, episode steps: 145, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.566 [0.000, 3.000],  loss: 0.000737, mae: 0.179822, mean_q: 0.245637, mean_eps: 0.777657\n",
      " 247268/1000000: episode: 1287, duration: 1.495s, episode steps: 147, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.544 [0.000, 3.000],  loss: 0.000864, mae: 0.182301, mean_q: 0.248893, mean_eps: 0.777525\n",
      " 247521/1000000: episode: 1288, duration: 2.575s, episode steps: 253, steps per second:  98, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: 0.001092, mae: 0.189127, mean_q: 0.255923, mean_eps: 0.777345\n",
      " 247778/1000000: episode: 1289, duration: 2.616s, episode steps: 257, steps per second:  98, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.447 [0.000, 3.000],  loss: 0.001667, mae: 0.193296, mean_q: 0.264777, mean_eps: 0.777115\n",
      " 248023/1000000: episode: 1290, duration: 2.495s, episode steps: 245, steps per second:  98, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.686 [0.000, 3.000],  loss: 0.001079, mae: 0.183517, mean_q: 0.251427, mean_eps: 0.776890\n",
      " 248317/1000000: episode: 1291, duration: 3.271s, episode steps: 294, steps per second:  90, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: 0.001067, mae: 0.190270, mean_q: 0.258170, mean_eps: 0.776647\n",
      " 248529/1000000: episode: 1292, duration: 2.200s, episode steps: 212, steps per second:  96, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.000975, mae: 0.190411, mean_q: 0.258393, mean_eps: 0.776418\n",
      " 248813/1000000: episode: 1293, duration: 2.938s, episode steps: 284, steps per second:  97, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.620 [0.000, 3.000],  loss: 0.001480, mae: 0.187532, mean_q: 0.255394, mean_eps: 0.776195\n",
      " 249087/1000000: episode: 1294, duration: 2.778s, episode steps: 274, steps per second:  99, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 0.000862, mae: 0.189249, mean_q: 0.257171, mean_eps: 0.775945\n",
      " 249216/1000000: episode: 1295, duration: 1.349s, episode steps: 129, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.457 [0.000, 3.000],  loss: 0.001176, mae: 0.198221, mean_q: 0.267318, mean_eps: 0.775765\n",
      " 249512/1000000: episode: 1296, duration: 3.180s, episode steps: 296, steps per second:  93, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 0.001379, mae: 0.188981, mean_q: 0.255171, mean_eps: 0.775574\n",
      " 249734/1000000: episode: 1297, duration: 2.673s, episode steps: 222, steps per second:  83, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.473 [0.000, 3.000],  loss: 0.001497, mae: 0.192951, mean_q: 0.260230, mean_eps: 0.775340\n",
      " 249872/1000000: episode: 1298, duration: 1.663s, episode steps: 138, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.616 [0.000, 3.000],  loss: 0.001245, mae: 0.183197, mean_q: 0.247023, mean_eps: 0.775178\n",
      " 250096/1000000: episode: 1299, duration: 2.657s, episode steps: 224, steps per second:  84, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.420 [0.000, 3.000],  loss: 0.002257, mae: 0.193636, mean_q: 0.262111, mean_eps: 0.775016\n",
      " 250305/1000000: episode: 1300, duration: 2.264s, episode steps: 209, steps per second:  92, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 0.003161, mae: 0.204202, mean_q: 0.276540, mean_eps: 0.774820\n",
      " 250460/1000000: episode: 1301, duration: 1.785s, episode steps: 155, steps per second:  87, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.716 [0.000, 3.000],  loss: 0.002283, mae: 0.204192, mean_q: 0.277899, mean_eps: 0.774656\n",
      " 250691/1000000: episode: 1302, duration: 2.549s, episode steps: 231, steps per second:  91, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.745 [0.000, 3.000],  loss: 0.001709, mae: 0.195222, mean_q: 0.266248, mean_eps: 0.774483\n",
      " 250856/1000000: episode: 1303, duration: 1.913s, episode steps: 165, steps per second:  86, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.521 [0.000, 3.000],  loss: 0.001377, mae: 0.179830, mean_q: 0.244516, mean_eps: 0.774305\n",
      " 251067/1000000: episode: 1304, duration: 2.437s, episode steps: 211, steps per second:  87, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.002214, mae: 0.212987, mean_q: 0.288006, mean_eps: 0.774136\n",
      " 251272/1000000: episode: 1305, duration: 2.180s, episode steps: 205, steps per second:  94, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.332 [0.000, 3.000],  loss: 0.001749, mae: 0.200383, mean_q: 0.271193, mean_eps: 0.773949\n",
      " 251481/1000000: episode: 1306, duration: 2.412s, episode steps: 209, steps per second:  87, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 0.002146, mae: 0.197779, mean_q: 0.270048, mean_eps: 0.773762\n",
      " 251646/1000000: episode: 1307, duration: 1.917s, episode steps: 165, steps per second:  86, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.721 [0.000, 3.000],  loss: 0.001394, mae: 0.199492, mean_q: 0.272009, mean_eps: 0.773592\n",
      " 251863/1000000: episode: 1308, duration: 2.293s, episode steps: 217, steps per second:  95, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 0.001704, mae: 0.197966, mean_q: 0.268340, mean_eps: 0.773421\n",
      " 252155/1000000: episode: 1309, duration: 3.066s, episode steps: 292, steps per second:  95, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.661 [0.000, 3.000],  loss: 0.001204, mae: 0.198395, mean_q: 0.269174, mean_eps: 0.773193\n",
      " 252448/1000000: episode: 1310, duration: 3.445s, episode steps: 293, steps per second:  85, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.703 [0.000, 3.000],  loss: 0.001716, mae: 0.209031, mean_q: 0.284011, mean_eps: 0.772930\n",
      " 252640/1000000: episode: 1311, duration: 2.088s, episode steps: 192, steps per second:  92, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 0.001643, mae: 0.192383, mean_q: 0.264154, mean_eps: 0.772712\n",
      " 252769/1000000: episode: 1312, duration: 1.366s, episode steps: 129, steps per second:  94, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.527 [0.000, 3.000],  loss: 0.000907, mae: 0.201610, mean_q: 0.273559, mean_eps: 0.772566\n",
      " 253113/1000000: episode: 1313, duration: 3.590s, episode steps: 344, steps per second:  96, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 0.001792, mae: 0.202619, mean_q: 0.276018, mean_eps: 0.772352\n",
      " 253416/1000000: episode: 1314, duration: 3.101s, episode steps: 303, steps per second:  98, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.001243, mae: 0.200004, mean_q: 0.272173, mean_eps: 0.772062\n",
      " 253662/1000000: episode: 1315, duration: 2.569s, episode steps: 246, steps per second:  96, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.000935, mae: 0.196846, mean_q: 0.267881, mean_eps: 0.771816\n",
      " 253889/1000000: episode: 1316, duration: 2.392s, episode steps: 227, steps per second:  95, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: 0.000940, mae: 0.190620, mean_q: 0.259396, mean_eps: 0.771602\n",
      " 254022/1000000: episode: 1317, duration: 1.432s, episode steps: 133, steps per second:  93, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.429 [0.000, 3.000],  loss: 0.000698, mae: 0.185095, mean_q: 0.251255, mean_eps: 0.771440\n",
      " 254223/1000000: episode: 1318, duration: 2.104s, episode steps: 201, steps per second:  96, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 0.001397, mae: 0.201386, mean_q: 0.272313, mean_eps: 0.771290\n",
      " 254556/1000000: episode: 1319, duration: 3.488s, episode steps: 333, steps per second:  95, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.769 [0.000, 3.000],  loss: 0.001483, mae: 0.199179, mean_q: 0.272222, mean_eps: 0.771051\n",
      " 254845/1000000: episode: 1320, duration: 3.045s, episode steps: 289, steps per second:  95, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 0.001627, mae: 0.205011, mean_q: 0.278338, mean_eps: 0.770770\n",
      " 255081/1000000: episode: 1321, duration: 2.418s, episode steps: 236, steps per second:  98, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.001384, mae: 0.187192, mean_q: 0.253670, mean_eps: 0.770532\n",
      " 255215/1000000: episode: 1322, duration: 1.379s, episode steps: 134, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.821 [0.000, 3.000],  loss: 0.000814, mae: 0.179927, mean_q: 0.244500, mean_eps: 0.770367\n",
      " 255394/1000000: episode: 1323, duration: 1.888s, episode steps: 179, steps per second:  95, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.001046, mae: 0.199830, mean_q: 0.272052, mean_eps: 0.770226\n",
      " 255613/1000000: episode: 1324, duration: 2.364s, episode steps: 219, steps per second:  93, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001605, mae: 0.205910, mean_q: 0.278976, mean_eps: 0.770046\n",
      " 255886/1000000: episode: 1325, duration: 2.884s, episode steps: 273, steps per second:  95, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.001489, mae: 0.199288, mean_q: 0.271650, mean_eps: 0.769825\n",
      " 256162/1000000: episode: 1326, duration: 2.908s, episode steps: 276, steps per second:  95, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: 0.001291, mae: 0.191886, mean_q: 0.259779, mean_eps: 0.769578\n",
      " 256306/1000000: episode: 1327, duration: 1.508s, episode steps: 144, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.681 [0.000, 3.000],  loss: 0.000868, mae: 0.193479, mean_q: 0.262872, mean_eps: 0.769389\n",
      " 256593/1000000: episode: 1328, duration: 2.938s, episode steps: 287, steps per second:  98, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.001565, mae: 0.197021, mean_q: 0.266323, mean_eps: 0.769195\n",
      " 256778/1000000: episode: 1329, duration: 1.939s, episode steps: 185, steps per second:  95, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.681 [0.000, 3.000],  loss: 0.001482, mae: 0.200900, mean_q: 0.273599, mean_eps: 0.768983\n",
      " 256915/1000000: episode: 1330, duration: 1.460s, episode steps: 137, steps per second:  94, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.001167, mae: 0.198382, mean_q: 0.271057, mean_eps: 0.768839\n",
      " 257171/1000000: episode: 1331, duration: 2.790s, episode steps: 256, steps per second:  92, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 0.001602, mae: 0.197654, mean_q: 0.269566, mean_eps: 0.768662\n",
      " 257376/1000000: episode: 1332, duration: 2.214s, episode steps: 205, steps per second:  93, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.001222, mae: 0.206296, mean_q: 0.279977, mean_eps: 0.768455\n",
      " 257592/1000000: episode: 1333, duration: 2.313s, episode steps: 216, steps per second:  93, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.324 [0.000, 3.000],  loss: 0.000914, mae: 0.203841, mean_q: 0.276410, mean_eps: 0.768266\n",
      " 257742/1000000: episode: 1334, duration: 1.622s, episode steps: 150, steps per second:  93, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.220 [0.000, 3.000],  loss: 0.000839, mae: 0.198204, mean_q: 0.270026, mean_eps: 0.768101\n",
      " 257906/1000000: episode: 1335, duration: 1.683s, episode steps: 164, steps per second:  97, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.689 [0.000, 3.000],  loss: 0.001112, mae: 0.200597, mean_q: 0.273699, mean_eps: 0.767958\n",
      " 258090/1000000: episode: 1336, duration: 1.908s, episode steps: 184, steps per second:  96, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.707 [0.000, 3.000],  loss: 0.001049, mae: 0.190983, mean_q: 0.261239, mean_eps: 0.767802\n",
      " 258258/1000000: episode: 1337, duration: 1.777s, episode steps: 168, steps per second:  95, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 0.000645, mae: 0.186175, mean_q: 0.252325, mean_eps: 0.767643\n",
      " 258549/1000000: episode: 1338, duration: 3.167s, episode steps: 291, steps per second:  92, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.001593, mae: 0.195155, mean_q: 0.265109, mean_eps: 0.767436\n",
      " 258714/1000000: episode: 1339, duration: 1.734s, episode steps: 165, steps per second:  95, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.376 [0.000, 3.000],  loss: 0.001409, mae: 0.203399, mean_q: 0.275907, mean_eps: 0.767231\n",
      " 258881/1000000: episode: 1340, duration: 1.732s, episode steps: 167, steps per second:  96, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.001103, mae: 0.198610, mean_q: 0.271511, mean_eps: 0.767082\n",
      " 259089/1000000: episode: 1341, duration: 2.157s, episode steps: 208, steps per second:  96, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.308 [0.000, 3.000],  loss: 0.001545, mae: 0.188378, mean_q: 0.254365, mean_eps: 0.766913\n",
      " 259243/1000000: episode: 1342, duration: 1.604s, episode steps: 154, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.643 [0.000, 3.000],  loss: 0.001272, mae: 0.198263, mean_q: 0.270699, mean_eps: 0.766751\n",
      " 259498/1000000: episode: 1343, duration: 2.689s, episode steps: 255, steps per second:  95, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.392 [0.000, 3.000],  loss: 0.000961, mae: 0.206074, mean_q: 0.280256, mean_eps: 0.766567\n",
      " 259635/1000000: episode: 1344, duration: 1.463s, episode steps: 137, steps per second:  94, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.577 [0.000, 3.000],  loss: 0.001552, mae: 0.202109, mean_q: 0.277479, mean_eps: 0.766391\n",
      " 259871/1000000: episode: 1345, duration: 2.513s, episode steps: 236, steps per second:  94, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: 0.001275, mae: 0.202281, mean_q: 0.274019, mean_eps: 0.766223\n",
      " 260008/1000000: episode: 1346, duration: 1.533s, episode steps: 137, steps per second:  89, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.664 [0.000, 3.000],  loss: 0.001351, mae: 0.193291, mean_q: 0.263139, mean_eps: 0.766056\n",
      " 260180/1000000: episode: 1347, duration: 1.875s, episode steps: 172, steps per second:  92, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.355 [0.000, 3.000],  loss: 0.003937, mae: 0.212519, mean_q: 0.284638, mean_eps: 0.765917\n",
      " 260387/1000000: episode: 1348, duration: 2.149s, episode steps: 207, steps per second:  96, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.002793, mae: 0.219905, mean_q: 0.298877, mean_eps: 0.765746\n",
      " 260690/1000000: episode: 1349, duration: 3.195s, episode steps: 303, steps per second:  95, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 0.002168, mae: 0.221133, mean_q: 0.301555, mean_eps: 0.765516\n",
      " 260951/1000000: episode: 1350, duration: 2.678s, episode steps: 261, steps per second:  97, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.001864, mae: 0.203807, mean_q: 0.279421, mean_eps: 0.765262\n",
      " 261202/1000000: episode: 1351, duration: 2.593s, episode steps: 251, steps per second:  97, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.705 [0.000, 3.000],  loss: 0.001890, mae: 0.205444, mean_q: 0.280602, mean_eps: 0.765032\n",
      " 261361/1000000: episode: 1352, duration: 1.704s, episode steps: 159, steps per second:  93, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.780 [0.000, 3.000],  loss: 0.002508, mae: 0.207785, mean_q: 0.284998, mean_eps: 0.764846\n",
      " 261542/1000000: episode: 1353, duration: 1.945s, episode steps: 181, steps per second:  93, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.326 [0.000, 3.000],  loss: 0.002158, mae: 0.209000, mean_q: 0.288151, mean_eps: 0.764693\n",
      " 261777/1000000: episode: 1354, duration: 2.490s, episode steps: 235, steps per second:  94, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 0.002506, mae: 0.211945, mean_q: 0.289283, mean_eps: 0.764506\n",
      " 261943/1000000: episode: 1355, duration: 1.730s, episode steps: 166, steps per second:  96, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: 0.001969, mae: 0.218163, mean_q: 0.295403, mean_eps: 0.764326\n",
      " 262175/1000000: episode: 1356, duration: 2.416s, episode steps: 232, steps per second:  96, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 0.002113, mae: 0.221159, mean_q: 0.304040, mean_eps: 0.764148\n",
      " 262392/1000000: episode: 1357, duration: 2.265s, episode steps: 217, steps per second:  96, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.613 [0.000, 3.000],  loss: 0.002286, mae: 0.211182, mean_q: 0.288728, mean_eps: 0.763946\n",
      " 262604/1000000: episode: 1358, duration: 2.213s, episode steps: 212, steps per second:  96, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.575 [0.000, 3.000],  loss: 0.001628, mae: 0.210776, mean_q: 0.287863, mean_eps: 0.763754\n",
      " 262819/1000000: episode: 1359, duration: 2.230s, episode steps: 215, steps per second:  96, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.586 [0.000, 3.000],  loss: 0.001376, mae: 0.205881, mean_q: 0.279232, mean_eps: 0.763561\n",
      " 263010/1000000: episode: 1360, duration: 2.039s, episode steps: 191, steps per second:  94, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.001868, mae: 0.212065, mean_q: 0.289613, mean_eps: 0.763377\n",
      " 263151/1000000: episode: 1361, duration: 1.497s, episode steps: 141, steps per second:  94, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.809 [0.000, 3.000],  loss: 0.002172, mae: 0.207436, mean_q: 0.286433, mean_eps: 0.763228\n",
      " 263360/1000000: episode: 1362, duration: 2.193s, episode steps: 209, steps per second:  95, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.751 [0.000, 3.000],  loss: 0.001444, mae: 0.205651, mean_q: 0.281560, mean_eps: 0.763071\n",
      " 263570/1000000: episode: 1363, duration: 2.213s, episode steps: 210, steps per second:  95, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.710 [0.000, 3.000],  loss: 0.000988, mae: 0.202227, mean_q: 0.274820, mean_eps: 0.762882\n",
      " 263777/1000000: episode: 1364, duration: 2.156s, episode steps: 207, steps per second:  96, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.676 [0.000, 3.000],  loss: 0.001819, mae: 0.212368, mean_q: 0.289264, mean_eps: 0.762693\n",
      " 264022/1000000: episode: 1365, duration: 2.590s, episode steps: 245, steps per second:  95, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 0.001441, mae: 0.203919, mean_q: 0.277925, mean_eps: 0.762490\n",
      " 264352/1000000: episode: 1366, duration: 3.427s, episode steps: 330, steps per second:  96, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.645 [0.000, 3.000],  loss: 0.001650, mae: 0.206510, mean_q: 0.283130, mean_eps: 0.762233\n",
      " 264531/1000000: episode: 1367, duration: 1.964s, episode steps: 179, steps per second:  91, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.670 [0.000, 3.000],  loss: 0.001464, mae: 0.213956, mean_q: 0.295076, mean_eps: 0.762004\n",
      " 264712/1000000: episode: 1368, duration: 1.971s, episode steps: 181, steps per second:  92, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.652 [0.000, 3.000],  loss: 0.001352, mae: 0.214277, mean_q: 0.293991, mean_eps: 0.761842\n",
      " 265000/1000000: episode: 1369, duration: 3.049s, episode steps: 288, steps per second:  94, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.653 [0.000, 3.000],  loss: 0.001213, mae: 0.212462, mean_q: 0.291117, mean_eps: 0.761631\n",
      " 265182/1000000: episode: 1370, duration: 1.878s, episode steps: 182, steps per second:  97, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.808 [0.000, 3.000],  loss: 0.001553, mae: 0.207640, mean_q: 0.285372, mean_eps: 0.761419\n",
      " 265335/1000000: episode: 1371, duration: 1.575s, episode steps: 153, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.549 [0.000, 3.000],  loss: 0.001512, mae: 0.214400, mean_q: 0.296199, mean_eps: 0.761268\n",
      " 265534/1000000: episode: 1372, duration: 2.077s, episode steps: 199, steps per second:  96, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.001511, mae: 0.204160, mean_q: 0.281743, mean_eps: 0.761109\n",
      " 265665/1000000: episode: 1373, duration: 1.380s, episode steps: 131, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.450 [0.000, 3.000],  loss: 0.001487, mae: 0.203876, mean_q: 0.278410, mean_eps: 0.760960\n",
      " 265854/1000000: episode: 1374, duration: 1.943s, episode steps: 189, steps per second:  97, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.767 [0.000, 3.000],  loss: 0.001315, mae: 0.208629, mean_q: 0.287676, mean_eps: 0.760816\n",
      " 266057/1000000: episode: 1375, duration: 2.183s, episode steps: 203, steps per second:  93, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.473 [0.000, 3.000],  loss: 0.001396, mae: 0.216644, mean_q: 0.295686, mean_eps: 0.760640\n",
      " 266226/1000000: episode: 1376, duration: 1.788s, episode steps: 169, steps per second:  95, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.822 [0.000, 3.000],  loss: 0.000976, mae: 0.199521, mean_q: 0.274968, mean_eps: 0.760472\n",
      " 266444/1000000: episode: 1377, duration: 2.274s, episode steps: 218, steps per second:  96, episode reward:  3.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.619 [0.000, 3.000],  loss: 0.001242, mae: 0.200866, mean_q: 0.274795, mean_eps: 0.760299\n",
      " 266646/1000000: episode: 1378, duration: 2.115s, episode steps: 202, steps per second:  96, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.619 [0.000, 3.000],  loss: 0.001541, mae: 0.206472, mean_q: 0.284045, mean_eps: 0.760110\n",
      " 266876/1000000: episode: 1379, duration: 2.420s, episode steps: 230, steps per second:  95, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 0.001891, mae: 0.210530, mean_q: 0.288670, mean_eps: 0.759916\n",
      " 267040/1000000: episode: 1380, duration: 1.736s, episode steps: 164, steps per second:  94, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.701 [0.000, 3.000],  loss: 0.001354, mae: 0.205735, mean_q: 0.280324, mean_eps: 0.759740\n",
      " 267245/1000000: episode: 1381, duration: 2.160s, episode steps: 205, steps per second:  95, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 0.001049, mae: 0.200367, mean_q: 0.275510, mean_eps: 0.759572\n",
      " 267456/1000000: episode: 1382, duration: 2.254s, episode steps: 211, steps per second:  94, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.692 [0.000, 3.000],  loss: 0.001253, mae: 0.210389, mean_q: 0.291401, mean_eps: 0.759385\n",
      " 267627/1000000: episode: 1383, duration: 1.866s, episode steps: 171, steps per second:  92, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 0.001521, mae: 0.216922, mean_q: 0.294750, mean_eps: 0.759214\n",
      " 267842/1000000: episode: 1384, duration: 2.345s, episode steps: 215, steps per second:  92, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 0.001401, mae: 0.214549, mean_q: 0.293023, mean_eps: 0.759039\n",
      " 268044/1000000: episode: 1385, duration: 2.150s, episode steps: 202, steps per second:  94, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.842 [0.000, 3.000],  loss: 0.001686, mae: 0.217955, mean_q: 0.300565, mean_eps: 0.758852\n",
      " 268247/1000000: episode: 1386, duration: 2.117s, episode steps: 203, steps per second:  96, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.000938, mae: 0.209863, mean_q: 0.286362, mean_eps: 0.758670\n",
      " 268597/1000000: episode: 1387, duration: 3.644s, episode steps: 350, steps per second:  96, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.614 [0.000, 3.000],  loss: 0.001546, mae: 0.197968, mean_q: 0.270783, mean_eps: 0.758420\n",
      " 268832/1000000: episode: 1388, duration: 2.459s, episode steps: 235, steps per second:  96, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.553 [0.000, 3.000],  loss: 0.001167, mae: 0.218079, mean_q: 0.298439, mean_eps: 0.758157\n",
      " 269093/1000000: episode: 1389, duration: 2.791s, episode steps: 261, steps per second:  94, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.636 [0.000, 3.000],  loss: 0.001471, mae: 0.208585, mean_q: 0.284648, mean_eps: 0.757934\n",
      " 269343/1000000: episode: 1390, duration: 2.616s, episode steps: 250, steps per second:  96, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: 0.001587, mae: 0.221852, mean_q: 0.305320, mean_eps: 0.757704\n",
      " 269525/1000000: episode: 1391, duration: 1.980s, episode steps: 182, steps per second:  92, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.385 [0.000, 3.000],  loss: 0.001135, mae: 0.210257, mean_q: 0.287801, mean_eps: 0.757509\n",
      " 269710/1000000: episode: 1392, duration: 1.947s, episode steps: 185, steps per second:  95, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.686 [0.000, 3.000],  loss: 0.001006, mae: 0.194844, mean_q: 0.266770, mean_eps: 0.757344\n",
      " 269837/1000000: episode: 1393, duration: 1.359s, episode steps: 127, steps per second:  93, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.630 [0.000, 3.000],  loss: 0.001167, mae: 0.204792, mean_q: 0.278448, mean_eps: 0.757203\n",
      " 269962/1000000: episode: 1394, duration: 1.298s, episode steps: 125, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.664 [0.000, 3.000],  loss: 0.001331, mae: 0.219443, mean_q: 0.301267, mean_eps: 0.757090\n",
      " 270168/1000000: episode: 1395, duration: 2.200s, episode steps: 206, steps per second:  94, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: 0.004128, mae: 0.223040, mean_q: 0.304306, mean_eps: 0.756942\n",
      " 270460/1000000: episode: 1396, duration: 3.050s, episode steps: 292, steps per second:  96, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.733 [0.000, 3.000],  loss: 0.002443, mae: 0.220045, mean_q: 0.302791, mean_eps: 0.756719\n",
      " 270643/1000000: episode: 1397, duration: 2.023s, episode steps: 183, steps per second:  90, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.486 [0.000, 3.000],  loss: 0.002331, mae: 0.213013, mean_q: 0.290692, mean_eps: 0.756505\n",
      " 270803/1000000: episode: 1398, duration: 1.728s, episode steps: 160, steps per second:  93, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.637 [0.000, 3.000],  loss: 0.001352, mae: 0.216088, mean_q: 0.295048, mean_eps: 0.756350\n",
      " 271068/1000000: episode: 1399, duration: 2.776s, episode steps: 265, steps per second:  95, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.638 [0.000, 3.000],  loss: 0.001150, mae: 0.225568, mean_q: 0.310272, mean_eps: 0.756159\n",
      " 271342/1000000: episode: 1400, duration: 2.793s, episode steps: 274, steps per second:  98, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.737 [0.000, 3.000],  loss: 0.001993, mae: 0.212228, mean_q: 0.291383, mean_eps: 0.755916\n",
      " 271471/1000000: episode: 1401, duration: 1.315s, episode steps: 129, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.791 [0.000, 3.000],  loss: 0.002002, mae: 0.219460, mean_q: 0.302702, mean_eps: 0.755735\n",
      " 271687/1000000: episode: 1402, duration: 2.177s, episode steps: 216, steps per second:  99, episode reward:  3.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.597 [0.000, 3.000],  loss: 0.001545, mae: 0.212014, mean_q: 0.291069, mean_eps: 0.755580\n",
      " 271902/1000000: episode: 1403, duration: 2.288s, episode steps: 215, steps per second:  94, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.805 [0.000, 3.000],  loss: 0.001594, mae: 0.216102, mean_q: 0.299326, mean_eps: 0.755385\n",
      " 272195/1000000: episode: 1404, duration: 3.138s, episode steps: 293, steps per second:  93, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.751 [0.000, 3.000],  loss: 0.001948, mae: 0.220923, mean_q: 0.304410, mean_eps: 0.755157\n",
      " 272389/1000000: episode: 1405, duration: 2.079s, episode steps: 194, steps per second:  93, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 0.001750, mae: 0.212392, mean_q: 0.290898, mean_eps: 0.754937\n",
      " 272580/1000000: episode: 1406, duration: 2.030s, episode steps: 191, steps per second:  94, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.002175, mae: 0.219366, mean_q: 0.301042, mean_eps: 0.754764\n",
      " 272870/1000000: episode: 1407, duration: 3.016s, episode steps: 290, steps per second:  96, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.001525, mae: 0.225031, mean_q: 0.307773, mean_eps: 0.754548\n",
      " 273032/1000000: episode: 1408, duration: 1.697s, episode steps: 162, steps per second:  95, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.001429, mae: 0.213646, mean_q: 0.291944, mean_eps: 0.754345\n",
      " 273192/1000000: episode: 1409, duration: 1.715s, episode steps: 160, steps per second:  93, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.431 [0.000, 3.000],  loss: 0.001579, mae: 0.228336, mean_q: 0.308007, mean_eps: 0.754201\n",
      " 273323/1000000: episode: 1410, duration: 1.352s, episode steps: 131, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.328 [0.000, 3.000],  loss: 0.001411, mae: 0.209920, mean_q: 0.288465, mean_eps: 0.754070\n",
      " 273470/1000000: episode: 1411, duration: 1.507s, episode steps: 147, steps per second:  98, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.551 [0.000, 3.000],  loss: 0.001253, mae: 0.209005, mean_q: 0.284527, mean_eps: 0.753944\n",
      " 273629/1000000: episode: 1412, duration: 1.777s, episode steps: 159, steps per second:  89, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.002181, mae: 0.221453, mean_q: 0.303090, mean_eps: 0.753805\n",
      " 273790/1000000: episode: 1413, duration: 1.831s, episode steps: 161, steps per second:  88, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.708 [0.000, 3.000],  loss: 0.001224, mae: 0.223245, mean_q: 0.305528, mean_eps: 0.753661\n",
      " 273949/1000000: episode: 1414, duration: 1.681s, episode steps: 159, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.774 [0.000, 3.000],  loss: 0.000843, mae: 0.207225, mean_q: 0.285232, mean_eps: 0.753517\n",
      " 274176/1000000: episode: 1415, duration: 2.373s, episode steps: 227, steps per second:  96, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 0.001248, mae: 0.215999, mean_q: 0.294234, mean_eps: 0.753344\n",
      " 274440/1000000: episode: 1416, duration: 2.749s, episode steps: 264, steps per second:  96, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.001301, mae: 0.212250, mean_q: 0.291601, mean_eps: 0.753125\n",
      " 274646/1000000: episode: 1417, duration: 2.208s, episode steps: 206, steps per second:  93, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.001284, mae: 0.210359, mean_q: 0.286126, mean_eps: 0.752912\n",
      " 274894/1000000: episode: 1418, duration: 2.642s, episode steps: 248, steps per second:  94, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.665 [0.000, 3.000],  loss: 0.001470, mae: 0.223766, mean_q: 0.310013, mean_eps: 0.752707\n",
      " 275180/1000000: episode: 1419, duration: 3.064s, episode steps: 286, steps per second:  93, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.619 [0.000, 3.000],  loss: 0.001329, mae: 0.219923, mean_q: 0.301276, mean_eps: 0.752468\n",
      " 275410/1000000: episode: 1420, duration: 2.500s, episode steps: 230, steps per second:  92, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.700 [0.000, 3.000],  loss: 0.001555, mae: 0.230692, mean_q: 0.317413, mean_eps: 0.752235\n",
      " 275548/1000000: episode: 1421, duration: 1.438s, episode steps: 138, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001336, mae: 0.215947, mean_q: 0.295729, mean_eps: 0.752070\n",
      " 275782/1000000: episode: 1422, duration: 2.411s, episode steps: 234, steps per second:  97, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.001281, mae: 0.209405, mean_q: 0.288055, mean_eps: 0.751902\n",
      " 275997/1000000: episode: 1423, duration: 2.259s, episode steps: 215, steps per second:  95, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.637 [0.000, 3.000],  loss: 0.001327, mae: 0.214025, mean_q: 0.293561, mean_eps: 0.751699\n",
      " 276168/1000000: episode: 1424, duration: 1.787s, episode steps: 171, steps per second:  96, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.883 [0.000, 3.000],  loss: 0.001609, mae: 0.214995, mean_q: 0.297973, mean_eps: 0.751526\n",
      " 276448/1000000: episode: 1425, duration: 3.127s, episode steps: 280, steps per second:  90, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.582 [0.000, 3.000],  loss: 0.001317, mae: 0.216363, mean_q: 0.297181, mean_eps: 0.751325\n",
      " 276703/1000000: episode: 1426, duration: 2.803s, episode steps: 255, steps per second:  91, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.404 [0.000, 3.000],  loss: 0.001399, mae: 0.216094, mean_q: 0.296156, mean_eps: 0.751083\n",
      " 276885/1000000: episode: 1427, duration: 2.055s, episode steps: 182, steps per second:  89, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.758 [0.000, 3.000],  loss: 0.000774, mae: 0.210463, mean_q: 0.288799, mean_eps: 0.750885\n",
      " 277043/1000000: episode: 1428, duration: 1.656s, episode steps: 158, steps per second:  95, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.734 [0.000, 3.000],  loss: 0.001068, mae: 0.227963, mean_q: 0.312383, mean_eps: 0.750732\n",
      " 277210/1000000: episode: 1429, duration: 1.838s, episode steps: 167, steps per second:  91, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.389 [0.000, 3.000],  loss: 0.001694, mae: 0.211474, mean_q: 0.292921, mean_eps: 0.750587\n",
      " 277415/1000000: episode: 1430, duration: 2.186s, episode steps: 205, steps per second:  94, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 0.001549, mae: 0.212553, mean_q: 0.292839, mean_eps: 0.750419\n",
      " 277833/1000000: episode: 1431, duration: 4.316s, episode steps: 418, steps per second:  97, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.586 [0.000, 3.000],  loss: 0.001452, mae: 0.219375, mean_q: 0.304469, mean_eps: 0.750138\n",
      " 278022/1000000: episode: 1432, duration: 1.981s, episode steps: 189, steps per second:  95, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.582 [0.000, 3.000],  loss: 0.001781, mae: 0.225460, mean_q: 0.309430, mean_eps: 0.749865\n",
      " 278214/1000000: episode: 1433, duration: 2.222s, episode steps: 192, steps per second:  86, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.635 [0.000, 3.000],  loss: 0.001105, mae: 0.221224, mean_q: 0.303620, mean_eps: 0.749694\n",
      " 278424/1000000: episode: 1434, duration: 2.231s, episode steps: 210, steps per second:  94, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.705 [0.000, 3.000],  loss: 0.001209, mae: 0.222915, mean_q: 0.306779, mean_eps: 0.749514\n",
      " 278701/1000000: episode: 1435, duration: 2.978s, episode steps: 277, steps per second:  93, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.596 [0.000, 3.000],  loss: 0.001257, mae: 0.223782, mean_q: 0.307342, mean_eps: 0.749294\n",
      " 278857/1000000: episode: 1436, duration: 1.645s, episode steps: 156, steps per second:  95, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.308 [0.000, 3.000],  loss: 0.001294, mae: 0.214407, mean_q: 0.295686, mean_eps: 0.749098\n",
      " 279030/1000000: episode: 1437, duration: 1.826s, episode steps: 173, steps per second:  95, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.705 [0.000, 3.000],  loss: 0.001595, mae: 0.228292, mean_q: 0.313723, mean_eps: 0.748950\n",
      " 279322/1000000: episode: 1438, duration: 2.986s, episode steps: 292, steps per second:  98, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.767 [0.000, 3.000],  loss: 0.001371, mae: 0.218917, mean_q: 0.300020, mean_eps: 0.748742\n",
      " 279606/1000000: episode: 1439, duration: 2.996s, episode steps: 284, steps per second:  95, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.574 [0.000, 3.000],  loss: 0.001491, mae: 0.228683, mean_q: 0.315385, mean_eps: 0.748482\n",
      " 279860/1000000: episode: 1440, duration: 2.765s, episode steps: 254, steps per second:  92, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.001827, mae: 0.208752, mean_q: 0.286586, mean_eps: 0.748241\n",
      " 280154/1000000: episode: 1441, duration: 3.067s, episode steps: 294, steps per second:  96, episode reward:  5.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: 0.002796, mae: 0.232113, mean_q: 0.315645, mean_eps: 0.747995\n",
      " 280346/1000000: episode: 1442, duration: 1.987s, episode steps: 192, steps per second:  97, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.443 [0.000, 3.000],  loss: 0.005063, mae: 0.237531, mean_q: 0.322176, mean_eps: 0.747775\n",
      " 280624/1000000: episode: 1443, duration: 2.912s, episode steps: 278, steps per second:  95, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 0.002865, mae: 0.248517, mean_q: 0.340196, mean_eps: 0.747564\n",
      " 280851/1000000: episode: 1444, duration: 2.380s, episode steps: 227, steps per second:  95, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.714 [0.000, 3.000],  loss: 0.001526, mae: 0.233764, mean_q: 0.321952, mean_eps: 0.747338\n",
      " 281016/1000000: episode: 1445, duration: 1.785s, episode steps: 165, steps per second:  92, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.533 [0.000, 3.000],  loss: 0.001537, mae: 0.232669, mean_q: 0.320170, mean_eps: 0.747161\n",
      " 281167/1000000: episode: 1446, duration: 1.665s, episode steps: 151, steps per second:  91, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.861 [0.000, 3.000],  loss: 0.001517, mae: 0.227570, mean_q: 0.312113, mean_eps: 0.747019\n",
      " 281388/1000000: episode: 1447, duration: 2.408s, episode steps: 221, steps per second:  92, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.624 [0.000, 3.000],  loss: 0.001455, mae: 0.237399, mean_q: 0.325435, mean_eps: 0.746852\n",
      " 281519/1000000: episode: 1448, duration: 1.385s, episode steps: 131, steps per second:  95, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.573 [0.000, 3.000],  loss: 0.001324, mae: 0.239904, mean_q: 0.331155, mean_eps: 0.746693\n",
      " 281683/1000000: episode: 1449, duration: 1.735s, episode steps: 164, steps per second:  95, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: 0.001412, mae: 0.223403, mean_q: 0.306917, mean_eps: 0.746560\n",
      " 281907/1000000: episode: 1450, duration: 2.332s, episode steps: 224, steps per second:  96, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.001665, mae: 0.219718, mean_q: 0.300011, mean_eps: 0.746385\n",
      " 282105/1000000: episode: 1451, duration: 2.077s, episode steps: 198, steps per second:  95, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 0.002052, mae: 0.234018, mean_q: 0.320240, mean_eps: 0.746195\n",
      " 282400/1000000: episode: 1452, duration: 3.038s, episode steps: 295, steps per second:  97, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.678 [0.000, 3.000],  loss: 0.001441, mae: 0.231441, mean_q: 0.319375, mean_eps: 0.745973\n",
      " 282567/1000000: episode: 1453, duration: 1.735s, episode steps: 167, steps per second:  96, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.689 [0.000, 3.000],  loss: 0.001702, mae: 0.226033, mean_q: 0.308245, mean_eps: 0.745766\n",
      " 282857/1000000: episode: 1454, duration: 3.202s, episode steps: 290, steps per second:  91, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.603 [0.000, 3.000],  loss: 0.001660, mae: 0.230351, mean_q: 0.314870, mean_eps: 0.745559\n",
      " 283042/1000000: episode: 1455, duration: 1.953s, episode steps: 185, steps per second:  95, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.876 [0.000, 3.000],  loss: 0.001062, mae: 0.224859, mean_q: 0.307147, mean_eps: 0.745345\n",
      " 283250/1000000: episode: 1456, duration: 2.170s, episode steps: 208, steps per second:  96, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.846 [0.000, 3.000],  loss: 0.001382, mae: 0.232807, mean_q: 0.317933, mean_eps: 0.745169\n",
      " 283494/1000000: episode: 1457, duration: 2.509s, episode steps: 244, steps per second:  97, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.537 [0.000, 3.000],  loss: 0.002209, mae: 0.248278, mean_q: 0.340241, mean_eps: 0.744965\n",
      " 283760/1000000: episode: 1458, duration: 2.807s, episode steps: 266, steps per second:  95, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.002225, mae: 0.242715, mean_q: 0.332687, mean_eps: 0.744737\n",
      " 284054/1000000: episode: 1459, duration: 3.101s, episode steps: 294, steps per second:  95, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.001290, mae: 0.231375, mean_q: 0.317466, mean_eps: 0.744485\n",
      " 284191/1000000: episode: 1460, duration: 1.513s, episode steps: 137, steps per second:  91, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.679 [0.000, 3.000],  loss: 0.001950, mae: 0.226644, mean_q: 0.310125, mean_eps: 0.744290\n",
      " 284370/1000000: episode: 1461, duration: 1.988s, episode steps: 179, steps per second:  90, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.508 [0.000, 3.000],  loss: 0.001862, mae: 0.228294, mean_q: 0.308520, mean_eps: 0.744148\n",
      " 284603/1000000: episode: 1462, duration: 2.419s, episode steps: 233, steps per second:  96, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.001279, mae: 0.229895, mean_q: 0.312692, mean_eps: 0.743963\n",
      " 284766/1000000: episode: 1463, duration: 1.693s, episode steps: 163, steps per second:  96, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.644 [0.000, 3.000],  loss: 0.001251, mae: 0.230425, mean_q: 0.314243, mean_eps: 0.743784\n",
      " 284999/1000000: episode: 1464, duration: 2.384s, episode steps: 233, steps per second:  98, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.781 [0.000, 3.000],  loss: 0.001587, mae: 0.243323, mean_q: 0.334098, mean_eps: 0.743606\n",
      " 285216/1000000: episode: 1465, duration: 2.320s, episode steps: 217, steps per second:  94, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.705 [0.000, 3.000],  loss: 0.001537, mae: 0.229844, mean_q: 0.318566, mean_eps: 0.743405\n",
      " 285595/1000000: episode: 1466, duration: 3.981s, episode steps: 379, steps per second:  95, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.001477, mae: 0.230235, mean_q: 0.316980, mean_eps: 0.743136\n",
      " 285737/1000000: episode: 1467, duration: 1.567s, episode steps: 142, steps per second:  91, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.345 [0.000, 3.000],  loss: 0.000912, mae: 0.227470, mean_q: 0.312772, mean_eps: 0.742901\n",
      " 285961/1000000: episode: 1468, duration: 2.391s, episode steps: 224, steps per second:  94, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.603 [0.000, 3.000],  loss: 0.001291, mae: 0.239527, mean_q: 0.329193, mean_eps: 0.742735\n",
      " 286247/1000000: episode: 1469, duration: 2.936s, episode steps: 286, steps per second:  97, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 0.001298, mae: 0.238245, mean_q: 0.327104, mean_eps: 0.742506\n",
      " 286382/1000000: episode: 1470, duration: 1.472s, episode steps: 135, steps per second:  92, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.778 [0.000, 3.000],  loss: 0.001035, mae: 0.236081, mean_q: 0.322448, mean_eps: 0.742317\n",
      " 286607/1000000: episode: 1471, duration: 2.350s, episode steps: 225, steps per second:  96, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 0.001589, mae: 0.241146, mean_q: 0.330116, mean_eps: 0.742155\n",
      " 286839/1000000: episode: 1472, duration: 2.397s, episode steps: 232, steps per second:  97, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.629 [0.000, 3.000],  loss: 0.000995, mae: 0.222317, mean_q: 0.304444, mean_eps: 0.741950\n",
      " 287124/1000000: episode: 1473, duration: 2.965s, episode steps: 285, steps per second:  96, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: 0.001663, mae: 0.235353, mean_q: 0.323227, mean_eps: 0.741718\n",
      " 287255/1000000: episode: 1474, duration: 1.503s, episode steps: 131, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.366 [0.000, 3.000],  loss: 0.001771, mae: 0.230407, mean_q: 0.314178, mean_eps: 0.741531\n",
      " 287564/1000000: episode: 1475, duration: 3.345s, episode steps: 309, steps per second:  92, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.680 [0.000, 3.000],  loss: 0.001629, mae: 0.243251, mean_q: 0.334598, mean_eps: 0.741333\n",
      " 287695/1000000: episode: 1476, duration: 1.347s, episode steps: 131, steps per second:  97, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.473 [0.000, 3.000],  loss: 0.001551, mae: 0.246239, mean_q: 0.337320, mean_eps: 0.741135\n",
      " 287858/1000000: episode: 1477, duration: 1.684s, episode steps: 163, steps per second:  97, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.693 [0.000, 3.000],  loss: 0.001914, mae: 0.231483, mean_q: 0.318696, mean_eps: 0.741002\n",
      " 288094/1000000: episode: 1478, duration: 2.465s, episode steps: 236, steps per second:  96, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.636 [0.000, 3.000],  loss: 0.001817, mae: 0.242875, mean_q: 0.339211, mean_eps: 0.740822\n",
      " 288284/1000000: episode: 1479, duration: 2.010s, episode steps: 190, steps per second:  95, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.516 [0.000, 3.000],  loss: 0.001197, mae: 0.231728, mean_q: 0.319238, mean_eps: 0.740631\n",
      " 288474/1000000: episode: 1480, duration: 2.033s, episode steps: 190, steps per second:  93, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.695 [0.000, 3.000],  loss: 0.001324, mae: 0.253914, mean_q: 0.348094, mean_eps: 0.740460\n",
      " 288616/1000000: episode: 1481, duration: 1.529s, episode steps: 142, steps per second:  93, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.746 [0.000, 3.000],  loss: 0.001557, mae: 0.239018, mean_q: 0.328282, mean_eps: 0.740310\n",
      " 288846/1000000: episode: 1482, duration: 2.581s, episode steps: 230, steps per second:  89, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.613 [0.000, 3.000],  loss: 0.001498, mae: 0.244801, mean_q: 0.335999, mean_eps: 0.740143\n",
      " 288995/1000000: episode: 1483, duration: 1.557s, episode steps: 149, steps per second:  96, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.738 [0.000, 3.000],  loss: 0.001329, mae: 0.237154, mean_q: 0.327755, mean_eps: 0.739972\n",
      " 289219/1000000: episode: 1484, duration: 2.347s, episode steps: 224, steps per second:  95, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.661 [0.000, 3.000],  loss: 0.001329, mae: 0.227800, mean_q: 0.313450, mean_eps: 0.739805\n",
      " 289481/1000000: episode: 1485, duration: 2.944s, episode steps: 262, steps per second:  89, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.462 [0.000, 3.000],  loss: 0.001626, mae: 0.228055, mean_q: 0.311299, mean_eps: 0.739585\n",
      " 289686/1000000: episode: 1486, duration: 2.395s, episode steps: 205, steps per second:  86, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.824 [0.000, 3.000],  loss: 0.001576, mae: 0.230467, mean_q: 0.317599, mean_eps: 0.739374\n",
      " 289830/1000000: episode: 1487, duration: 1.753s, episode steps: 144, steps per second:  82, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.826 [0.000, 3.000],  loss: 0.001113, mae: 0.233211, mean_q: 0.318462, mean_eps: 0.739218\n",
      " 290037/1000000: episode: 1488, duration: 2.950s, episode steps: 207, steps per second:  70, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 0.002318, mae: 0.227153, mean_q: 0.312834, mean_eps: 0.739059\n",
      " 290292/1000000: episode: 1489, duration: 3.007s, episode steps: 255, steps per second:  85, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.753 [0.000, 3.000],  loss: 0.005357, mae: 0.269042, mean_q: 0.368735, mean_eps: 0.738852\n",
      " 290576/1000000: episode: 1490, duration: 3.062s, episode steps: 284, steps per second:  93, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.732 [0.000, 3.000],  loss: 0.001804, mae: 0.255655, mean_q: 0.354694, mean_eps: 0.738611\n",
      " 290814/1000000: episode: 1491, duration: 2.505s, episode steps: 238, steps per second:  95, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.706 [0.000, 3.000],  loss: 0.002135, mae: 0.252051, mean_q: 0.346107, mean_eps: 0.738375\n",
      " 291142/1000000: episode: 1492, duration: 3.427s, episode steps: 328, steps per second:  96, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.662 [0.000, 3.000],  loss: 0.001520, mae: 0.241724, mean_q: 0.333298, mean_eps: 0.738120\n",
      " 291277/1000000: episode: 1493, duration: 1.435s, episode steps: 135, steps per second:  94, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.726 [0.000, 3.000],  loss: 0.002325, mae: 0.249562, mean_q: 0.347786, mean_eps: 0.737911\n",
      " 291440/1000000: episode: 1494, duration: 1.747s, episode steps: 163, steps per second:  93, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.368 [0.000, 3.000],  loss: 0.002427, mae: 0.257955, mean_q: 0.355599, mean_eps: 0.737778\n",
      " 291710/1000000: episode: 1495, duration: 3.030s, episode steps: 270, steps per second:  89, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.002159, mae: 0.257476, mean_q: 0.354786, mean_eps: 0.737583\n",
      " 291933/1000000: episode: 1496, duration: 2.380s, episode steps: 223, steps per second:  94, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.704 [0.000, 3.000],  loss: 0.001986, mae: 0.263067, mean_q: 0.364961, mean_eps: 0.737360\n",
      " 292179/1000000: episode: 1497, duration: 2.626s, episode steps: 246, steps per second:  94, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.573 [0.000, 3.000],  loss: 0.002014, mae: 0.255150, mean_q: 0.351120, mean_eps: 0.737150\n",
      " 292486/1000000: episode: 1498, duration: 3.249s, episode steps: 307, steps per second:  95, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.687 [0.000, 3.000],  loss: 0.002606, mae: 0.256274, mean_q: 0.354308, mean_eps: 0.736901\n",
      " 292748/1000000: episode: 1499, duration: 2.790s, episode steps: 262, steps per second:  94, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.653 [0.000, 3.000],  loss: 0.001888, mae: 0.241293, mean_q: 0.332953, mean_eps: 0.736646\n",
      " 292938/1000000: episode: 1500, duration: 2.067s, episode steps: 190, steps per second:  92, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.711 [0.000, 3.000],  loss: 0.001713, mae: 0.265405, mean_q: 0.365887, mean_eps: 0.736442\n",
      " 293112/1000000: episode: 1501, duration: 1.963s, episode steps: 174, steps per second:  89, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.684 [0.000, 3.000],  loss: 0.001352, mae: 0.252387, mean_q: 0.345022, mean_eps: 0.736278\n",
      " 293328/1000000: episode: 1502, duration: 2.338s, episode steps: 216, steps per second:  92, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.806 [0.000, 3.000],  loss: 0.001582, mae: 0.241371, mean_q: 0.331442, mean_eps: 0.736104\n",
      " 293468/1000000: episode: 1503, duration: 1.505s, episode steps: 140, steps per second:  93, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.793 [0.000, 3.000],  loss: 0.001943, mae: 0.248319, mean_q: 0.343739, mean_eps: 0.735944\n",
      " 293630/1000000: episode: 1504, duration: 1.706s, episode steps: 162, steps per second:  95, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.586 [0.000, 3.000],  loss: 0.001331, mae: 0.247958, mean_q: 0.343205, mean_eps: 0.735807\n",
      " 293863/1000000: episode: 1505, duration: 2.451s, episode steps: 233, steps per second:  95, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.536 [0.000, 3.000],  loss: 0.001441, mae: 0.265441, mean_q: 0.368421, mean_eps: 0.735629\n",
      " 294110/1000000: episode: 1506, duration: 2.589s, episode steps: 247, steps per second:  95, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: 0.001838, mae: 0.259040, mean_q: 0.358628, mean_eps: 0.735413\n",
      " 294307/1000000: episode: 1507, duration: 2.092s, episode steps: 197, steps per second:  94, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.655 [0.000, 3.000],  loss: 0.001566, mae: 0.251192, mean_q: 0.346055, mean_eps: 0.735213\n",
      " 294652/1000000: episode: 1508, duration: 3.772s, episode steps: 345, steps per second:  91, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.678 [0.000, 3.000],  loss: 0.001628, mae: 0.257330, mean_q: 0.354838, mean_eps: 0.734970\n",
      " 294899/1000000: episode: 1509, duration: 2.619s, episode steps: 247, steps per second:  94, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.001475, mae: 0.252290, mean_q: 0.348001, mean_eps: 0.734703\n",
      " 295221/1000000: episode: 1510, duration: 3.550s, episode steps: 322, steps per second:  91, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.817 [0.000, 3.000],  loss: 0.001431, mae: 0.252765, mean_q: 0.351418, mean_eps: 0.734446\n",
      " 295501/1000000: episode: 1511, duration: 2.972s, episode steps: 280, steps per second:  94, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.001700, mae: 0.256944, mean_q: 0.355829, mean_eps: 0.734174\n",
      " 295871/1000000: episode: 1512, duration: 3.872s, episode steps: 370, steps per second:  96, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.001996, mae: 0.249182, mean_q: 0.347564, mean_eps: 0.733883\n",
      " 296004/1000000: episode: 1513, duration: 1.510s, episode steps: 133, steps per second:  88, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.496 [0.000, 3.000],  loss: 0.001320, mae: 0.241506, mean_q: 0.333078, mean_eps: 0.733658\n",
      " 296246/1000000: episode: 1514, duration: 2.718s, episode steps: 242, steps per second:  89, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.702 [0.000, 3.000],  loss: 0.001961, mae: 0.250386, mean_q: 0.348218, mean_eps: 0.733488\n",
      " 296555/1000000: episode: 1515, duration: 3.279s, episode steps: 309, steps per second:  94, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.770 [0.000, 3.000],  loss: 0.001491, mae: 0.257316, mean_q: 0.356803, mean_eps: 0.733240\n",
      " 296706/1000000: episode: 1516, duration: 1.616s, episode steps: 151, steps per second:  93, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.768 [0.000, 3.000],  loss: 0.001427, mae: 0.253489, mean_q: 0.352392, mean_eps: 0.733033\n",
      " 296912/1000000: episode: 1517, duration: 2.213s, episode steps: 206, steps per second:  93, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.699 [0.000, 3.000],  loss: 0.001224, mae: 0.250804, mean_q: 0.351044, mean_eps: 0.732873\n",
      " 297142/1000000: episode: 1518, duration: 2.445s, episode steps: 230, steps per second:  94, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 0.001354, mae: 0.237614, mean_q: 0.331343, mean_eps: 0.732677\n",
      " 297331/1000000: episode: 1519, duration: 1.977s, episode steps: 189, steps per second:  96, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.635 [0.000, 3.000],  loss: 0.001150, mae: 0.256565, mean_q: 0.355998, mean_eps: 0.732488\n",
      " 297620/1000000: episode: 1520, duration: 3.238s, episode steps: 289, steps per second:  89, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.664 [0.000, 3.000],  loss: 0.001334, mae: 0.251221, mean_q: 0.348187, mean_eps: 0.732273\n",
      " 297883/1000000: episode: 1521, duration: 3.127s, episode steps: 263, steps per second:  84, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.730 [0.000, 3.000],  loss: 0.001634, mae: 0.256491, mean_q: 0.354138, mean_eps: 0.732025\n",
      " 298114/1000000: episode: 1522, duration: 2.521s, episode steps: 231, steps per second:  92, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.662 [0.000, 3.000],  loss: 0.001142, mae: 0.247665, mean_q: 0.344400, mean_eps: 0.731802\n",
      " 298315/1000000: episode: 1523, duration: 2.241s, episode steps: 201, steps per second:  90, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.418 [0.000, 3.000],  loss: 0.001558, mae: 0.248440, mean_q: 0.342448, mean_eps: 0.731607\n",
      " 298557/1000000: episode: 1524, duration: 2.571s, episode steps: 242, steps per second:  94, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.653 [0.000, 3.000],  loss: 0.001490, mae: 0.252279, mean_q: 0.351107, mean_eps: 0.731408\n",
      " 298750/1000000: episode: 1525, duration: 2.043s, episode steps: 193, steps per second:  94, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.606 [0.000, 3.000],  loss: 0.000990, mae: 0.250614, mean_q: 0.350720, mean_eps: 0.731211\n",
      " 299031/1000000: episode: 1526, duration: 3.227s, episode steps: 281, steps per second:  87, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.662 [0.000, 3.000],  loss: 0.001668, mae: 0.249741, mean_q: 0.348841, mean_eps: 0.730999\n",
      " 299350/1000000: episode: 1527, duration: 3.469s, episode steps: 319, steps per second:  92, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.001580, mae: 0.250702, mean_q: 0.349143, mean_eps: 0.730729\n",
      " 299506/1000000: episode: 1528, duration: 1.661s, episode steps: 156, steps per second:  94, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001457, mae: 0.260941, mean_q: 0.360867, mean_eps: 0.730515\n",
      " 299687/1000000: episode: 1529, duration: 1.908s, episode steps: 181, steps per second:  95, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 0.001334, mae: 0.262863, mean_q: 0.363509, mean_eps: 0.730364\n",
      " 299853/1000000: episode: 1530, duration: 1.796s, episode steps: 166, steps per second:  92, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.705 [0.000, 3.000],  loss: 0.001479, mae: 0.236223, mean_q: 0.328981, mean_eps: 0.730207\n",
      " 300063/1000000: episode: 1531, duration: 2.154s, episode steps: 210, steps per second:  98, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.757 [0.000, 3.000],  loss: 0.003407, mae: 0.270447, mean_q: 0.373602, mean_eps: 0.730038\n",
      " 300317/1000000: episode: 1532, duration: 2.655s, episode steps: 254, steps per second:  96, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.004523, mae: 0.279662, mean_q: 0.387659, mean_eps: 0.729829\n",
      " 300573/1000000: episode: 1533, duration: 2.911s, episode steps: 256, steps per second:  88, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.003167, mae: 0.291573, mean_q: 0.398252, mean_eps: 0.729599\n",
      " 300745/1000000: episode: 1534, duration: 1.838s, episode steps: 172, steps per second:  94, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.002862, mae: 0.294413, mean_q: 0.409811, mean_eps: 0.729406\n",
      " 300980/1000000: episode: 1535, duration: 2.554s, episode steps: 235, steps per second:  92, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.745 [0.000, 3.000],  loss: 0.002954, mae: 0.279149, mean_q: 0.384331, mean_eps: 0.729224\n",
      " 301393/1000000: episode: 1536, duration: 4.399s, episode steps: 413, steps per second:  94, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.755 [0.000, 3.000],  loss: 0.002670, mae: 0.282588, mean_q: 0.388378, mean_eps: 0.728933\n",
      " 301653/1000000: episode: 1537, duration: 2.681s, episode steps: 260, steps per second:  97, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.677 [0.000, 3.000],  loss: 0.002583, mae: 0.287945, mean_q: 0.395929, mean_eps: 0.728628\n",
      " 301921/1000000: episode: 1538, duration: 2.831s, episode steps: 268, steps per second:  95, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 0.001978, mae: 0.279833, mean_q: 0.383736, mean_eps: 0.728391\n",
      " 302156/1000000: episode: 1539, duration: 2.573s, episode steps: 235, steps per second:  91, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.830 [0.000, 3.000],  loss: 0.002098, mae: 0.275466, mean_q: 0.381164, mean_eps: 0.728166\n",
      " 302332/1000000: episode: 1540, duration: 1.943s, episode steps: 176, steps per second:  91, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.801 [0.000, 3.000],  loss: 0.002132, mae: 0.269234, mean_q: 0.369419, mean_eps: 0.727982\n",
      " 302494/1000000: episode: 1541, duration: 1.716s, episode steps: 162, steps per second:  94, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.765 [0.000, 3.000],  loss: 0.002341, mae: 0.272151, mean_q: 0.376515, mean_eps: 0.727829\n",
      " 302640/1000000: episode: 1542, duration: 1.573s, episode steps: 146, steps per second:  93, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.740 [0.000, 3.000],  loss: 0.002200, mae: 0.288586, mean_q: 0.399108, mean_eps: 0.727691\n",
      " 302869/1000000: episode: 1543, duration: 2.451s, episode steps: 229, steps per second:  93, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.664 [0.000, 3.000],  loss: 0.002647, mae: 0.283753, mean_q: 0.392677, mean_eps: 0.727521\n",
      " 303161/1000000: episode: 1544, duration: 3.090s, episode steps: 292, steps per second:  95, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.860 [0.000, 3.000],  loss: 0.002341, mae: 0.273853, mean_q: 0.380302, mean_eps: 0.727286\n",
      " 303420/1000000: episode: 1545, duration: 2.835s, episode steps: 259, steps per second:  91, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.002087, mae: 0.278162, mean_q: 0.383229, mean_eps: 0.727039\n",
      " 303758/1000000: episode: 1546, duration: 3.649s, episode steps: 338, steps per second:  93, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.627 [0.000, 3.000],  loss: 0.001940, mae: 0.283768, mean_q: 0.392649, mean_eps: 0.726771\n",
      " 304025/1000000: episode: 1547, duration: 2.918s, episode steps: 267, steps per second:  91, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.637 [0.000, 3.000],  loss: 0.001645, mae: 0.276951, mean_q: 0.383604, mean_eps: 0.726497\n",
      " 304334/1000000: episode: 1548, duration: 3.308s, episode steps: 309, steps per second:  93, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.001697, mae: 0.278936, mean_q: 0.381848, mean_eps: 0.726238\n",
      " 304685/1000000: episode: 1549, duration: 3.693s, episode steps: 351, steps per second:  95, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.715 [0.000, 3.000],  loss: 0.001660, mae: 0.282988, mean_q: 0.390349, mean_eps: 0.725941\n",
      " 304826/1000000: episode: 1550, duration: 1.571s, episode steps: 141, steps per second:  90, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.809 [0.000, 3.000],  loss: 0.002359, mae: 0.289038, mean_q: 0.398051, mean_eps: 0.725720\n",
      " 304999/1000000: episode: 1551, duration: 1.975s, episode steps: 173, steps per second:  88, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.838 [0.000, 3.000],  loss: 0.001686, mae: 0.284602, mean_q: 0.401519, mean_eps: 0.725579\n",
      " 305385/1000000: episode: 1552, duration: 4.058s, episode steps: 386, steps per second:  95, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.661 [0.000, 3.000],  loss: 0.001534, mae: 0.283201, mean_q: 0.390697, mean_eps: 0.725327\n",
      " 305732/1000000: episode: 1553, duration: 3.706s, episode steps: 347, steps per second:  94, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.651 [0.000, 3.000],  loss: 0.002022, mae: 0.276218, mean_q: 0.381328, mean_eps: 0.724998\n",
      " 305909/1000000: episode: 1554, duration: 2.054s, episode steps: 177, steps per second:  86, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.655 [0.000, 3.000],  loss: 0.002191, mae: 0.286606, mean_q: 0.396491, mean_eps: 0.724762\n",
      " 306149/1000000: episode: 1555, duration: 2.563s, episode steps: 240, steps per second:  94, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.575 [0.000, 3.000],  loss: 0.001316, mae: 0.275778, mean_q: 0.381847, mean_eps: 0.724573\n",
      " 306401/1000000: episode: 1556, duration: 2.777s, episode steps: 252, steps per second:  91, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.552 [0.000, 3.000],  loss: 0.002117, mae: 0.285225, mean_q: 0.394333, mean_eps: 0.724352\n",
      " 306731/1000000: episode: 1557, duration: 3.493s, episode steps: 330, steps per second:  94, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.752 [0.000, 3.000],  loss: 0.001744, mae: 0.273106, mean_q: 0.379068, mean_eps: 0.724091\n",
      " 306982/1000000: episode: 1558, duration: 2.680s, episode steps: 251, steps per second:  94, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.757 [0.000, 3.000],  loss: 0.001696, mae: 0.274125, mean_q: 0.378108, mean_eps: 0.723830\n",
      " 307243/1000000: episode: 1559, duration: 2.775s, episode steps: 261, steps per second:  94, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.678 [0.000, 3.000],  loss: 0.001591, mae: 0.279480, mean_q: 0.385741, mean_eps: 0.723599\n",
      " 307686/1000000: episode: 1560, duration: 4.749s, episode steps: 443, steps per second:  93, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.797 [0.000, 3.000],  loss: 0.002102, mae: 0.283542, mean_q: 0.391261, mean_eps: 0.723282\n",
      " 307874/1000000: episode: 1561, duration: 2.053s, episode steps: 188, steps per second:  92, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 0.001639, mae: 0.293298, mean_q: 0.403497, mean_eps: 0.722998\n",
      " 308180/1000000: episode: 1562, duration: 3.297s, episode steps: 306, steps per second:  93, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.627 [0.000, 3.000],  loss: 0.001598, mae: 0.278969, mean_q: 0.387125, mean_eps: 0.722777\n",
      " 308383/1000000: episode: 1563, duration: 2.150s, episode steps: 203, steps per second:  94, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.002600, mae: 0.282736, mean_q: 0.393123, mean_eps: 0.722548\n",
      " 308629/1000000: episode: 1564, duration: 2.617s, episode steps: 246, steps per second:  94, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.001896, mae: 0.289659, mean_q: 0.398144, mean_eps: 0.722345\n",
      " 308951/1000000: episode: 1565, duration: 3.411s, episode steps: 322, steps per second:  94, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.637 [0.000, 3.000],  loss: 0.001986, mae: 0.291860, mean_q: 0.401406, mean_eps: 0.722089\n",
      " 309144/1000000: episode: 1566, duration: 2.101s, episode steps: 193, steps per second:  92, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.710 [0.000, 3.000],  loss: 0.001656, mae: 0.271024, mean_q: 0.373830, mean_eps: 0.721859\n",
      " 309459/1000000: episode: 1567, duration: 3.525s, episode steps: 315, steps per second:  89, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.001635, mae: 0.272641, mean_q: 0.378491, mean_eps: 0.721630\n",
      " 309608/1000000: episode: 1568, duration: 1.632s, episode steps: 149, steps per second:  91, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.711 [0.000, 3.000],  loss: 0.001964, mae: 0.270781, mean_q: 0.374256, mean_eps: 0.721421\n",
      " 310047/1000000: episode: 1569, duration: 4.699s, episode steps: 439, steps per second:  93, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.651 [0.000, 3.000],  loss: 0.002132, mae: 0.283321, mean_q: 0.389177, mean_eps: 0.721157\n",
      " 310360/1000000: episode: 1570, duration: 3.393s, episode steps: 313, steps per second:  92, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: 0.004575, mae: 0.303472, mean_q: 0.414521, mean_eps: 0.720818\n",
      " 310536/1000000: episode: 1571, duration: 1.911s, episode steps: 176, steps per second:  92, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.619 [0.000, 3.000],  loss: 0.002916, mae: 0.301132, mean_q: 0.414266, mean_eps: 0.720599\n",
      " 310806/1000000: episode: 1572, duration: 3.042s, episode steps: 270, steps per second:  89, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.774 [0.000, 3.000],  loss: 0.002982, mae: 0.310228, mean_q: 0.431317, mean_eps: 0.720397\n",
      " 311050/1000000: episode: 1573, duration: 2.676s, episode steps: 244, steps per second:  91, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.002672, mae: 0.294287, mean_q: 0.415073, mean_eps: 0.720165\n",
      " 311220/1000000: episode: 1574, duration: 1.899s, episode steps: 170, steps per second:  90, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.712 [0.000, 3.000],  loss: 0.002748, mae: 0.304823, mean_q: 0.425760, mean_eps: 0.719979\n",
      " 311416/1000000: episode: 1575, duration: 2.206s, episode steps: 196, steps per second:  89, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.694 [0.000, 3.000],  loss: 0.002593, mae: 0.308587, mean_q: 0.431155, mean_eps: 0.719816\n",
      " 311746/1000000: episode: 1576, duration: 3.595s, episode steps: 330, steps per second:  92, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.670 [0.000, 3.000],  loss: 0.002755, mae: 0.305582, mean_q: 0.422056, mean_eps: 0.719578\n",
      " 311943/1000000: episode: 1577, duration: 2.085s, episode steps: 197, steps per second:  94, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.726 [0.000, 3.000],  loss: 0.002512, mae: 0.286204, mean_q: 0.395808, mean_eps: 0.719340\n",
      " 312210/1000000: episode: 1578, duration: 2.900s, episode steps: 267, steps per second:  92, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.745 [0.000, 3.000],  loss: 0.002799, mae: 0.314725, mean_q: 0.438090, mean_eps: 0.719132\n",
      " 312421/1000000: episode: 1579, duration: 2.359s, episode steps: 211, steps per second:  89, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 0.001974, mae: 0.308789, mean_q: 0.426374, mean_eps: 0.718916\n",
      " 312647/1000000: episode: 1580, duration: 2.464s, episode steps: 226, steps per second:  92, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.002777, mae: 0.296698, mean_q: 0.407259, mean_eps: 0.718719\n",
      " 312877/1000000: episode: 1581, duration: 2.455s, episode steps: 230, steps per second:  94, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.700 [0.000, 3.000],  loss: 0.001871, mae: 0.302342, mean_q: 0.418339, mean_eps: 0.718514\n",
      " 313109/1000000: episode: 1582, duration: 2.495s, episode steps: 232, steps per second:  93, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.001933, mae: 0.315719, mean_q: 0.437427, mean_eps: 0.718305\n",
      " 313344/1000000: episode: 1583, duration: 2.547s, episode steps: 235, steps per second:  92, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.689 [0.000, 3.000],  loss: 0.001754, mae: 0.294320, mean_q: 0.408206, mean_eps: 0.718097\n",
      " 313652/1000000: episode: 1584, duration: 3.320s, episode steps: 308, steps per second:  93, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.002286, mae: 0.301297, mean_q: 0.420055, mean_eps: 0.717854\n",
      " 313921/1000000: episode: 1585, duration: 3.070s, episode steps: 269, steps per second:  88, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.001751, mae: 0.293555, mean_q: 0.410029, mean_eps: 0.717593\n",
      " 314141/1000000: episode: 1586, duration: 2.361s, episode steps: 220, steps per second:  93, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.632 [0.000, 3.000],  loss: 0.002452, mae: 0.303156, mean_q: 0.421027, mean_eps: 0.717371\n",
      " 314443/1000000: episode: 1587, duration: 3.197s, episode steps: 302, steps per second:  94, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.768 [0.000, 3.000],  loss: 0.001496, mae: 0.302027, mean_q: 0.418680, mean_eps: 0.717137\n",
      " 314645/1000000: episode: 1588, duration: 2.180s, episode steps: 202, steps per second:  93, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 0.001838, mae: 0.305470, mean_q: 0.421718, mean_eps: 0.716910\n",
      " 315035/1000000: episode: 1589, duration: 4.196s, episode steps: 390, steps per second:  93, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.654 [0.000, 3.000],  loss: 0.002261, mae: 0.302885, mean_q: 0.420909, mean_eps: 0.716644\n",
      " 315262/1000000: episode: 1590, duration: 2.559s, episode steps: 227, steps per second:  89, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.590 [0.000, 3.000],  loss: 0.002332, mae: 0.302977, mean_q: 0.421832, mean_eps: 0.716367\n",
      " 315510/1000000: episode: 1591, duration: 2.732s, episode steps: 248, steps per second:  91, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.617 [0.000, 3.000],  loss: 0.001920, mae: 0.308137, mean_q: 0.425871, mean_eps: 0.716153\n",
      " 315807/1000000: episode: 1592, duration: 3.162s, episode steps: 297, steps per second:  94, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 0.001849, mae: 0.314649, mean_q: 0.436223, mean_eps: 0.715908\n",
      " 315968/1000000: episode: 1593, duration: 1.765s, episode steps: 161, steps per second:  91, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.795 [0.000, 3.000],  loss: 0.002168, mae: 0.299412, mean_q: 0.415383, mean_eps: 0.715703\n",
      " 316267/1000000: episode: 1594, duration: 3.161s, episode steps: 299, steps per second:  95, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.602 [0.000, 3.000],  loss: 0.002366, mae: 0.302950, mean_q: 0.419821, mean_eps: 0.715496\n",
      " 316507/1000000: episode: 1595, duration: 2.555s, episode steps: 240, steps per second:  94, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.650 [0.000, 3.000],  loss: 0.001784, mae: 0.301285, mean_q: 0.417961, mean_eps: 0.715253\n",
      " 316742/1000000: episode: 1596, duration: 2.620s, episode steps: 235, steps per second:  90, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.821 [0.000, 3.000],  loss: 0.001718, mae: 0.292275, mean_q: 0.406656, mean_eps: 0.715038\n",
      " 316959/1000000: episode: 1597, duration: 2.393s, episode steps: 217, steps per second:  91, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.636 [0.000, 3.000],  loss: 0.001676, mae: 0.305795, mean_q: 0.424533, mean_eps: 0.714835\n",
      " 317263/1000000: episode: 1598, duration: 3.310s, episode steps: 304, steps per second:  92, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.681 [0.000, 3.000],  loss: 0.002185, mae: 0.304174, mean_q: 0.424261, mean_eps: 0.714601\n",
      " 317470/1000000: episode: 1599, duration: 2.265s, episode steps: 207, steps per second:  91, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.768 [0.000, 3.000],  loss: 0.002405, mae: 0.315844, mean_q: 0.438776, mean_eps: 0.714371\n",
      " 317820/1000000: episode: 1600, duration: 3.785s, episode steps: 350, steps per second:  92, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.660 [0.000, 3.000],  loss: 0.001836, mae: 0.303715, mean_q: 0.421472, mean_eps: 0.714120\n",
      " 318037/1000000: episode: 1601, duration: 2.327s, episode steps: 217, steps per second:  93, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.751 [0.000, 3.000],  loss: 0.002359, mae: 0.304118, mean_q: 0.426914, mean_eps: 0.713865\n",
      " 318219/1000000: episode: 1602, duration: 2.028s, episode steps: 182, steps per second:  90, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 0.001612, mae: 0.290764, mean_q: 0.404435, mean_eps: 0.713685\n",
      " 318482/1000000: episode: 1603, duration: 2.799s, episode steps: 263, steps per second:  94, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.002162, mae: 0.306894, mean_q: 0.426304, mean_eps: 0.713485\n",
      " 318664/1000000: episode: 1604, duration: 1.931s, episode steps: 182, steps per second:  94, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.791 [0.000, 3.000],  loss: 0.001335, mae: 0.308358, mean_q: 0.429372, mean_eps: 0.713285\n",
      " 318959/1000000: episode: 1605, duration: 3.145s, episode steps: 295, steps per second:  94, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.875 [0.000, 3.000],  loss: 0.002367, mae: 0.305552, mean_q: 0.429587, mean_eps: 0.713071\n",
      " 319162/1000000: episode: 1606, duration: 2.173s, episode steps: 203, steps per second:  93, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.793 [0.000, 3.000],  loss: 0.002074, mae: 0.296987, mean_q: 0.412870, mean_eps: 0.712846\n",
      " 319428/1000000: episode: 1607, duration: 2.829s, episode steps: 266, steps per second:  94, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 0.002341, mae: 0.311110, mean_q: 0.432368, mean_eps: 0.712635\n",
      " 319711/1000000: episode: 1608, duration: 3.107s, episode steps: 283, steps per second:  91, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.834 [0.000, 3.000],  loss: 0.001628, mae: 0.298701, mean_q: 0.415976, mean_eps: 0.712389\n",
      " 320088/1000000: episode: 1609, duration: 4.077s, episode steps: 377, steps per second:  92, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.684 [0.000, 3.000],  loss: 0.002818, mae: 0.305959, mean_q: 0.422329, mean_eps: 0.712092\n",
      " 320292/1000000: episode: 1610, duration: 2.239s, episode steps: 204, steps per second:  91, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.005569, mae: 0.333794, mean_q: 0.456910, mean_eps: 0.711831\n",
      " 320530/1000000: episode: 1611, duration: 2.562s, episode steps: 238, steps per second:  93, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.660 [0.000, 3.000],  loss: 0.003586, mae: 0.353197, mean_q: 0.490986, mean_eps: 0.711631\n",
      " 320851/1000000: episode: 1612, duration: 3.409s, episode steps: 321, steps per second:  94, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.648 [0.000, 3.000],  loss: 0.003049, mae: 0.338014, mean_q: 0.471716, mean_eps: 0.711379\n",
      " 321240/1000000: episode: 1613, duration: 4.284s, episode steps: 389, steps per second:  91, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.661 [0.000, 3.000],  loss: 0.002857, mae: 0.332612, mean_q: 0.462284, mean_eps: 0.711060\n",
      " 321405/1000000: episode: 1614, duration: 1.775s, episode steps: 165, steps per second:  93, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.782 [0.000, 3.000],  loss: 0.002401, mae: 0.342093, mean_q: 0.475816, mean_eps: 0.710810\n",
      " 321642/1000000: episode: 1615, duration: 2.515s, episode steps: 237, steps per second:  94, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.650 [0.000, 3.000],  loss: 0.002817, mae: 0.334049, mean_q: 0.466820, mean_eps: 0.710628\n",
      " 321843/1000000: episode: 1616, duration: 2.139s, episode steps: 201, steps per second:  94, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002697, mae: 0.335272, mean_q: 0.463565, mean_eps: 0.710432\n",
      " 322123/1000000: episode: 1617, duration: 2.945s, episode steps: 280, steps per second:  95, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.002150, mae: 0.334942, mean_q: 0.465975, mean_eps: 0.710216\n",
      " 322339/1000000: episode: 1618, duration: 2.358s, episode steps: 216, steps per second:  92, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.741 [0.000, 3.000],  loss: 0.002144, mae: 0.330673, mean_q: 0.463170, mean_eps: 0.709993\n",
      " 322625/1000000: episode: 1619, duration: 3.119s, episode steps: 286, steps per second:  92, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: 0.001786, mae: 0.343051, mean_q: 0.478007, mean_eps: 0.709766\n",
      " 322926/1000000: episode: 1620, duration: 3.290s, episode steps: 301, steps per second:  91, episode reward:  5.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.651 [0.000, 3.000],  loss: 0.002359, mae: 0.334196, mean_q: 0.465318, mean_eps: 0.709502\n",
      " 323296/1000000: episode: 1621, duration: 3.925s, episode steps: 370, steps per second:  94, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.611 [0.000, 3.000],  loss: 0.002060, mae: 0.336163, mean_q: 0.465921, mean_eps: 0.709201\n",
      " 323532/1000000: episode: 1622, duration: 2.559s, episode steps: 236, steps per second:  92, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.729 [0.000, 3.000],  loss: 0.001862, mae: 0.320863, mean_q: 0.445530, mean_eps: 0.708929\n",
      " 323761/1000000: episode: 1623, duration: 2.491s, episode steps: 229, steps per second:  92, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.721 [0.000, 3.000],  loss: 0.001967, mae: 0.331769, mean_q: 0.461963, mean_eps: 0.708719\n",
      " 323985/1000000: episode: 1624, duration: 2.458s, episode steps: 224, steps per second:  91, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.732 [0.000, 3.000],  loss: 0.002452, mae: 0.325288, mean_q: 0.454274, mean_eps: 0.708513\n",
      " 324196/1000000: episode: 1625, duration: 2.396s, episode steps: 211, steps per second:  88, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.602 [0.000, 3.000],  loss: 0.002224, mae: 0.341893, mean_q: 0.470495, mean_eps: 0.708319\n",
      " 324547/1000000: episode: 1626, duration: 3.851s, episode steps: 351, steps per second:  91, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.812 [0.000, 3.000],  loss: 0.001946, mae: 0.341664, mean_q: 0.476654, mean_eps: 0.708067\n",
      " 324829/1000000: episode: 1627, duration: 3.013s, episode steps: 282, steps per second:  94, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.635 [0.000, 3.000],  loss: 0.002103, mae: 0.322522, mean_q: 0.449871, mean_eps: 0.707781\n",
      " 325169/1000000: episode: 1628, duration: 3.585s, episode steps: 340, steps per second:  95, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.647 [0.000, 3.000],  loss: 0.002478, mae: 0.332062, mean_q: 0.464299, mean_eps: 0.707500\n",
      " 325379/1000000: episode: 1629, duration: 2.214s, episode steps: 210, steps per second:  95, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.767 [0.000, 3.000],  loss: 0.002105, mae: 0.329821, mean_q: 0.459862, mean_eps: 0.707253\n",
      " 325795/1000000: episode: 1630, duration: 4.624s, episode steps: 416, steps per second:  90, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.702 [0.000, 3.000],  loss: 0.002318, mae: 0.333376, mean_q: 0.463365, mean_eps: 0.706973\n",
      " 325983/1000000: episode: 1631, duration: 2.036s, episode steps: 188, steps per second:  92, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.824 [0.000, 3.000],  loss: 0.002459, mae: 0.337855, mean_q: 0.471552, mean_eps: 0.706701\n",
      " 326252/1000000: episode: 1632, duration: 2.912s, episode steps: 269, steps per second:  92, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.669 [0.000, 3.000],  loss: 0.002592, mae: 0.339759, mean_q: 0.474626, mean_eps: 0.706496\n",
      " 326505/1000000: episode: 1633, duration: 2.708s, episode steps: 253, steps per second:  93, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.676 [0.000, 3.000],  loss: 0.002563, mae: 0.338139, mean_q: 0.469987, mean_eps: 0.706260\n",
      " 326759/1000000: episode: 1634, duration: 2.661s, episode steps: 254, steps per second:  95, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.791 [0.000, 3.000],  loss: 0.002138, mae: 0.322745, mean_q: 0.449786, mean_eps: 0.706031\n",
      " 327085/1000000: episode: 1635, duration: 3.649s, episode steps: 326, steps per second:  89, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.712 [0.000, 3.000],  loss: 0.002555, mae: 0.346426, mean_q: 0.481827, mean_eps: 0.705770\n",
      " 327431/1000000: episode: 1636, duration: 3.772s, episode steps: 346, steps per second:  92, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.731 [0.000, 3.000],  loss: 0.002223, mae: 0.325886, mean_q: 0.455280, mean_eps: 0.705468\n",
      " 327749/1000000: episode: 1637, duration: 3.479s, episode steps: 318, steps per second:  91, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.698 [0.000, 3.000],  loss: 0.001726, mae: 0.331153, mean_q: 0.462781, mean_eps: 0.705169\n",
      " 328055/1000000: episode: 1638, duration: 3.270s, episode steps: 306, steps per second:  94, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.647 [0.000, 3.000],  loss: 0.002359, mae: 0.332665, mean_q: 0.462444, mean_eps: 0.704888\n",
      " 328191/1000000: episode: 1639, duration: 1.475s, episode steps: 136, steps per second:  92, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.588 [0.000, 3.000],  loss: 0.002887, mae: 0.348172, mean_q: 0.485916, mean_eps: 0.704690\n",
      " 328424/1000000: episode: 1640, duration: 2.543s, episode steps: 233, steps per second:  92, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002294, mae: 0.339278, mean_q: 0.472297, mean_eps: 0.704525\n",
      " 328695/1000000: episode: 1641, duration: 3.011s, episode steps: 271, steps per second:  90, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.620 [0.000, 3.000],  loss: 0.002154, mae: 0.350424, mean_q: 0.487859, mean_eps: 0.704298\n",
      " 328871/1000000: episode: 1642, duration: 1.904s, episode steps: 176, steps per second:  92, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.665 [0.000, 3.000],  loss: 0.002399, mae: 0.319097, mean_q: 0.444685, mean_eps: 0.704096\n",
      " 329246/1000000: episode: 1643, duration: 3.994s, episode steps: 375, steps per second:  94, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.725 [0.000, 3.000],  loss: 0.001937, mae: 0.324043, mean_q: 0.452587, mean_eps: 0.703848\n",
      " 329552/1000000: episode: 1644, duration: 3.215s, episode steps: 306, steps per second:  95, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.686 [0.000, 3.000],  loss: 0.002030, mae: 0.334855, mean_q: 0.467845, mean_eps: 0.703542\n",
      " 329858/1000000: episode: 1645, duration: 3.353s, episode steps: 306, steps per second:  91, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.752 [0.000, 3.000],  loss: 0.002512, mae: 0.334745, mean_q: 0.466163, mean_eps: 0.703266\n",
      " 330069/1000000: episode: 1646, duration: 2.418s, episode steps: 211, steps per second:  87, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.825 [0.000, 3.000],  loss: 0.005333, mae: 0.348352, mean_q: 0.484161, mean_eps: 0.703032\n",
      " 330491/1000000: episode: 1647, duration: 4.570s, episode steps: 422, steps per second:  92, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.680 [0.000, 3.000],  loss: 0.004861, mae: 0.362096, mean_q: 0.501114, mean_eps: 0.702748\n",
      " 330800/1000000: episode: 1648, duration: 3.262s, episode steps: 309, steps per second:  95, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.806 [0.000, 3.000],  loss: 0.002855, mae: 0.355963, mean_q: 0.490480, mean_eps: 0.702420\n",
      " 330944/1000000: episode: 1649, duration: 1.533s, episode steps: 144, steps per second:  94, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002288, mae: 0.371115, mean_q: 0.509540, mean_eps: 0.702217\n",
      " 331174/1000000: episode: 1650, duration: 2.480s, episode steps: 230, steps per second:  93, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: 0.002887, mae: 0.347486, mean_q: 0.478252, mean_eps: 0.702048\n",
      " 331451/1000000: episode: 1651, duration: 3.033s, episode steps: 277, steps per second:  91, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.657 [0.000, 3.000],  loss: 0.002940, mae: 0.359857, mean_q: 0.498434, mean_eps: 0.701819\n",
      " 331636/1000000: episode: 1652, duration: 2.075s, episode steps: 185, steps per second:  89, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.492 [0.000, 3.000],  loss: 0.003266, mae: 0.363324, mean_q: 0.502259, mean_eps: 0.701612\n",
      " 331802/1000000: episode: 1653, duration: 1.928s, episode steps: 166, steps per second:  86, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.717 [0.000, 3.000],  loss: 0.002649, mae: 0.356060, mean_q: 0.491343, mean_eps: 0.701454\n",
      " 332079/1000000: episode: 1654, duration: 2.977s, episode steps: 277, steps per second:  93, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.002179, mae: 0.365485, mean_q: 0.505471, mean_eps: 0.701254\n",
      " 332294/1000000: episode: 1655, duration: 2.334s, episode steps: 215, steps per second:  92, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.693 [0.000, 3.000],  loss: 0.002492, mae: 0.354834, mean_q: 0.491194, mean_eps: 0.701033\n",
      " 332512/1000000: episode: 1656, duration: 2.343s, episode steps: 218, steps per second:  93, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.711 [0.000, 3.000],  loss: 0.002702, mae: 0.371270, mean_q: 0.513041, mean_eps: 0.700838\n",
      " 332749/1000000: episode: 1657, duration: 2.523s, episode steps: 237, steps per second:  94, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: 0.002205, mae: 0.367129, mean_q: 0.507698, mean_eps: 0.700633\n",
      " 332961/1000000: episode: 1658, duration: 2.333s, episode steps: 212, steps per second:  91, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.642 [0.000, 3.000],  loss: 0.002429, mae: 0.352760, mean_q: 0.486030, mean_eps: 0.700430\n",
      " 333296/1000000: episode: 1659, duration: 3.614s, episode steps: 335, steps per second:  93, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.678 [0.000, 3.000],  loss: 0.002462, mae: 0.352198, mean_q: 0.488317, mean_eps: 0.700185\n",
      " 333518/1000000: episode: 1660, duration: 2.381s, episode steps: 222, steps per second:  93, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.761 [0.000, 3.000],  loss: 0.002160, mae: 0.359335, mean_q: 0.498768, mean_eps: 0.699935\n",
      " 333712/1000000: episode: 1661, duration: 2.083s, episode steps: 194, steps per second:  93, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.680 [0.000, 3.000],  loss: 0.002765, mae: 0.356958, mean_q: 0.495019, mean_eps: 0.699747\n",
      " 333960/1000000: episode: 1662, duration: 2.654s, episode steps: 248, steps per second:  93, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.002529, mae: 0.369582, mean_q: 0.510154, mean_eps: 0.699549\n",
      " 334162/1000000: episode: 1663, duration: 2.190s, episode steps: 202, steps per second:  92, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.708 [0.000, 3.000],  loss: 0.002391, mae: 0.376831, mean_q: 0.521344, mean_eps: 0.699346\n",
      " 334412/1000000: episode: 1664, duration: 2.720s, episode steps: 250, steps per second:  92, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.796 [0.000, 3.000],  loss: 0.001975, mae: 0.360627, mean_q: 0.499399, mean_eps: 0.699143\n",
      " 334650/1000000: episode: 1665, duration: 2.707s, episode steps: 238, steps per second:  88, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.773 [0.000, 3.000],  loss: 0.001812, mae: 0.370358, mean_q: 0.513876, mean_eps: 0.698923\n",
      " 334948/1000000: episode: 1666, duration: 3.178s, episode steps: 298, steps per second:  94, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.738 [0.000, 3.000],  loss: 0.001902, mae: 0.358737, mean_q: 0.496803, mean_eps: 0.698682\n",
      " 335256/1000000: episode: 1667, duration: 3.360s, episode steps: 308, steps per second:  92, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.701 [0.000, 3.000],  loss: 0.002588, mae: 0.352352, mean_q: 0.492186, mean_eps: 0.698410\n",
      " 335418/1000000: episode: 1668, duration: 1.792s, episode steps: 162, steps per second:  90, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.704 [0.000, 3.000],  loss: 0.002016, mae: 0.361576, mean_q: 0.505551, mean_eps: 0.698198\n",
      " 335806/1000000: episode: 1669, duration: 4.149s, episode steps: 388, steps per second:  94, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: 0.002272, mae: 0.360102, mean_q: 0.498893, mean_eps: 0.697949\n",
      " 335970/1000000: episode: 1670, duration: 1.883s, episode steps: 164, steps per second:  87, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.726 [0.000, 3.000],  loss: 0.002309, mae: 0.365264, mean_q: 0.509901, mean_eps: 0.697701\n",
      " 336350/1000000: episode: 1671, duration: 4.045s, episode steps: 380, steps per second:  94, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.747 [0.000, 3.000],  loss: 0.002303, mae: 0.364549, mean_q: 0.506698, mean_eps: 0.697456\n",
      " 336611/1000000: episode: 1672, duration: 2.706s, episode steps: 261, steps per second:  96, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002256, mae: 0.373594, mean_q: 0.518163, mean_eps: 0.697168\n",
      " 336800/1000000: episode: 1673, duration: 2.023s, episode steps: 189, steps per second:  93, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002218, mae: 0.353954, mean_q: 0.489940, mean_eps: 0.696966\n",
      " 337160/1000000: episode: 1674, duration: 3.849s, episode steps: 360, steps per second:  94, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.001985, mae: 0.357215, mean_q: 0.495151, mean_eps: 0.696720\n",
      " 337396/1000000: episode: 1675, duration: 2.658s, episode steps: 236, steps per second:  89, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.725 [0.000, 3.000],  loss: 0.002404, mae: 0.357057, mean_q: 0.495534, mean_eps: 0.696452\n",
      " 337709/1000000: episode: 1676, duration: 3.462s, episode steps: 313, steps per second:  90, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.751 [0.000, 3.000],  loss: 0.002030, mae: 0.368739, mean_q: 0.515855, mean_eps: 0.696203\n",
      " 337990/1000000: episode: 1677, duration: 2.962s, episode steps: 281, steps per second:  95, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.569 [0.000, 3.000],  loss: 0.002338, mae: 0.363140, mean_q: 0.504636, mean_eps: 0.695935\n",
      " 338235/1000000: episode: 1678, duration: 2.615s, episode steps: 245, steps per second:  94, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.902 [0.000, 3.000],  loss: 0.001786, mae: 0.366730, mean_q: 0.511990, mean_eps: 0.695699\n",
      " 338572/1000000: episode: 1679, duration: 3.578s, episode steps: 337, steps per second:  94, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.680 [0.000, 3.000],  loss: 0.001969, mae: 0.359819, mean_q: 0.500404, mean_eps: 0.695438\n",
      " 338802/1000000: episode: 1680, duration: 2.564s, episode steps: 230, steps per second:  90, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.826 [0.000, 3.000],  loss: 0.002373, mae: 0.372597, mean_q: 0.519687, mean_eps: 0.695183\n",
      " 339323/1000000: episode: 1681, duration: 5.713s, episode steps: 521, steps per second:  91, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.582 [0.000, 3.000],  loss: 0.002176, mae: 0.362538, mean_q: 0.500781, mean_eps: 0.694844\n",
      " 339738/1000000: episode: 1682, duration: 4.474s, episode steps: 415, steps per second:  93, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.002250, mae: 0.362211, mean_q: 0.502683, mean_eps: 0.694423\n",
      " 340025/1000000: episode: 1683, duration: 3.096s, episode steps: 287, steps per second:  93, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.714 [0.000, 3.000],  loss: 0.001805, mae: 0.365670, mean_q: 0.504694, mean_eps: 0.694106\n",
      " 340195/1000000: episode: 1684, duration: 1.830s, episode steps: 170, steps per second:  93, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.765 [0.000, 3.000],  loss: 0.007300, mae: 0.404836, mean_q: 0.554583, mean_eps: 0.693901\n",
      " 340513/1000000: episode: 1685, duration: 3.644s, episode steps: 318, steps per second:  87, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.698 [0.000, 3.000],  loss: 0.004450, mae: 0.395136, mean_q: 0.544784, mean_eps: 0.693681\n",
      " 340783/1000000: episode: 1686, duration: 2.916s, episode steps: 270, steps per second:  93, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.744 [0.000, 3.000],  loss: 0.003031, mae: 0.396775, mean_q: 0.546090, mean_eps: 0.693417\n",
      " 341074/1000000: episode: 1687, duration: 3.149s, episode steps: 291, steps per second:  92, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.636 [0.000, 3.000],  loss: 0.002201, mae: 0.387568, mean_q: 0.537570, mean_eps: 0.693165\n",
      " 341478/1000000: episode: 1688, duration: 4.315s, episode steps: 404, steps per second:  94, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002858, mae: 0.380288, mean_q: 0.526018, mean_eps: 0.692852\n",
      " 341815/1000000: episode: 1689, duration: 3.753s, episode steps: 337, steps per second:  90, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.002575, mae: 0.397004, mean_q: 0.546254, mean_eps: 0.692519\n",
      " 342072/1000000: episode: 1690, duration: 2.871s, episode steps: 257, steps per second:  90, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.002316, mae: 0.411230, mean_q: 0.567178, mean_eps: 0.692252\n",
      " 342264/1000000: episode: 1691, duration: 2.076s, episode steps: 192, steps per second:  92, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.698 [0.000, 3.000],  loss: 0.001528, mae: 0.396884, mean_q: 0.548880, mean_eps: 0.692051\n",
      " 342543/1000000: episode: 1692, duration: 3.006s, episode steps: 279, steps per second:  93, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.699 [0.000, 3.000],  loss: 0.003162, mae: 0.388617, mean_q: 0.537773, mean_eps: 0.691838\n",
      " 342819/1000000: episode: 1693, duration: 3.029s, episode steps: 276, steps per second:  91, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.779 [0.000, 3.000],  loss: 0.002918, mae: 0.404064, mean_q: 0.557273, mean_eps: 0.691588\n",
      " 343119/1000000: episode: 1694, duration: 3.210s, episode steps: 300, steps per second:  93, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.413 [0.000, 3.000],  loss: 0.002221, mae: 0.387141, mean_q: 0.533573, mean_eps: 0.691329\n",
      " 343314/1000000: episode: 1695, duration: 2.215s, episode steps: 195, steps per second:  88, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.703 [0.000, 3.000],  loss: 0.002260, mae: 0.401683, mean_q: 0.556608, mean_eps: 0.691106\n",
      " 343771/1000000: episode: 1696, duration: 4.859s, episode steps: 457, steps per second:  94, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.799 [0.000, 3.000],  loss: 0.002497, mae: 0.393870, mean_q: 0.545002, mean_eps: 0.690812\n",
      " 344114/1000000: episode: 1697, duration: 3.738s, episode steps: 343, steps per second:  92, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.691 [0.000, 3.000],  loss: 0.002185, mae: 0.390137, mean_q: 0.538144, mean_eps: 0.690452\n",
      " 344337/1000000: episode: 1698, duration: 2.339s, episode steps: 223, steps per second:  95, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.664 [0.000, 3.000],  loss: 0.002323, mae: 0.402547, mean_q: 0.556697, mean_eps: 0.690197\n",
      " 344598/1000000: episode: 1699, duration: 2.763s, episode steps: 261, steps per second:  94, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.548 [0.000, 3.000],  loss: 0.002540, mae: 0.399537, mean_q: 0.553301, mean_eps: 0.689979\n",
      " 344857/1000000: episode: 1700, duration: 2.983s, episode steps: 259, steps per second:  87, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.656 [0.000, 3.000],  loss: 0.002331, mae: 0.395865, mean_q: 0.547295, mean_eps: 0.689745\n",
      " 345230/1000000: episode: 1701, duration: 4.018s, episode steps: 373, steps per second:  93, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 0.001789, mae: 0.381882, mean_q: 0.530521, mean_eps: 0.689460\n",
      " 345422/1000000: episode: 1702, duration: 2.042s, episode steps: 192, steps per second:  94, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.724 [0.000, 3.000],  loss: 0.002440, mae: 0.410415, mean_q: 0.567573, mean_eps: 0.689207\n",
      " 345784/1000000: episode: 1703, duration: 3.927s, episode steps: 362, steps per second:  92, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.638 [0.000, 3.000],  loss: 0.002306, mae: 0.398969, mean_q: 0.557153, mean_eps: 0.688958\n",
      " 346032/1000000: episode: 1704, duration: 2.695s, episode steps: 248, steps per second:  92, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.460 [0.000, 3.000],  loss: 0.002127, mae: 0.402092, mean_q: 0.556093, mean_eps: 0.688685\n",
      " 346502/1000000: episode: 1705, duration: 5.216s, episode steps: 470, steps per second:  90, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.657 [0.000, 3.000],  loss: 0.001826, mae: 0.398993, mean_q: 0.553297, mean_eps: 0.688361\n",
      " 346756/1000000: episode: 1706, duration: 2.713s, episode steps: 254, steps per second:  94, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.752 [0.000, 3.000],  loss: 0.002388, mae: 0.395400, mean_q: 0.547053, mean_eps: 0.688035\n",
      " 346969/1000000: episode: 1707, duration: 2.296s, episode steps: 213, steps per second:  93, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.737 [0.000, 3.000],  loss: 0.002457, mae: 0.416299, mean_q: 0.574800, mean_eps: 0.687824\n",
      " 347203/1000000: episode: 1708, duration: 2.491s, episode steps: 234, steps per second:  94, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.735 [0.000, 3.000],  loss: 0.002439, mae: 0.392614, mean_q: 0.547887, mean_eps: 0.687623\n",
      " 347597/1000000: episode: 1709, duration: 4.299s, episode steps: 394, steps per second:  92, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.706 [0.000, 3.000],  loss: 0.002652, mae: 0.389892, mean_q: 0.537833, mean_eps: 0.687340\n",
      " 347817/1000000: episode: 1710, duration: 2.524s, episode steps: 220, steps per second:  87, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.695 [0.000, 3.000],  loss: 0.002058, mae: 0.392036, mean_q: 0.544924, mean_eps: 0.687063\n",
      " 348147/1000000: episode: 1711, duration: 3.489s, episode steps: 330, steps per second:  95, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.658 [0.000, 3.000],  loss: 0.002581, mae: 0.400248, mean_q: 0.552101, mean_eps: 0.686816\n",
      " 348401/1000000: episode: 1712, duration: 2.690s, episode steps: 254, steps per second:  94, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.724 [0.000, 3.000],  loss: 0.002702, mae: 0.387152, mean_q: 0.534880, mean_eps: 0.686553\n",
      " 348642/1000000: episode: 1713, duration: 2.638s, episode steps: 241, steps per second:  91, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: 0.002240, mae: 0.400327, mean_q: 0.551119, mean_eps: 0.686330\n",
      " 349074/1000000: episode: 1714, duration: 4.635s, episode steps: 432, steps per second:  93, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.720 [0.000, 3.000],  loss: 0.002028, mae: 0.396536, mean_q: 0.549226, mean_eps: 0.686028\n",
      " 349267/1000000: episode: 1715, duration: 2.297s, episode steps: 193, steps per second:  84, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.772 [0.000, 3.000],  loss: 0.002838, mae: 0.393403, mean_q: 0.548706, mean_eps: 0.685747\n",
      " 349518/1000000: episode: 1716, duration: 2.730s, episode steps: 251, steps per second:  92, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.574 [0.000, 3.000],  loss: 0.002141, mae: 0.394547, mean_q: 0.545086, mean_eps: 0.685547\n",
      " 349872/1000000: episode: 1717, duration: 3.854s, episode steps: 354, steps per second:  92, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.681 [0.000, 3.000],  loss: 0.002481, mae: 0.394769, mean_q: 0.546569, mean_eps: 0.685275\n",
      " 350150/1000000: episode: 1718, duration: 3.054s, episode steps: 278, steps per second:  91, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.709 [0.000, 3.000],  loss: 0.004937, mae: 0.421025, mean_q: 0.575219, mean_eps: 0.684991\n",
      " 350378/1000000: episode: 1719, duration: 2.484s, episode steps: 228, steps per second:  92, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.614 [0.000, 3.000],  loss: 0.003405, mae: 0.424532, mean_q: 0.578742, mean_eps: 0.684762\n",
      " 350603/1000000: episode: 1720, duration: 2.491s, episode steps: 225, steps per second:  90, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.889 [0.000, 3.000],  loss: 0.003208, mae: 0.431924, mean_q: 0.592982, mean_eps: 0.684559\n",
      " 350978/1000000: episode: 1721, duration: 4.258s, episode steps: 375, steps per second:  88, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.651 [0.000, 3.000],  loss: 0.003078, mae: 0.427893, mean_q: 0.586187, mean_eps: 0.684289\n",
      " 351213/1000000: episode: 1722, duration: 2.572s, episode steps: 235, steps per second:  91, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.677 [0.000, 3.000],  loss: 0.002523, mae: 0.431577, mean_q: 0.595317, mean_eps: 0.684014\n",
      " 351562/1000000: episode: 1723, duration: 3.739s, episode steps: 349, steps per second:  93, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.642 [0.000, 3.000],  loss: 0.002732, mae: 0.430144, mean_q: 0.591635, mean_eps: 0.683751\n",
      " 351831/1000000: episode: 1724, duration: 2.888s, episode steps: 269, steps per second:  93, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.773 [0.000, 3.000],  loss: 0.002578, mae: 0.431962, mean_q: 0.594956, mean_eps: 0.683474\n",
      " 352011/1000000: episode: 1725, duration: 2.030s, episode steps: 180, steps per second:  89, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.739 [0.000, 3.000],  loss: 0.002930, mae: 0.431236, mean_q: 0.592438, mean_eps: 0.683272\n",
      " 352304/1000000: episode: 1726, duration: 3.362s, episode steps: 293, steps per second:  87, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.758 [0.000, 3.000],  loss: 0.002477, mae: 0.430579, mean_q: 0.593358, mean_eps: 0.683060\n",
      " 352623/1000000: episode: 1727, duration: 3.423s, episode steps: 319, steps per second:  93, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.821 [0.000, 3.000],  loss: 0.002306, mae: 0.434829, mean_q: 0.598814, mean_eps: 0.682784\n",
      " 353049/1000000: episode: 1728, duration: 4.620s, episode steps: 426, steps per second:  92, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.613 [0.000, 3.000],  loss: 0.002246, mae: 0.418869, mean_q: 0.578242, mean_eps: 0.682448\n",
      " 353193/1000000: episode: 1729, duration: 1.572s, episode steps: 144, steps per second:  92, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.688 [0.000, 3.000],  loss: 0.002121, mae: 0.428319, mean_q: 0.592508, mean_eps: 0.682190\n",
      " 353491/1000000: episode: 1730, duration: 3.271s, episode steps: 298, steps per second:  91, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.745 [0.000, 3.000],  loss: 0.001826, mae: 0.432856, mean_q: 0.598478, mean_eps: 0.681992\n",
      " 353720/1000000: episode: 1731, duration: 2.696s, episode steps: 229, steps per second:  85, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.707 [0.000, 3.000],  loss: 0.001930, mae: 0.417705, mean_q: 0.575761, mean_eps: 0.681756\n",
      " 354043/1000000: episode: 1732, duration: 3.450s, episode steps: 323, steps per second:  94, episode reward:  6.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.709 [0.000, 3.000],  loss: 0.002118, mae: 0.423045, mean_q: 0.582331, mean_eps: 0.681508\n",
      " 354526/1000000: episode: 1733, duration: 5.234s, episode steps: 483, steps per second:  92, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.656 [0.000, 3.000],  loss: 0.001808, mae: 0.428893, mean_q: 0.589831, mean_eps: 0.681144\n",
      " 354892/1000000: episode: 1734, duration: 3.958s, episode steps: 366, steps per second:  92, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: 0.002525, mae: 0.426040, mean_q: 0.588076, mean_eps: 0.680763\n",
      " 355129/1000000: episode: 1735, duration: 2.768s, episode steps: 237, steps per second:  86, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.734 [0.000, 3.000],  loss: 0.002300, mae: 0.429785, mean_q: 0.592622, mean_eps: 0.680491\n",
      " 355491/1000000: episode: 1736, duration: 3.853s, episode steps: 362, steps per second:  94, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.831 [0.000, 3.000],  loss: 0.002164, mae: 0.419695, mean_q: 0.579180, mean_eps: 0.680221\n",
      " 355926/1000000: episode: 1737, duration: 4.737s, episode steps: 435, steps per second:  92, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.722 [0.000, 3.000],  loss: 0.002106, mae: 0.428558, mean_q: 0.592916, mean_eps: 0.679863\n",
      " 356255/1000000: episode: 1738, duration: 3.517s, episode steps: 329, steps per second:  94, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: 0.001939, mae: 0.422569, mean_q: 0.583902, mean_eps: 0.679519\n",
      " 356597/1000000: episode: 1739, duration: 3.900s, episode steps: 342, steps per second:  88, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.743 [0.000, 3.000],  loss: 0.002149, mae: 0.434091, mean_q: 0.596424, mean_eps: 0.679217\n",
      " 356845/1000000: episode: 1740, duration: 2.617s, episode steps: 248, steps per second:  95, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.002089, mae: 0.428981, mean_q: 0.589809, mean_eps: 0.678950\n",
      " 357046/1000000: episode: 1741, duration: 2.181s, episode steps: 201, steps per second:  92, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.910 [0.000, 3.000],  loss: 0.002344, mae: 0.437592, mean_q: 0.605308, mean_eps: 0.678749\n",
      " 357263/1000000: episode: 1742, duration: 2.319s, episode steps: 217, steps per second:  94, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.001941, mae: 0.434267, mean_q: 0.598872, mean_eps: 0.678561\n",
      " 357533/1000000: episode: 1743, duration: 2.901s, episode steps: 270, steps per second:  93, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.685 [0.000, 3.000],  loss: 0.002462, mae: 0.413756, mean_q: 0.572176, mean_eps: 0.678342\n",
      " 357700/1000000: episode: 1744, duration: 1.792s, episode steps: 167, steps per second:  93, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.749 [0.000, 3.000],  loss: 0.002232, mae: 0.434803, mean_q: 0.598433, mean_eps: 0.678146\n",
      " 358030/1000000: episode: 1745, duration: 3.788s, episode steps: 330, steps per second:  87, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 0.002557, mae: 0.428809, mean_q: 0.592056, mean_eps: 0.677922\n",
      " 358246/1000000: episode: 1746, duration: 2.315s, episode steps: 216, steps per second:  93, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 0.001897, mae: 0.419538, mean_q: 0.579073, mean_eps: 0.677676\n",
      " 358559/1000000: episode: 1747, duration: 3.340s, episode steps: 313, steps per second:  94, episode reward:  3.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.002028, mae: 0.422408, mean_q: 0.581378, mean_eps: 0.677438\n",
      " 358880/1000000: episode: 1748, duration: 3.460s, episode steps: 321, steps per second:  93, episode reward:  6.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.713 [0.000, 3.000],  loss: 0.002366, mae: 0.425661, mean_q: 0.585605, mean_eps: 0.677154\n",
      " 359102/1000000: episode: 1749, duration: 2.460s, episode steps: 222, steps per second:  90, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.739 [0.000, 3.000],  loss: 0.001730, mae: 0.429830, mean_q: 0.593405, mean_eps: 0.676909\n",
      " 359360/1000000: episode: 1750, duration: 2.903s, episode steps: 258, steps per second:  89, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.841 [0.000, 3.000],  loss: 0.002243, mae: 0.426527, mean_q: 0.585996, mean_eps: 0.676693\n",
      " 359584/1000000: episode: 1751, duration: 2.642s, episode steps: 224, steps per second:  85, episode reward:  1.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.763 [0.000, 3.000],  loss: 0.002213, mae: 0.436277, mean_q: 0.599302, mean_eps: 0.676477\n",
      " 359799/1000000: episode: 1752, duration: 2.370s, episode steps: 215, steps per second:  91, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.805 [0.000, 3.000],  loss: 0.001871, mae: 0.417256, mean_q: 0.576950, mean_eps: 0.676279\n",
      " 360075/1000000: episode: 1753, duration: 3.038s, episode steps: 276, steps per second:  91, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.703 [0.000, 3.000],  loss: 0.004129, mae: 0.433871, mean_q: 0.597433, mean_eps: 0.676058\n",
      " 360368/1000000: episode: 1754, duration: 3.222s, episode steps: 293, steps per second:  91, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.420 [0.000, 3.000],  loss: 0.009247, mae: 0.451261, mean_q: 0.634319, mean_eps: 0.675802\n",
      " 360665/1000000: episode: 1755, duration: 3.228s, episode steps: 297, steps per second:  92, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.475 [0.000, 3.000],  loss: 0.010352, mae: 0.450436, mean_q: 0.641961, mean_eps: 0.675536\n",
      " 360969/1000000: episode: 1756, duration: 3.554s, episode steps: 304, steps per second:  86, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 0.009617, mae: 0.460968, mean_q: 0.656762, mean_eps: 0.675264\n",
      " 361217/1000000: episode: 1757, duration: 2.741s, episode steps: 248, steps per second:  90, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.528 [0.000, 3.000],  loss: 0.007804, mae: 0.458007, mean_q: 0.657664, mean_eps: 0.675015\n",
      " 361418/1000000: episode: 1758, duration: 2.189s, episode steps: 201, steps per second:  92, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 0.005827, mae: 0.453157, mean_q: 0.644081, mean_eps: 0.674814\n",
      " 361662/1000000: episode: 1759, duration: 2.653s, episode steps: 244, steps per second:  92, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.006759, mae: 0.454285, mean_q: 0.636244, mean_eps: 0.674614\n",
      " 361818/1000000: episode: 1760, duration: 1.730s, episode steps: 156, steps per second:  90, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.359 [0.000, 3.000],  loss: 0.006874, mae: 0.439348, mean_q: 0.622613, mean_eps: 0.674434\n",
      " 362090/1000000: episode: 1761, duration: 2.971s, episode steps: 272, steps per second:  92, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.603 [0.000, 3.000],  loss: 0.006854, mae: 0.447524, mean_q: 0.629827, mean_eps: 0.674241\n",
      " 362370/1000000: episode: 1762, duration: 3.238s, episode steps: 280, steps per second:  86, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 0.008216, mae: 0.454681, mean_q: 0.647545, mean_eps: 0.673993\n",
      " 362599/1000000: episode: 1763, duration: 2.457s, episode steps: 229, steps per second:  93, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.437 [0.000, 3.000],  loss: 0.007982, mae: 0.458738, mean_q: 0.650983, mean_eps: 0.673764\n",
      " 362999/1000000: episode: 1764, duration: 4.322s, episode steps: 400, steps per second:  93, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 0.007094, mae: 0.450517, mean_q: 0.646385, mean_eps: 0.673482\n",
      " 363280/1000000: episode: 1765, duration: 3.100s, episode steps: 281, steps per second:  91, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 0.007588, mae: 0.465408, mean_q: 0.658809, mean_eps: 0.673176\n",
      " 363533/1000000: episode: 1766, duration: 2.792s, episode steps: 253, steps per second:  91, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.007764, mae: 0.444679, mean_q: 0.639329, mean_eps: 0.672935\n",
      " 363791/1000000: episode: 1767, duration: 3.041s, episode steps: 258, steps per second:  85, episode reward:  4.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.005890, mae: 0.464036, mean_q: 0.666768, mean_eps: 0.672704\n",
      " 363947/1000000: episode: 1768, duration: 1.731s, episode steps: 156, steps per second:  90, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.468 [0.000, 3.000],  loss: 0.008435, mae: 0.460996, mean_q: 0.660143, mean_eps: 0.672519\n",
      " 364104/1000000: episode: 1769, duration: 1.800s, episode steps: 157, steps per second:  87, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.427 [0.000, 3.000],  loss: 0.006703, mae: 0.454474, mean_q: 0.659436, mean_eps: 0.672378\n",
      " 364387/1000000: episode: 1770, duration: 3.101s, episode steps: 283, steps per second:  91, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.006993, mae: 0.454601, mean_q: 0.655740, mean_eps: 0.672180\n",
      " 364558/1000000: episode: 1771, duration: 1.889s, episode steps: 171, steps per second:  91, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.468 [0.000, 3.000],  loss: 0.006575, mae: 0.458581, mean_q: 0.655256, mean_eps: 0.671975\n",
      " 364815/1000000: episode: 1772, duration: 2.787s, episode steps: 257, steps per second:  92, episode reward:  4.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.006337, mae: 0.442908, mean_q: 0.638315, mean_eps: 0.671783\n",
      " 364971/1000000: episode: 1773, duration: 1.721s, episode steps: 156, steps per second:  91, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.005833, mae: 0.449205, mean_q: 0.646183, mean_eps: 0.671597\n",
      " 365095/1000000: episode: 1774, duration: 1.435s, episode steps: 124, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.435 [0.000, 3.000],  loss: 0.008228, mae: 0.455901, mean_q: 0.655535, mean_eps: 0.671471\n",
      " 365280/1000000: episode: 1775, duration: 2.202s, episode steps: 185, steps per second:  84, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.519 [0.000, 3.000],  loss: 0.007588, mae: 0.452563, mean_q: 0.650816, mean_eps: 0.671333\n",
      " 365510/1000000: episode: 1776, duration: 2.492s, episode steps: 230, steps per second:  92, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: 0.006631, mae: 0.458043, mean_q: 0.657484, mean_eps: 0.671145\n",
      " 365664/1000000: episode: 1777, duration: 1.736s, episode steps: 154, steps per second:  89, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.008423, mae: 0.454040, mean_q: 0.649929, mean_eps: 0.670973\n",
      " 365815/1000000: episode: 1778, duration: 1.661s, episode steps: 151, steps per second:  91, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: 0.004558, mae: 0.444830, mean_q: 0.638662, mean_eps: 0.670836\n",
      " 366148/1000000: episode: 1779, duration: 3.633s, episode steps: 333, steps per second:  92, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 0.006599, mae: 0.442113, mean_q: 0.638181, mean_eps: 0.670618\n",
      " 366320/1000000: episode: 1780, duration: 1.930s, episode steps: 172, steps per second:  89, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.006686, mae: 0.454444, mean_q: 0.651559, mean_eps: 0.670391\n",
      " 366568/1000000: episode: 1781, duration: 2.818s, episode steps: 248, steps per second:  88, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.005950, mae: 0.452801, mean_q: 0.657423, mean_eps: 0.670202\n",
      " 366941/1000000: episode: 1782, duration: 4.220s, episode steps: 373, steps per second:  88, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: 0.007224, mae: 0.453838, mean_q: 0.658475, mean_eps: 0.669921\n",
      " 367125/1000000: episode: 1783, duration: 1.992s, episode steps: 184, steps per second:  92, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.478 [0.000, 3.000],  loss: 0.005937, mae: 0.430054, mean_q: 0.625745, mean_eps: 0.669669\n",
      " 367277/1000000: episode: 1784, duration: 1.674s, episode steps: 152, steps per second:  91, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.474 [0.000, 3.000],  loss: 0.007076, mae: 0.451913, mean_q: 0.648642, mean_eps: 0.669518\n",
      " 367571/1000000: episode: 1785, duration: 3.227s, episode steps: 294, steps per second:  91, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.660 [0.000, 3.000],  loss: 0.005103, mae: 0.447757, mean_q: 0.642227, mean_eps: 0.669318\n",
      " 367827/1000000: episode: 1786, duration: 2.799s, episode steps: 256, steps per second:  91, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.006004, mae: 0.442438, mean_q: 0.637094, mean_eps: 0.669072\n",
      " 368032/1000000: episode: 1787, duration: 2.386s, episode steps: 205, steps per second:  86, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.006546, mae: 0.454774, mean_q: 0.656545, mean_eps: 0.668865\n",
      " 368340/1000000: episode: 1788, duration: 3.508s, episode steps: 308, steps per second:  88, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.468 [0.000, 3.000],  loss: 0.007163, mae: 0.460784, mean_q: 0.663361, mean_eps: 0.668634\n",
      " 368573/1000000: episode: 1789, duration: 2.615s, episode steps: 233, steps per second:  89, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.006728, mae: 0.469105, mean_q: 0.673683, mean_eps: 0.668390\n",
      " 368823/1000000: episode: 1790, duration: 2.775s, episode steps: 250, steps per second:  90, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.652 [0.000, 3.000],  loss: 0.006322, mae: 0.451742, mean_q: 0.653099, mean_eps: 0.668172\n",
      " 369050/1000000: episode: 1791, duration: 2.483s, episode steps: 227, steps per second:  91, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.683 [0.000, 3.000],  loss: 0.007604, mae: 0.446498, mean_q: 0.646726, mean_eps: 0.667958\n",
      " 369225/1000000: episode: 1792, duration: 1.930s, episode steps: 175, steps per second:  91, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.469 [0.000, 3.000],  loss: 0.005146, mae: 0.458250, mean_q: 0.660168, mean_eps: 0.667776\n",
      " 369483/1000000: episode: 1793, duration: 2.972s, episode steps: 258, steps per second:  87, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 0.007112, mae: 0.459013, mean_q: 0.661594, mean_eps: 0.667581\n",
      " 369609/1000000: episode: 1794, duration: 1.474s, episode steps: 126, steps per second:  85, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.627 [0.000, 3.000],  loss: 0.006454, mae: 0.464259, mean_q: 0.667669, mean_eps: 0.667409\n",
      " 369973/1000000: episode: 1795, duration: 4.008s, episode steps: 364, steps per second:  91, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.555 [0.000, 3.000],  loss: 0.005266, mae: 0.445795, mean_q: 0.645298, mean_eps: 0.667187\n",
      " 370331/1000000: episode: 1796, duration: 3.891s, episode steps: 358, steps per second:  92, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.007898, mae: 0.485693, mean_q: 0.686033, mean_eps: 0.666863\n",
      " 370702/1000000: episode: 1797, duration: 4.133s, episode steps: 371, steps per second:  90, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.663 [0.000, 3.000],  loss: 0.007996, mae: 0.488113, mean_q: 0.694508, mean_eps: 0.666536\n",
      " 370943/1000000: episode: 1798, duration: 2.859s, episode steps: 241, steps per second:  84, episode reward:  4.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.614 [0.000, 3.000],  loss: 0.006342, mae: 0.491941, mean_q: 0.697595, mean_eps: 0.666260\n",
      " 371203/1000000: episode: 1799, duration: 2.861s, episode steps: 260, steps per second:  91, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.569 [0.000, 3.000],  loss: 0.005683, mae: 0.489848, mean_q: 0.695659, mean_eps: 0.666035\n",
      " 371468/1000000: episode: 1800, duration: 2.924s, episode steps: 265, steps per second:  91, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: 0.007179, mae: 0.482789, mean_q: 0.692810, mean_eps: 0.665799\n",
      " 371887/1000000: episode: 1801, duration: 4.653s, episode steps: 419, steps per second:  90, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.005933, mae: 0.501612, mean_q: 0.711205, mean_eps: 0.665492\n",
      " 372105/1000000: episode: 1802, duration: 2.399s, episode steps: 218, steps per second:  91, episode reward:  3.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 0.007255, mae: 0.474767, mean_q: 0.678580, mean_eps: 0.665204\n",
      " 372336/1000000: episode: 1803, duration: 2.714s, episode steps: 231, steps per second:  85, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.654 [0.000, 3.000],  loss: 0.006822, mae: 0.483885, mean_q: 0.690168, mean_eps: 0.665002\n",
      " 372621/1000000: episode: 1804, duration: 3.213s, episode steps: 285, steps per second:  89, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.632 [0.000, 3.000],  loss: 0.006172, mae: 0.485240, mean_q: 0.695270, mean_eps: 0.664770\n",
      " 372920/1000000: episode: 1805, duration: 3.350s, episode steps: 299, steps per second:  89, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.575 [0.000, 3.000],  loss: 0.006067, mae: 0.493088, mean_q: 0.700458, mean_eps: 0.664507\n",
      " 373165/1000000: episode: 1806, duration: 2.716s, episode steps: 245, steps per second:  90, episode reward:  4.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.686 [0.000, 3.000],  loss: 0.006016, mae: 0.489013, mean_q: 0.690128, mean_eps: 0.664262\n",
      " 373365/1000000: episode: 1807, duration: 2.218s, episode steps: 200, steps per second:  90, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.745 [0.000, 3.000],  loss: 0.007109, mae: 0.483399, mean_q: 0.684376, mean_eps: 0.664061\n",
      " 373615/1000000: episode: 1808, duration: 2.775s, episode steps: 250, steps per second:  90, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.007677, mae: 0.486946, mean_q: 0.692386, mean_eps: 0.663859\n",
      " 373832/1000000: episode: 1809, duration: 2.599s, episode steps: 217, steps per second:  83, episode reward:  3.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.645 [0.000, 3.000],  loss: 0.005957, mae: 0.489008, mean_q: 0.692099, mean_eps: 0.663650\n",
      " 374025/1000000: episode: 1810, duration: 2.151s, episode steps: 193, steps per second:  90, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.684 [0.000, 3.000],  loss: 0.004988, mae: 0.483115, mean_q: 0.682646, mean_eps: 0.663465\n",
      " 374297/1000000: episode: 1811, duration: 2.981s, episode steps: 272, steps per second:  91, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.654 [0.000, 3.000],  loss: 0.006616, mae: 0.490608, mean_q: 0.695728, mean_eps: 0.663254\n",
      " 374563/1000000: episode: 1812, duration: 2.868s, episode steps: 266, steps per second:  93, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 0.005647, mae: 0.485736, mean_q: 0.691727, mean_eps: 0.663013\n",
      " 374854/1000000: episode: 1813, duration: 3.195s, episode steps: 291, steps per second:  91, episode reward:  5.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.006478, mae: 0.503208, mean_q: 0.720902, mean_eps: 0.662763\n",
      " 375163/1000000: episode: 1814, duration: 3.491s, episode steps: 309, steps per second:  89, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.006439, mae: 0.478251, mean_q: 0.681045, mean_eps: 0.662493\n",
      " 375566/1000000: episode: 1815, duration: 4.566s, episode steps: 403, steps per second:  88, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.005953, mae: 0.483125, mean_q: 0.684542, mean_eps: 0.662172\n",
      " 375888/1000000: episode: 1816, duration: 3.519s, episode steps: 322, steps per second:  92, episode reward:  6.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.624 [0.000, 3.000],  loss: 0.006344, mae: 0.487125, mean_q: 0.690283, mean_eps: 0.661847\n",
      " 376130/1000000: episode: 1817, duration: 2.715s, episode steps: 242, steps per second:  89, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.006533, mae: 0.497203, mean_q: 0.710593, mean_eps: 0.661593\n",
      " 376358/1000000: episode: 1818, duration: 2.453s, episode steps: 228, steps per second:  93, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.004883, mae: 0.488206, mean_q: 0.697320, mean_eps: 0.661380\n",
      " 376675/1000000: episode: 1819, duration: 3.632s, episode steps: 317, steps per second:  87, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 0.006623, mae: 0.491386, mean_q: 0.699809, mean_eps: 0.661136\n",
      " 376943/1000000: episode: 1820, duration: 2.895s, episode steps: 268, steps per second:  93, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: 0.005462, mae: 0.484504, mean_q: 0.689618, mean_eps: 0.660873\n",
      " 377115/1000000: episode: 1821, duration: 1.882s, episode steps: 172, steps per second:  91, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 0.006491, mae: 0.487038, mean_q: 0.692540, mean_eps: 0.660675\n",
      " 377428/1000000: episode: 1822, duration: 3.424s, episode steps: 313, steps per second:  91, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 0.006354, mae: 0.480235, mean_q: 0.685205, mean_eps: 0.660457\n",
      " 377812/1000000: episode: 1823, duration: 4.214s, episode steps: 384, steps per second:  91, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.602 [0.000, 3.000],  loss: 0.005566, mae: 0.490517, mean_q: 0.696256, mean_eps: 0.660144\n",
      " 378193/1000000: episode: 1824, duration: 4.419s, episode steps: 381, steps per second:  86, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.711 [0.000, 3.000],  loss: 0.005432, mae: 0.476771, mean_q: 0.676911, mean_eps: 0.659798\n",
      " 378442/1000000: episode: 1825, duration: 2.773s, episode steps: 249, steps per second:  90, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.005139, mae: 0.489776, mean_q: 0.692641, mean_eps: 0.659514\n",
      " 378659/1000000: episode: 1826, duration: 2.351s, episode steps: 217, steps per second:  92, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.005373, mae: 0.483678, mean_q: 0.689370, mean_eps: 0.659305\n",
      " 378987/1000000: episode: 1827, duration: 3.626s, episode steps: 328, steps per second:  90, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 0.006187, mae: 0.487130, mean_q: 0.692116, mean_eps: 0.659060\n",
      " 379421/1000000: episode: 1828, duration: 4.841s, episode steps: 434, steps per second:  90, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.657 [0.000, 3.000],  loss: 0.006277, mae: 0.486425, mean_q: 0.692049, mean_eps: 0.658716\n",
      " 379667/1000000: episode: 1829, duration: 2.859s, episode steps: 246, steps per second:  86, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.512 [0.000, 3.000],  loss: 0.005479, mae: 0.502214, mean_q: 0.712306, mean_eps: 0.658410\n",
      " 379957/1000000: episode: 1830, duration: 3.214s, episode steps: 290, steps per second:  90, episode reward:  5.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.497 [0.000, 3.000],  loss: 0.006505, mae: 0.490993, mean_q: 0.695543, mean_eps: 0.658169\n",
      " 380159/1000000: episode: 1831, duration: 2.189s, episode steps: 202, steps per second:  92, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.569 [0.000, 3.000],  loss: 0.007831, mae: 0.517832, mean_q: 0.723934, mean_eps: 0.657948\n",
      " 380407/1000000: episode: 1832, duration: 2.710s, episode steps: 248, steps per second:  92, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 0.007164, mae: 0.527993, mean_q: 0.739324, mean_eps: 0.657746\n",
      " 380624/1000000: episode: 1833, duration: 2.424s, episode steps: 217, steps per second:  90, episode reward:  3.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.659 [0.000, 3.000],  loss: 0.006502, mae: 0.519101, mean_q: 0.728513, mean_eps: 0.657537\n",
      " 380993/1000000: episode: 1834, duration: 4.326s, episode steps: 369, steps per second:  85, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.648 [0.000, 3.000],  loss: 0.006203, mae: 0.527589, mean_q: 0.735830, mean_eps: 0.657273\n",
      " 381296/1000000: episode: 1835, duration: 3.395s, episode steps: 303, steps per second:  89, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.650 [0.000, 3.000],  loss: 0.006541, mae: 0.520199, mean_q: 0.726260, mean_eps: 0.656970\n",
      " 381634/1000000: episode: 1836, duration: 3.729s, episode steps: 338, steps per second:  91, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: 0.007292, mae: 0.529420, mean_q: 0.739679, mean_eps: 0.656682\n",
      " 381909/1000000: episode: 1837, duration: 3.096s, episode steps: 275, steps per second:  89, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 0.005645, mae: 0.527637, mean_q: 0.737176, mean_eps: 0.656405\n",
      " 382219/1000000: episode: 1838, duration: 3.414s, episode steps: 310, steps per second:  91, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.687 [0.000, 3.000],  loss: 0.005420, mae: 0.530668, mean_q: 0.737702, mean_eps: 0.656142\n",
      " 382544/1000000: episode: 1839, duration: 3.826s, episode steps: 325, steps per second:  85, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 0.007166, mae: 0.528607, mean_q: 0.734445, mean_eps: 0.655858\n",
      " 382907/1000000: episode: 1840, duration: 3.965s, episode steps: 363, steps per second:  92, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.645 [0.000, 3.000],  loss: 0.006094, mae: 0.528632, mean_q: 0.739089, mean_eps: 0.655548\n",
      " 383239/1000000: episode: 1841, duration: 3.659s, episode steps: 332, steps per second:  91, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 0.006402, mae: 0.531500, mean_q: 0.738651, mean_eps: 0.655235\n",
      " 383392/1000000: episode: 1842, duration: 1.706s, episode steps: 153, steps per second:  90, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.005957, mae: 0.528868, mean_q: 0.739138, mean_eps: 0.655017\n",
      " 383807/1000000: episode: 1843, duration: 4.669s, episode steps: 415, steps per second:  89, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.689 [0.000, 3.000],  loss: 0.006055, mae: 0.528989, mean_q: 0.737868, mean_eps: 0.654762\n",
      " 384114/1000000: episode: 1844, duration: 3.397s, episode steps: 307, steps per second:  90, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.596 [0.000, 3.000],  loss: 0.005842, mae: 0.530362, mean_q: 0.742213, mean_eps: 0.654436\n",
      " 384517/1000000: episode: 1845, duration: 4.432s, episode steps: 403, steps per second:  91, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.670 [0.000, 3.000],  loss: 0.006501, mae: 0.521146, mean_q: 0.725254, mean_eps: 0.654116\n",
      " 384929/1000000: episode: 1846, duration: 4.617s, episode steps: 412, steps per second:  89, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.590 [0.000, 3.000],  loss: 0.006649, mae: 0.522760, mean_q: 0.727280, mean_eps: 0.653748\n",
      " 385228/1000000: episode: 1847, duration: 3.475s, episode steps: 299, steps per second:  86, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.599 [0.000, 3.000],  loss: 0.006165, mae: 0.523432, mean_q: 0.732597, mean_eps: 0.653430\n",
      " 385509/1000000: episode: 1848, duration: 3.180s, episode steps: 281, steps per second:  88, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.669 [0.000, 3.000],  loss: 0.005744, mae: 0.529005, mean_q: 0.736072, mean_eps: 0.653169\n",
      " 385788/1000000: episode: 1849, duration: 3.100s, episode steps: 279, steps per second:  90, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.638 [0.000, 3.000],  loss: 0.006104, mae: 0.532190, mean_q: 0.741861, mean_eps: 0.652917\n",
      " 386265/1000000: episode: 1850, duration: 5.434s, episode steps: 477, steps per second:  88, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.698 [0.000, 3.000],  loss: 0.005908, mae: 0.527747, mean_q: 0.735120, mean_eps: 0.652577\n",
      " 386522/1000000: episode: 1851, duration: 2.888s, episode steps: 257, steps per second:  89, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.638 [0.000, 3.000],  loss: 0.006340, mae: 0.533443, mean_q: 0.743351, mean_eps: 0.652245\n",
      " 386802/1000000: episode: 1852, duration: 3.362s, episode steps: 280, steps per second:  83, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.671 [0.000, 3.000],  loss: 0.006925, mae: 0.540339, mean_q: 0.752143, mean_eps: 0.652004\n",
      " 386958/1000000: episode: 1853, duration: 1.767s, episode steps: 156, steps per second:  88, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: 0.004778, mae: 0.519939, mean_q: 0.732690, mean_eps: 0.651808\n",
      " 387194/1000000: episode: 1854, duration: 2.627s, episode steps: 236, steps per second:  90, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.500 [0.000, 3.000],  loss: 0.006587, mae: 0.526381, mean_q: 0.734814, mean_eps: 0.651632\n",
      " 387547/1000000: episode: 1855, duration: 3.924s, episode steps: 353, steps per second:  90, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.606 [0.000, 3.000],  loss: 0.005525, mae: 0.531470, mean_q: 0.741849, mean_eps: 0.651367\n",
      " 387747/1000000: episode: 1856, duration: 2.193s, episode steps: 200, steps per second:  91, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.005532, mae: 0.530487, mean_q: 0.741437, mean_eps: 0.651119\n",
      " 388056/1000000: episode: 1857, duration: 3.782s, episode steps: 309, steps per second:  82, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.693 [0.000, 3.000],  loss: 0.006177, mae: 0.529639, mean_q: 0.744040, mean_eps: 0.650890\n",
      " 388301/1000000: episode: 1858, duration: 2.877s, episode steps: 245, steps per second:  85, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.005129, mae: 0.515871, mean_q: 0.721863, mean_eps: 0.650640\n",
      " 388682/1000000: episode: 1859, duration: 4.359s, episode steps: 381, steps per second:  87, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.005349, mae: 0.528953, mean_q: 0.737339, mean_eps: 0.650357\n",
      " 388927/1000000: episode: 1860, duration: 2.747s, episode steps: 245, steps per second:  89, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.657 [0.000, 3.000],  loss: 0.005634, mae: 0.520176, mean_q: 0.725543, mean_eps: 0.650076\n",
      " 389244/1000000: episode: 1861, duration: 3.644s, episode steps: 317, steps per second:  87, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.735 [0.000, 3.000],  loss: 0.005990, mae: 0.521919, mean_q: 0.729735, mean_eps: 0.649824\n",
      " 389463/1000000: episode: 1862, duration: 2.631s, episode steps: 219, steps per second:  83, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.749 [0.000, 3.000],  loss: 0.005438, mae: 0.526123, mean_q: 0.737377, mean_eps: 0.649583\n",
      " 389754/1000000: episode: 1863, duration: 3.267s, episode steps: 291, steps per second:  89, episode reward:  5.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.718 [0.000, 3.000],  loss: 0.006424, mae: 0.529356, mean_q: 0.736047, mean_eps: 0.649353\n",
      " 390254/1000000: episode: 1864, duration: 5.598s, episode steps: 500, steps per second:  89, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: 0.007424, mae: 0.555773, mean_q: 0.769544, mean_eps: 0.648996\n",
      " 390629/1000000: episode: 1865, duration: 4.238s, episode steps: 375, steps per second:  88, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.005770, mae: 0.581344, mean_q: 0.803700, mean_eps: 0.648602\n",
      " 390881/1000000: episode: 1866, duration: 2.976s, episode steps: 252, steps per second:  85, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.726 [0.000, 3.000],  loss: 0.005858, mae: 0.576563, mean_q: 0.794871, mean_eps: 0.648320\n",
      " 391110/1000000: episode: 1867, duration: 2.613s, episode steps: 229, steps per second:  88, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.410 [0.000, 3.000],  loss: 0.006519, mae: 0.579353, mean_q: 0.800107, mean_eps: 0.648104\n",
      " 391409/1000000: episode: 1868, duration: 3.378s, episode steps: 299, steps per second:  89, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.005932, mae: 0.578317, mean_q: 0.797752, mean_eps: 0.647866\n",
      " 391703/1000000: episode: 1869, duration: 3.286s, episode steps: 294, steps per second:  89, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: 0.006080, mae: 0.576346, mean_q: 0.796269, mean_eps: 0.647600\n",
      " 392256/1000000: episode: 1870, duration: 6.348s, episode steps: 553, steps per second:  87, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.675 [0.000, 3.000],  loss: 0.005494, mae: 0.584453, mean_q: 0.808650, mean_eps: 0.647220\n",
      " 392616/1000000: episode: 1871, duration: 4.225s, episode steps: 360, steps per second:  85, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.617 [0.000, 3.000],  loss: 0.006254, mae: 0.579258, mean_q: 0.803438, mean_eps: 0.646809\n",
      " 392894/1000000: episode: 1872, duration: 3.124s, episode steps: 278, steps per second:  89, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.601 [0.000, 3.000],  loss: 0.006207, mae: 0.578412, mean_q: 0.800184, mean_eps: 0.646521\n",
      " 393267/1000000: episode: 1873, duration: 4.191s, episode steps: 373, steps per second:  89, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.681 [0.000, 3.000],  loss: 0.005116, mae: 0.583869, mean_q: 0.808676, mean_eps: 0.646228\n",
      " 393593/1000000: episode: 1874, duration: 3.759s, episode steps: 326, steps per second:  87, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.702 [0.000, 3.000],  loss: 0.005684, mae: 0.578278, mean_q: 0.799029, mean_eps: 0.645913\n",
      " 393762/1000000: episode: 1875, duration: 1.921s, episode steps: 169, steps per second:  88, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: 0.006012, mae: 0.586273, mean_q: 0.808112, mean_eps: 0.645690\n",
      " 393997/1000000: episode: 1876, duration: 2.655s, episode steps: 235, steps per second:  89, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.005652, mae: 0.585263, mean_q: 0.811932, mean_eps: 0.645508\n",
      " 394250/1000000: episode: 1877, duration: 2.895s, episode steps: 253, steps per second:  87, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.006021, mae: 0.572200, mean_q: 0.793621, mean_eps: 0.645288\n",
      " 394583/1000000: episode: 1878, duration: 3.688s, episode steps: 333, steps per second:  90, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 0.006526, mae: 0.569300, mean_q: 0.786422, mean_eps: 0.645026\n",
      " 394817/1000000: episode: 1879, duration: 2.690s, episode steps: 234, steps per second:  87, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: 0.005756, mae: 0.574863, mean_q: 0.794737, mean_eps: 0.644770\n",
      " 395068/1000000: episode: 1880, duration: 2.995s, episode steps: 251, steps per second:  84, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.005555, mae: 0.563470, mean_q: 0.781081, mean_eps: 0.644552\n",
      " 395316/1000000: episode: 1881, duration: 2.990s, episode steps: 248, steps per second:  83, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.690 [0.000, 3.000],  loss: 0.004656, mae: 0.576739, mean_q: 0.798672, mean_eps: 0.644329\n",
      " 395718/1000000: episode: 1882, duration: 5.061s, episode steps: 402, steps per second:  79, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.731 [0.000, 3.000],  loss: 0.005750, mae: 0.583179, mean_q: 0.807265, mean_eps: 0.644036\n",
      " 395982/1000000: episode: 1883, duration: 2.955s, episode steps: 264, steps per second:  89, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.006123, mae: 0.573905, mean_q: 0.796284, mean_eps: 0.643735\n",
      " 396447/1000000: episode: 1884, duration: 5.399s, episode steps: 465, steps per second:  86, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.632 [0.000, 3.000],  loss: 0.005695, mae: 0.572351, mean_q: 0.792274, mean_eps: 0.643407\n",
      " 396685/1000000: episode: 1885, duration: 2.740s, episode steps: 238, steps per second:  87, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 0.005473, mae: 0.572007, mean_q: 0.790957, mean_eps: 0.643091\n",
      " 396891/1000000: episode: 1886, duration: 2.289s, episode steps: 206, steps per second:  90, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.490 [0.000, 3.000],  loss: 0.005218, mae: 0.577321, mean_q: 0.797304, mean_eps: 0.642891\n",
      " 397140/1000000: episode: 1887, duration: 2.855s, episode steps: 249, steps per second:  87, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.675 [0.000, 3.000],  loss: 0.005394, mae: 0.569966, mean_q: 0.789293, mean_eps: 0.642687\n",
      " 397373/1000000: episode: 1888, duration: 2.629s, episode steps: 233, steps per second:  89, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.691 [0.000, 3.000],  loss: 0.005983, mae: 0.572826, mean_q: 0.792731, mean_eps: 0.642470\n",
      " 397653/1000000: episode: 1889, duration: 3.220s, episode steps: 280, steps per second:  87, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: 0.005701, mae: 0.578280, mean_q: 0.798441, mean_eps: 0.642237\n",
      " 397973/1000000: episode: 1890, duration: 3.689s, episode steps: 320, steps per second:  87, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.005605, mae: 0.580115, mean_q: 0.800493, mean_eps: 0.641967\n",
      " 398222/1000000: episode: 1891, duration: 2.797s, episode steps: 249, steps per second:  89, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.590 [0.000, 3.000],  loss: 0.005271, mae: 0.580789, mean_q: 0.803177, mean_eps: 0.641712\n",
      " 398474/1000000: episode: 1892, duration: 2.834s, episode steps: 252, steps per second:  89, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.722 [0.000, 3.000],  loss: 0.005564, mae: 0.584718, mean_q: 0.808461, mean_eps: 0.641487\n",
      " 398675/1000000: episode: 1893, duration: 2.308s, episode steps: 201, steps per second:  87, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.005513, mae: 0.588975, mean_q: 0.813643, mean_eps: 0.641283\n",
      " 398986/1000000: episode: 1894, duration: 3.516s, episode steps: 311, steps per second:  88, episode reward:  6.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.005535, mae: 0.580589, mean_q: 0.800265, mean_eps: 0.641053\n",
      " 399240/1000000: episode: 1895, duration: 3.036s, episode steps: 254, steps per second:  84, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.744 [0.000, 3.000],  loss: 0.004953, mae: 0.578440, mean_q: 0.800920, mean_eps: 0.640799\n",
      " 399422/1000000: episode: 1896, duration: 2.064s, episode steps: 182, steps per second:  88, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.742 [0.000, 3.000],  loss: 0.005168, mae: 0.585787, mean_q: 0.811216, mean_eps: 0.640603\n",
      " 399717/1000000: episode: 1897, duration: 3.401s, episode steps: 295, steps per second:  87, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 0.004370, mae: 0.572452, mean_q: 0.793283, mean_eps: 0.640387\n",
      " 399961/1000000: episode: 1898, duration: 2.763s, episode steps: 244, steps per second:  88, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.701 [0.000, 3.000],  loss: 0.004481, mae: 0.579813, mean_q: 0.804909, mean_eps: 0.640144\n",
      " 400087/1000000: episode: 1899, duration: 1.420s, episode steps: 126, steps per second:  89, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.667 [0.000, 3.000],  loss: 0.007144, mae: 0.596679, mean_q: 0.820195, mean_eps: 0.639978\n",
      " 400623/1000000: episode: 1900, duration: 6.267s, episode steps: 536, steps per second:  86, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.007103, mae: 0.621535, mean_q: 0.851985, mean_eps: 0.639681\n",
      " 400870/1000000: episode: 1901, duration: 2.834s, episode steps: 247, steps per second:  87, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.644 [0.000, 3.000],  loss: 0.006763, mae: 0.607038, mean_q: 0.831645, mean_eps: 0.639329\n",
      " 401207/1000000: episode: 1902, duration: 3.738s, episode steps: 337, steps per second:  90, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.006569, mae: 0.613446, mean_q: 0.838970, mean_eps: 0.639066\n",
      " 401597/1000000: episode: 1903, duration: 4.391s, episode steps: 390, steps per second:  89, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.006794, mae: 0.618953, mean_q: 0.848207, mean_eps: 0.638738\n",
      " 401924/1000000: episode: 1904, duration: 3.674s, episode steps: 327, steps per second:  89, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.679 [0.000, 3.000],  loss: 0.006715, mae: 0.619438, mean_q: 0.848048, mean_eps: 0.638416\n",
      " 402381/1000000: episode: 1905, duration: 5.202s, episode steps: 457, steps per second:  88, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.724 [0.000, 3.000],  loss: 0.006544, mae: 0.623330, mean_q: 0.852353, mean_eps: 0.638063\n",
      " 402665/1000000: episode: 1906, duration: 3.140s, episode steps: 284, steps per second:  90, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.806 [0.000, 3.000],  loss: 0.006714, mae: 0.609269, mean_q: 0.831940, mean_eps: 0.637728\n",
      " 402936/1000000: episode: 1907, duration: 3.017s, episode steps: 271, steps per second:  90, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 0.005572, mae: 0.612960, mean_q: 0.836959, mean_eps: 0.637480\n",
      " 403295/1000000: episode: 1908, duration: 3.993s, episode steps: 359, steps per second:  90, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.777 [0.000, 3.000],  loss: 0.006403, mae: 0.616010, mean_q: 0.842729, mean_eps: 0.637197\n",
      " 403704/1000000: episode: 1909, duration: 4.658s, episode steps: 409, steps per second:  88, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.770 [0.000, 3.000],  loss: 0.004989, mae: 0.613186, mean_q: 0.841788, mean_eps: 0.636852\n",
      " 404028/1000000: episode: 1910, duration: 3.591s, episode steps: 324, steps per second:  90, episode reward:  6.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.006249, mae: 0.612890, mean_q: 0.840063, mean_eps: 0.636522\n",
      " 404346/1000000: episode: 1911, duration: 3.594s, episode steps: 318, steps per second:  88, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.739 [0.000, 3.000],  loss: 0.006066, mae: 0.603420, mean_q: 0.827401, mean_eps: 0.636233\n",
      " 404700/1000000: episode: 1912, duration: 3.978s, episode steps: 354, steps per second:  89, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 0.006520, mae: 0.614398, mean_q: 0.839619, mean_eps: 0.635930\n",
      " 405103/1000000: episode: 1913, duration: 4.655s, episode steps: 403, steps per second:  87, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.658 [0.000, 3.000],  loss: 0.005996, mae: 0.619040, mean_q: 0.846519, mean_eps: 0.635590\n",
      " 405478/1000000: episode: 1914, duration: 4.202s, episode steps: 375, steps per second:  89, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.005019, mae: 0.610363, mean_q: 0.835998, mean_eps: 0.635239\n",
      " 406036/1000000: episode: 1915, duration: 6.355s, episode steps: 558, steps per second:  88, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.005595, mae: 0.612063, mean_q: 0.836519, mean_eps: 0.634820\n",
      " 406211/1000000: episode: 1916, duration: 2.054s, episode steps: 175, steps per second:  85, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.446 [0.000, 3.000],  loss: 0.004655, mae: 0.622770, mean_q: 0.853925, mean_eps: 0.634490\n",
      " 406569/1000000: episode: 1917, duration: 4.139s, episode steps: 358, steps per second:  86, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.654 [0.000, 3.000],  loss: 0.005660, mae: 0.628323, mean_q: 0.859843, mean_eps: 0.634249\n",
      " 406952/1000000: episode: 1918, duration: 4.246s, episode steps: 383, steps per second:  90, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.775 [0.000, 3.000],  loss: 0.005482, mae: 0.607541, mean_q: 0.831752, mean_eps: 0.633916\n",
      " 407253/1000000: episode: 1919, duration: 3.339s, episode steps: 301, steps per second:  90, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.691 [0.000, 3.000],  loss: 0.005298, mae: 0.606147, mean_q: 0.830577, mean_eps: 0.633608\n",
      " 407686/1000000: episode: 1920, duration: 4.926s, episode steps: 433, steps per second:  88, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.799 [0.000, 3.000],  loss: 0.005508, mae: 0.605308, mean_q: 0.829670, mean_eps: 0.633277\n",
      " 407839/1000000: episode: 1921, duration: 1.747s, episode steps: 153, steps per second:  88, episode reward:  1.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.005169, mae: 0.623260, mean_q: 0.852669, mean_eps: 0.633014\n",
      " 408295/1000000: episode: 1922, duration: 5.051s, episode steps: 456, steps per second:  90, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.689 [0.000, 3.000],  loss: 0.005281, mae: 0.602640, mean_q: 0.824269, mean_eps: 0.632741\n",
      " 408820/1000000: episode: 1923, duration: 5.860s, episode steps: 525, steps per second:  90, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.754 [0.000, 3.000],  loss: 0.006197, mae: 0.615832, mean_q: 0.841637, mean_eps: 0.632300\n",
      " 408997/1000000: episode: 1924, duration: 2.074s, episode steps: 177, steps per second:  85, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.695 [0.000, 3.000],  loss: 0.005966, mae: 0.613274, mean_q: 0.840114, mean_eps: 0.631983\n",
      " 409133/1000000: episode: 1925, duration: 1.678s, episode steps: 136, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.625 [0.000, 3.000],  loss: 0.006340, mae: 0.609173, mean_q: 0.835261, mean_eps: 0.631841\n",
      " 409615/1000000: episode: 1926, duration: 5.338s, episode steps: 482, steps per second:  90, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.005363, mae: 0.619170, mean_q: 0.844608, mean_eps: 0.631563\n",
      " 409874/1000000: episode: 1927, duration: 2.869s, episode steps: 259, steps per second:  90, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.552 [0.000, 3.000],  loss: 0.005914, mae: 0.618493, mean_q: 0.846506, mean_eps: 0.631230\n",
      " 410090/1000000: episode: 1928, duration: 2.403s, episode steps: 216, steps per second:  90, episode reward:  3.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.769 [0.000, 3.000],  loss: 0.008182, mae: 0.621677, mean_q: 0.846756, mean_eps: 0.631016\n",
      " 410417/1000000: episode: 1929, duration: 3.837s, episode steps: 327, steps per second:  85, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.716 [0.000, 3.000],  loss: 0.005303, mae: 0.655112, mean_q: 0.892886, mean_eps: 0.630771\n",
      " 410628/1000000: episode: 1930, duration: 2.517s, episode steps: 211, steps per second:  84, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.706 [0.000, 3.000],  loss: 0.005952, mae: 0.645267, mean_q: 0.877672, mean_eps: 0.630530\n",
      " 411097/1000000: episode: 1931, duration: 5.333s, episode steps: 469, steps per second:  88, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.755 [0.000, 3.000],  loss: 0.004648, mae: 0.650937, mean_q: 0.888937, mean_eps: 0.630224\n",
      " 411402/1000000: episode: 1932, duration: 3.406s, episode steps: 305, steps per second:  90, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.859 [0.000, 3.000],  loss: 0.005313, mae: 0.647897, mean_q: 0.884697, mean_eps: 0.629875\n",
      " 411726/1000000: episode: 1933, duration: 3.689s, episode steps: 324, steps per second:  88, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.784 [0.000, 3.000],  loss: 0.004818, mae: 0.649233, mean_q: 0.886760, mean_eps: 0.629592\n",
      " 411927/1000000: episode: 1934, duration: 2.501s, episode steps: 201, steps per second:  80, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.701 [0.000, 3.000],  loss: 0.005649, mae: 0.640827, mean_q: 0.872769, mean_eps: 0.629357\n",
      " 412359/1000000: episode: 1935, duration: 4.875s, episode steps: 432, steps per second:  89, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.620 [0.000, 3.000],  loss: 0.005832, mae: 0.648499, mean_q: 0.883649, mean_eps: 0.629072\n",
      " 412623/1000000: episode: 1936, duration: 3.030s, episode steps: 264, steps per second:  87, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.716 [0.000, 3.000],  loss: 0.005015, mae: 0.646146, mean_q: 0.881840, mean_eps: 0.628759\n",
      " 412963/1000000: episode: 1937, duration: 3.920s, episode steps: 340, steps per second:  87, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.653 [0.000, 3.000],  loss: 0.005251, mae: 0.654928, mean_q: 0.891773, mean_eps: 0.628487\n",
      " 413297/1000000: episode: 1938, duration: 4.019s, episode steps: 334, steps per second:  83, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.572 [0.000, 3.000],  loss: 0.005232, mae: 0.643790, mean_q: 0.878470, mean_eps: 0.628183\n",
      " 413675/1000000: episode: 1939, duration: 4.219s, episode steps: 378, steps per second:  90, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.651 [0.000, 3.000],  loss: 0.004762, mae: 0.646291, mean_q: 0.879900, mean_eps: 0.627863\n",
      " 414055/1000000: episode: 1940, duration: 4.214s, episode steps: 380, steps per second:  90, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.779 [0.000, 3.000],  loss: 0.005206, mae: 0.650284, mean_q: 0.887468, mean_eps: 0.627522\n",
      " 414410/1000000: episode: 1941, duration: 4.002s, episode steps: 355, steps per second:  89, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.594 [0.000, 3.000],  loss: 0.005189, mae: 0.651022, mean_q: 0.888451, mean_eps: 0.627191\n",
      " 414633/1000000: episode: 1942, duration: 2.596s, episode steps: 223, steps per second:  86, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.695 [0.000, 3.000],  loss: 0.004613, mae: 0.649436, mean_q: 0.887289, mean_eps: 0.626930\n",
      " 414977/1000000: episode: 1943, duration: 3.995s, episode steps: 344, steps per second:  86, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 0.005027, mae: 0.648725, mean_q: 0.883270, mean_eps: 0.626675\n",
      " 415349/1000000: episode: 1944, duration: 4.165s, episode steps: 372, steps per second:  89, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.004802, mae: 0.645361, mean_q: 0.880771, mean_eps: 0.626352\n",
      " 415680/1000000: episode: 1945, duration: 3.751s, episode steps: 331, steps per second:  88, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.813 [0.000, 3.000],  loss: 0.005174, mae: 0.652654, mean_q: 0.890484, mean_eps: 0.626037\n",
      " 416137/1000000: episode: 1946, duration: 5.362s, episode steps: 457, steps per second:  85, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.683 [0.000, 3.000],  loss: 0.004442, mae: 0.642195, mean_q: 0.877849, mean_eps: 0.625683\n",
      " 416565/1000000: episode: 1947, duration: 4.817s, episode steps: 428, steps per second:  89, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.612 [0.000, 3.000],  loss: 0.005025, mae: 0.652119, mean_q: 0.888852, mean_eps: 0.625283\n",
      " 416808/1000000: episode: 1948, duration: 2.772s, episode steps: 243, steps per second:  88, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.770 [0.000, 3.000],  loss: 0.005225, mae: 0.638492, mean_q: 0.872703, mean_eps: 0.624983\n",
      " 417075/1000000: episode: 1949, duration: 3.087s, episode steps: 267, steps per second:  86, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.674 [0.000, 3.000],  loss: 0.004628, mae: 0.652860, mean_q: 0.889635, mean_eps: 0.624754\n",
      " 417415/1000000: episode: 1950, duration: 3.901s, episode steps: 340, steps per second:  87, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.671 [0.000, 3.000],  loss: 0.004603, mae: 0.644802, mean_q: 0.878602, mean_eps: 0.624480\n",
      " 417778/1000000: episode: 1951, duration: 4.242s, episode steps: 363, steps per second:  86, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.702 [0.000, 3.000],  loss: 0.005332, mae: 0.653775, mean_q: 0.889956, mean_eps: 0.624164\n",
      " 418101/1000000: episode: 1952, duration: 3.628s, episode steps: 323, steps per second:  89, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.004509, mae: 0.645030, mean_q: 0.881430, mean_eps: 0.623854\n",
      " 418504/1000000: episode: 1953, duration: 4.528s, episode steps: 403, steps per second:  89, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.005197, mae: 0.654624, mean_q: 0.890786, mean_eps: 0.623528\n",
      " 418936/1000000: episode: 1954, duration: 5.053s, episode steps: 432, steps per second:  85, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: 0.005339, mae: 0.641933, mean_q: 0.873011, mean_eps: 0.623154\n",
      " 419064/1000000: episode: 1955, duration: 1.471s, episode steps: 128, steps per second:  87, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.773 [0.000, 3.000],  loss: 0.004514, mae: 0.648049, mean_q: 0.883957, mean_eps: 0.622902\n",
      " 419341/1000000: episode: 1956, duration: 3.121s, episode steps: 277, steps per second:  89, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.005256, mae: 0.643109, mean_q: 0.877364, mean_eps: 0.622718\n",
      " 419669/1000000: episode: 1957, duration: 3.767s, episode steps: 328, steps per second:  87, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.005902, mae: 0.647667, mean_q: 0.881425, mean_eps: 0.622445\n",
      " 420088/1000000: episode: 1958, duration: 4.748s, episode steps: 419, steps per second:  88, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.728 [0.000, 3.000],  loss: 0.004981, mae: 0.655361, mean_q: 0.891366, mean_eps: 0.622110\n",
      " 420356/1000000: episode: 1959, duration: 3.295s, episode steps: 268, steps per second:  81, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.690 [0.000, 3.000],  loss: 0.004228, mae: 0.686660, mean_q: 0.933094, mean_eps: 0.621802\n",
      " 420778/1000000: episode: 1960, duration: 4.808s, episode steps: 422, steps per second:  88, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: 0.004588, mae: 0.688798, mean_q: 0.935349, mean_eps: 0.621491\n",
      " 421033/1000000: episode: 1961, duration: 2.860s, episode steps: 255, steps per second:  89, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.004744, mae: 0.681085, mean_q: 0.928028, mean_eps: 0.621185\n",
      " 421346/1000000: episode: 1962, duration: 3.423s, episode steps: 313, steps per second:  91, episode reward:  6.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.706 [0.000, 3.000],  loss: 0.004291, mae: 0.677010, mean_q: 0.919387, mean_eps: 0.620929\n",
      " 421608/1000000: episode: 1963, duration: 3.012s, episode steps: 262, steps per second:  87, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 0.004031, mae: 0.682094, mean_q: 0.928886, mean_eps: 0.620672\n",
      " 421919/1000000: episode: 1964, duration: 3.632s, episode steps: 311, steps per second:  86, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: 0.003661, mae: 0.682939, mean_q: 0.928227, mean_eps: 0.620414\n",
      " 422381/1000000: episode: 1965, duration: 5.224s, episode steps: 462, steps per second:  88, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.708 [0.000, 3.000],  loss: 0.004626, mae: 0.683157, mean_q: 0.929017, mean_eps: 0.620065\n",
      " 422548/1000000: episode: 1966, duration: 1.933s, episode steps: 167, steps per second:  86, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.641 [0.000, 3.000],  loss: 0.003357, mae: 0.686232, mean_q: 0.936152, mean_eps: 0.619782\n",
      " 422799/1000000: episode: 1967, duration: 2.871s, episode steps: 251, steps per second:  87, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.657 [0.000, 3.000],  loss: 0.004508, mae: 0.684421, mean_q: 0.930065, mean_eps: 0.619595\n",
      " 422962/1000000: episode: 1968, duration: 1.893s, episode steps: 163, steps per second:  86, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.699 [0.000, 3.000],  loss: 0.004170, mae: 0.676270, mean_q: 0.920746, mean_eps: 0.619408\n",
      " 423236/1000000: episode: 1969, duration: 3.279s, episode steps: 274, steps per second:  84, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.708 [0.000, 3.000],  loss: 0.004646, mae: 0.686885, mean_q: 0.932056, mean_eps: 0.619212\n",
      " 423495/1000000: episode: 1970, duration: 2.949s, episode steps: 259, steps per second:  88, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.660 [0.000, 3.000],  loss: 0.003656, mae: 0.679381, mean_q: 0.924448, mean_eps: 0.618972\n",
      " 423764/1000000: episode: 1971, duration: 3.116s, episode steps: 269, steps per second:  86, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.662 [0.000, 3.000],  loss: 0.004248, mae: 0.674019, mean_q: 0.919092, mean_eps: 0.618735\n",
      " 424251/1000000: episode: 1972, duration: 5.462s, episode steps: 487, steps per second:  89, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.727 [0.000, 3.000],  loss: 0.004904, mae: 0.680588, mean_q: 0.924480, mean_eps: 0.618395\n",
      " 424484/1000000: episode: 1973, duration: 2.858s, episode steps: 233, steps per second:  82, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.627 [0.000, 3.000],  loss: 0.003885, mae: 0.679320, mean_q: 0.924258, mean_eps: 0.618071\n",
      " 424702/1000000: episode: 1974, duration: 2.480s, episode steps: 218, steps per second:  88, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.004386, mae: 0.678374, mean_q: 0.923753, mean_eps: 0.617867\n",
      " 424986/1000000: episode: 1975, duration: 3.182s, episode steps: 284, steps per second:  89, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.651 [0.000, 3.000],  loss: 0.003693, mae: 0.676736, mean_q: 0.923679, mean_eps: 0.617640\n",
      " 425303/1000000: episode: 1976, duration: 3.577s, episode steps: 317, steps per second:  89, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.659 [0.000, 3.000],  loss: 0.004069, mae: 0.678660, mean_q: 0.922850, mean_eps: 0.617370\n",
      " 425535/1000000: episode: 1977, duration: 2.671s, episode steps: 232, steps per second:  87, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.003791, mae: 0.684042, mean_q: 0.932154, mean_eps: 0.617124\n",
      " 425957/1000000: episode: 1978, duration: 5.041s, episode steps: 422, steps per second:  84, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.756 [0.000, 3.000],  loss: 0.003990, mae: 0.681370, mean_q: 0.928283, mean_eps: 0.616829\n",
      " 426629/1000000: episode: 1979, duration: 7.479s, episode steps: 672, steps per second:  90, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.656 [0.000, 3.000],  loss: 0.003664, mae: 0.681290, mean_q: 0.930403, mean_eps: 0.616335\n",
      " 426883/1000000: episode: 1980, duration: 2.876s, episode steps: 254, steps per second:  88, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.717 [0.000, 3.000],  loss: 0.004323, mae: 0.677522, mean_q: 0.925600, mean_eps: 0.615920\n",
      " 427076/1000000: episode: 1981, duration: 2.252s, episode steps: 193, steps per second:  86, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.003287, mae: 0.678476, mean_q: 0.924722, mean_eps: 0.615720\n",
      " 427356/1000000: episode: 1982, duration: 3.417s, episode steps: 280, steps per second:  82, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.854 [0.000, 3.000],  loss: 0.003669, mae: 0.672858, mean_q: 0.917902, mean_eps: 0.615507\n",
      " 427559/1000000: episode: 1983, duration: 2.294s, episode steps: 203, steps per second:  89, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.700 [0.000, 3.000],  loss: 0.003493, mae: 0.681986, mean_q: 0.926731, mean_eps: 0.615290\n",
      " 427784/1000000: episode: 1984, duration: 2.556s, episode steps: 225, steps per second:  88, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.542 [0.000, 3.000],  loss: 0.005198, mae: 0.673669, mean_q: 0.915064, mean_eps: 0.615097\n",
      " 428370/1000000: episode: 1985, duration: 6.646s, episode steps: 586, steps per second:  88, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003582, mae: 0.683573, mean_q: 0.932926, mean_eps: 0.614732\n",
      " 428559/1000000: episode: 1986, duration: 2.190s, episode steps: 189, steps per second:  86, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.593 [0.000, 3.000],  loss: 0.003603, mae: 0.680199, mean_q: 0.925771, mean_eps: 0.614382\n",
      " 428961/1000000: episode: 1987, duration: 4.694s, episode steps: 402, steps per second:  86, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.617 [0.000, 3.000],  loss: 0.004223, mae: 0.675379, mean_q: 0.918647, mean_eps: 0.614116\n",
      " 429378/1000000: episode: 1988, duration: 4.783s, episode steps: 417, steps per second:  87, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 0.004558, mae: 0.679951, mean_q: 0.923820, mean_eps: 0.613747\n",
      " 429831/1000000: episode: 1989, duration: 5.221s, episode steps: 453, steps per second:  87, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.671 [0.000, 3.000],  loss: 0.004427, mae: 0.677964, mean_q: 0.922252, mean_eps: 0.613356\n",
      " 430182/1000000: episode: 1990, duration: 4.272s, episode steps: 351, steps per second:  82, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.678 [0.000, 3.000],  loss: 0.004395, mae: 0.694301, mean_q: 0.941209, mean_eps: 0.612995\n",
      " 430639/1000000: episode: 1991, duration: 5.184s, episode steps: 457, steps per second:  88, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.722 [0.000, 3.000],  loss: 0.003275, mae: 0.700281, mean_q: 0.952593, mean_eps: 0.612631\n",
      " 430890/1000000: episode: 1992, duration: 2.862s, episode steps: 251, steps per second:  88, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.426 [0.000, 3.000],  loss: 0.004303, mae: 0.700895, mean_q: 0.952125, mean_eps: 0.612312\n",
      " 431195/1000000: episode: 1993, duration: 3.542s, episode steps: 305, steps per second:  86, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.856 [0.000, 3.000],  loss: 0.003963, mae: 0.701371, mean_q: 0.956445, mean_eps: 0.612062\n",
      " 431461/1000000: episode: 1994, duration: 3.267s, episode steps: 266, steps per second:  81, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.677 [0.000, 3.000],  loss: 0.003752, mae: 0.695403, mean_q: 0.947299, mean_eps: 0.611805\n",
      " 431749/1000000: episode: 1995, duration: 3.253s, episode steps: 288, steps per second:  89, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 0.003242, mae: 0.692668, mean_q: 0.942560, mean_eps: 0.611555\n",
      " 432155/1000000: episode: 1996, duration: 4.535s, episode steps: 406, steps per second:  90, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.596 [0.000, 3.000],  loss: 0.003093, mae: 0.698362, mean_q: 0.949493, mean_eps: 0.611243\n",
      " 432334/1000000: episode: 1997, duration: 2.072s, episode steps: 179, steps per second:  86, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.709 [0.000, 3.000],  loss: 0.003396, mae: 0.700159, mean_q: 0.951125, mean_eps: 0.610980\n",
      " 432512/1000000: episode: 1998, duration: 2.027s, episode steps: 178, steps per second:  88, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.803 [0.000, 3.000],  loss: 0.002484, mae: 0.697808, mean_q: 0.948105, mean_eps: 0.610820\n",
      " 432709/1000000: episode: 1999, duration: 2.380s, episode steps: 197, steps per second:  83, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.685 [0.000, 3.000],  loss: 0.002967, mae: 0.698088, mean_q: 0.948127, mean_eps: 0.610651\n",
      " 433033/1000000: episode: 2000, duration: 3.826s, episode steps: 324, steps per second:  85, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.704 [0.000, 3.000],  loss: 0.003910, mae: 0.700133, mean_q: 0.949291, mean_eps: 0.610415\n",
      " 433504/1000000: episode: 2001, duration: 5.419s, episode steps: 471, steps per second:  87, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.766 [0.000, 3.000],  loss: 0.003676, mae: 0.701539, mean_q: 0.951920, mean_eps: 0.610059\n",
      " 433976/1000000: episode: 2002, duration: 5.398s, episode steps: 472, steps per second:  87, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.682 [0.000, 3.000],  loss: 0.003391, mae: 0.696797, mean_q: 0.947287, mean_eps: 0.609636\n",
      " 434442/1000000: episode: 2003, duration: 5.431s, episode steps: 466, steps per second:  86, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.803 [0.000, 3.000],  loss: 0.003153, mae: 0.697890, mean_q: 0.949976, mean_eps: 0.609213\n",
      " 434808/1000000: episode: 2004, duration: 4.190s, episode steps: 366, steps per second:  87, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: 0.002674, mae: 0.696155, mean_q: 0.946907, mean_eps: 0.608838\n",
      " 435358/1000000: episode: 2005, duration: 6.276s, episode steps: 550, steps per second:  88, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.656 [0.000, 3.000],  loss: 0.003086, mae: 0.699694, mean_q: 0.951540, mean_eps: 0.608426\n",
      " 435830/1000000: episode: 2006, duration: 5.593s, episode steps: 472, steps per second:  84, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.699 [0.000, 3.000],  loss: 0.003122, mae: 0.701890, mean_q: 0.955144, mean_eps: 0.607965\n",
      " 436192/1000000: episode: 2007, duration: 4.148s, episode steps: 362, steps per second:  87, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.820 [0.000, 3.000],  loss: 0.003122, mae: 0.691939, mean_q: 0.943329, mean_eps: 0.607591\n",
      " 436533/1000000: episode: 2008, duration: 3.970s, episode steps: 341, steps per second:  86, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.730 [0.000, 3.000],  loss: 0.003249, mae: 0.698764, mean_q: 0.950889, mean_eps: 0.607274\n",
      " 436755/1000000: episode: 2009, duration: 2.573s, episode steps: 222, steps per second:  86, episode reward:  3.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.788 [0.000, 3.000],  loss: 0.002984, mae: 0.694990, mean_q: 0.944423, mean_eps: 0.607020\n",
      " 436938/1000000: episode: 2010, duration: 2.258s, episode steps: 183, steps per second:  81, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.760 [0.000, 3.000],  loss: 0.002976, mae: 0.702605, mean_q: 0.956039, mean_eps: 0.606839\n",
      " 437175/1000000: episode: 2011, duration: 2.694s, episode steps: 237, steps per second:  88, episode reward:  2.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.844 [0.000, 3.000],  loss: 0.003474, mae: 0.701568, mean_q: 0.953823, mean_eps: 0.606650\n",
      " 437363/1000000: episode: 2012, duration: 2.142s, episode steps: 188, steps per second:  88, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.002663, mae: 0.700892, mean_q: 0.958314, mean_eps: 0.606459\n",
      " 437579/1000000: episode: 2013, duration: 2.486s, episode steps: 216, steps per second:  87, episode reward:  3.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.690 [0.000, 3.000],  loss: 0.003835, mae: 0.696733, mean_q: 0.947503, mean_eps: 0.606277\n",
      " 438001/1000000: episode: 2014, duration: 4.831s, episode steps: 422, steps per second:  87, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.611 [0.000, 3.000],  loss: 0.002816, mae: 0.700414, mean_q: 0.955181, mean_eps: 0.605989\n",
      " 438321/1000000: episode: 2015, duration: 3.881s, episode steps: 320, steps per second:  82, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.712 [0.000, 3.000],  loss: 0.003761, mae: 0.701369, mean_q: 0.953062, mean_eps: 0.605654\n",
      " 438687/1000000: episode: 2016, duration: 4.264s, episode steps: 366, steps per second:  86, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.754 [0.000, 3.000],  loss: 0.003724, mae: 0.704215, mean_q: 0.956791, mean_eps: 0.605346\n",
      " 439131/1000000: episode: 2017, duration: 5.112s, episode steps: 444, steps per second:  87, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.631 [0.000, 3.000],  loss: 0.002904, mae: 0.694092, mean_q: 0.946003, mean_eps: 0.604983\n",
      " 439592/1000000: episode: 2018, duration: 5.317s, episode steps: 461, steps per second:  87, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.759 [0.000, 3.000],  loss: 0.003026, mae: 0.701600, mean_q: 0.954786, mean_eps: 0.604576\n",
      " 439858/1000000: episode: 2019, duration: 3.173s, episode steps: 266, steps per second:  84, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.714 [0.000, 3.000],  loss: 0.003459, mae: 0.696002, mean_q: 0.946811, mean_eps: 0.604248\n",
      " 440188/1000000: episode: 2020, duration: 3.772s, episode steps: 330, steps per second:  87, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.003403, mae: 0.711173, mean_q: 0.962591, mean_eps: 0.603980\n",
      " 440438/1000000: episode: 2021, duration: 2.950s, episode steps: 250, steps per second:  85, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.436 [0.000, 3.000],  loss: 0.002561, mae: 0.724163, mean_q: 0.981371, mean_eps: 0.603719\n",
      " 440830/1000000: episode: 2022, duration: 4.483s, episode steps: 392, steps per second:  87, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.002456, mae: 0.722829, mean_q: 0.982988, mean_eps: 0.603429\n",
      " 441223/1000000: episode: 2023, duration: 4.655s, episode steps: 393, steps per second:  84, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 0.002604, mae: 0.721777, mean_q: 0.980815, mean_eps: 0.603077\n",
      " 441429/1000000: episode: 2024, duration: 2.338s, episode steps: 206, steps per second:  88, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.631 [0.000, 3.000],  loss: 0.002159, mae: 0.720286, mean_q: 0.979665, mean_eps: 0.602807\n",
      " 441799/1000000: episode: 2025, duration: 4.303s, episode steps: 370, steps per second:  86, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.749 [0.000, 3.000],  loss: 0.002281, mae: 0.724962, mean_q: 0.985933, mean_eps: 0.602547\n",
      " 442126/1000000: episode: 2026, duration: 3.756s, episode steps: 327, steps per second:  87, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.593 [0.000, 3.000],  loss: 0.002488, mae: 0.720879, mean_q: 0.980395, mean_eps: 0.602234\n",
      " 442518/1000000: episode: 2027, duration: 4.695s, episode steps: 392, steps per second:  83, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002438, mae: 0.725654, mean_q: 0.986420, mean_eps: 0.601910\n",
      " 442842/1000000: episode: 2028, duration: 3.678s, episode steps: 324, steps per second:  88, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.728 [0.000, 3.000],  loss: 0.002815, mae: 0.721438, mean_q: 0.978246, mean_eps: 0.601588\n",
      " 443247/1000000: episode: 2029, duration: 4.614s, episode steps: 405, steps per second:  88, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.625 [0.000, 3.000],  loss: 0.002292, mae: 0.722360, mean_q: 0.981003, mean_eps: 0.601260\n",
      " 443614/1000000: episode: 2030, duration: 4.192s, episode steps: 367, steps per second:  88, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.741 [0.000, 3.000],  loss: 0.002331, mae: 0.720771, mean_q: 0.979560, mean_eps: 0.600913\n",
      " 443929/1000000: episode: 2031, duration: 3.788s, episode steps: 315, steps per second:  83, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.733 [0.000, 3.000],  loss: 0.002385, mae: 0.722731, mean_q: 0.983212, mean_eps: 0.600605\n",
      " 444196/1000000: episode: 2032, duration: 3.059s, episode steps: 267, steps per second:  87, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.659 [0.000, 3.000],  loss: 0.002651, mae: 0.723667, mean_q: 0.981781, mean_eps: 0.600344\n",
      " 444623/1000000: episode: 2033, duration: 4.884s, episode steps: 427, steps per second:  87, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 0.002684, mae: 0.718415, mean_q: 0.975432, mean_eps: 0.600033\n",
      " 444760/1000000: episode: 2034, duration: 1.585s, episode steps: 137, steps per second:  86, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.001845, mae: 0.725256, mean_q: 0.984437, mean_eps: 0.599779\n",
      " 445258/1000000: episode: 2035, duration: 5.863s, episode steps: 498, steps per second:  85, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.002269, mae: 0.717086, mean_q: 0.972947, mean_eps: 0.599493\n",
      " 445707/1000000: episode: 2036, duration: 5.062s, episode steps: 449, steps per second:  89, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.744 [0.000, 3.000],  loss: 0.002586, mae: 0.723526, mean_q: 0.982863, mean_eps: 0.599066\n",
      " 446101/1000000: episode: 2037, duration: 4.628s, episode steps: 394, steps per second:  85, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.741 [0.000, 3.000],  loss: 0.002207, mae: 0.719134, mean_q: 0.978045, mean_eps: 0.598686\n",
      " 446335/1000000: episode: 2038, duration: 2.652s, episode steps: 234, steps per second:  88, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.637 [0.000, 3.000],  loss: 0.002588, mae: 0.727388, mean_q: 0.986744, mean_eps: 0.598404\n",
      " 446722/1000000: episode: 2039, duration: 4.617s, episode steps: 387, steps per second:  84, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.695 [0.000, 3.000],  loss: 0.002563, mae: 0.721890, mean_q: 0.980178, mean_eps: 0.598125\n",
      " 447044/1000000: episode: 2040, duration: 3.664s, episode steps: 322, steps per second:  88, episode reward:  6.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: 0.002374, mae: 0.721239, mean_q: 0.979806, mean_eps: 0.597806\n",
      " 447250/1000000: episode: 2041, duration: 2.414s, episode steps: 206, steps per second:  85, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.850 [0.000, 3.000],  loss: 0.001827, mae: 0.723290, mean_q: 0.982101, mean_eps: 0.597569\n",
      " 447620/1000000: episode: 2042, duration: 4.259s, episode steps: 370, steps per second:  87, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002436, mae: 0.717220, mean_q: 0.974803, mean_eps: 0.597309\n",
      " 447950/1000000: episode: 2043, duration: 3.908s, episode steps: 330, steps per second:  84, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.676 [0.000, 3.000],  loss: 0.002095, mae: 0.718478, mean_q: 0.976399, mean_eps: 0.596994\n",
      " 448414/1000000: episode: 2044, duration: 5.264s, episode steps: 464, steps per second:  88, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.718 [0.000, 3.000],  loss: 0.002428, mae: 0.718344, mean_q: 0.976653, mean_eps: 0.596636\n",
      " 448683/1000000: episode: 2045, duration: 3.111s, episode steps: 269, steps per second:  86, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.717 [0.000, 3.000],  loss: 0.002431, mae: 0.722038, mean_q: 0.978441, mean_eps: 0.596307\n",
      " 448934/1000000: episode: 2046, duration: 2.883s, episode steps: 251, steps per second:  87, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.641 [0.000, 3.000],  loss: 0.002480, mae: 0.724511, mean_q: 0.984787, mean_eps: 0.596073\n",
      " 449226/1000000: episode: 2047, duration: 3.405s, episode steps: 292, steps per second:  86, episode reward:  5.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.002339, mae: 0.718991, mean_q: 0.975033, mean_eps: 0.595828\n",
      " 449680/1000000: episode: 2048, duration: 5.370s, episode steps: 454, steps per second:  85, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002236, mae: 0.720050, mean_q: 0.977923, mean_eps: 0.595493\n",
      " 450063/1000000: episode: 2049, duration: 4.407s, episode steps: 383, steps per second:  87, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.757 [0.000, 3.000],  loss: 0.002738, mae: 0.724155, mean_q: 0.983861, mean_eps: 0.595117\n",
      " 450563/1000000: episode: 2050, duration: 5.776s, episode steps: 500, steps per second:  87, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.644 [0.000, 3.000],  loss: 0.002195, mae: 0.740744, mean_q: 1.004305, mean_eps: 0.594719\n",
      " 450973/1000000: episode: 2051, duration: 4.978s, episode steps: 410, steps per second:  82, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.001992, mae: 0.734330, mean_q: 0.994000, mean_eps: 0.594309\n",
      " 451485/1000000: episode: 2052, duration: 5.905s, episode steps: 512, steps per second:  87, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 0.001584, mae: 0.737622, mean_q: 0.998784, mean_eps: 0.593893\n",
      " 451668/1000000: episode: 2053, duration: 2.175s, episode steps: 183, steps per second:  84, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.001688, mae: 0.725624, mean_q: 0.986562, mean_eps: 0.593582\n",
      " 451989/1000000: episode: 2054, duration: 3.784s, episode steps: 321, steps per second:  85, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.798 [0.000, 3.000],  loss: 0.001971, mae: 0.734353, mean_q: 0.994861, mean_eps: 0.593355\n",
      " 452347/1000000: episode: 2055, duration: 4.204s, episode steps: 358, steps per second:  85, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.601 [0.000, 3.000],  loss: 0.001984, mae: 0.733930, mean_q: 0.993708, mean_eps: 0.593049\n",
      " 452894/1000000: episode: 2056, duration: 6.298s, episode steps: 547, steps per second:  87, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: 0.001829, mae: 0.738820, mean_q: 1.001307, mean_eps: 0.592642\n",
      " 453201/1000000: episode: 2057, duration: 3.614s, episode steps: 307, steps per second:  85, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.612 [0.000, 3.000],  loss: 0.001951, mae: 0.736774, mean_q: 0.998872, mean_eps: 0.592257\n",
      " 453565/1000000: episode: 2058, duration: 4.417s, episode steps: 364, steps per second:  82, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.582 [0.000, 3.000],  loss: 0.001735, mae: 0.733689, mean_q: 0.994382, mean_eps: 0.591954\n",
      " 453868/1000000: episode: 2059, duration: 3.519s, episode steps: 303, steps per second:  86, episode reward:  5.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.818 [0.000, 3.000],  loss: 0.001777, mae: 0.735723, mean_q: 0.996355, mean_eps: 0.591656\n",
      " 454218/1000000: episode: 2060, duration: 3.990s, episode steps: 350, steps per second:  88, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 0.002145, mae: 0.736109, mean_q: 0.996607, mean_eps: 0.591362\n",
      " 454548/1000000: episode: 2061, duration: 3.831s, episode steps: 330, steps per second:  86, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.627 [0.000, 3.000],  loss: 0.001665, mae: 0.734535, mean_q: 0.995821, mean_eps: 0.591056\n",
      " 454899/1000000: episode: 2062, duration: 4.337s, episode steps: 351, steps per second:  81, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.735 [0.000, 3.000],  loss: 0.001824, mae: 0.738163, mean_q: 1.001381, mean_eps: 0.590750\n",
      " 455320/1000000: episode: 2063, duration: 4.840s, episode steps: 421, steps per second:  87, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.001540, mae: 0.740986, mean_q: 1.002905, mean_eps: 0.590403\n",
      " 455647/1000000: episode: 2064, duration: 3.747s, episode steps: 327, steps per second:  87, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.001640, mae: 0.739771, mean_q: 1.003000, mean_eps: 0.590066\n",
      " 456046/1000000: episode: 2065, duration: 4.714s, episode steps: 399, steps per second:  85, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.596 [0.000, 3.000],  loss: 0.001422, mae: 0.737056, mean_q: 0.997618, mean_eps: 0.589739\n",
      " 456249/1000000: episode: 2066, duration: 2.632s, episode steps: 203, steps per second:  77, episode reward:  1.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.665 [0.000, 3.000],  loss: 0.002107, mae: 0.737330, mean_q: 0.996734, mean_eps: 0.589467\n",
      " 456468/1000000: episode: 2067, duration: 2.610s, episode steps: 219, steps per second:  84, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.845 [0.000, 3.000],  loss: 0.002181, mae: 0.735197, mean_q: 0.996751, mean_eps: 0.589278\n",
      " 456797/1000000: episode: 2068, duration: 3.866s, episode steps: 329, steps per second:  85, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.574 [0.000, 3.000],  loss: 0.001603, mae: 0.735801, mean_q: 0.995359, mean_eps: 0.589031\n",
      " 456978/1000000: episode: 2069, duration: 2.106s, episode steps: 181, steps per second:  86, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.785 [0.000, 3.000],  loss: 0.001612, mae: 0.736535, mean_q: 0.999388, mean_eps: 0.588801\n",
      " 457144/1000000: episode: 2070, duration: 1.966s, episode steps: 166, steps per second:  84, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.771 [0.000, 3.000],  loss: 0.001904, mae: 0.731154, mean_q: 0.990765, mean_eps: 0.588646\n",
      " 457567/1000000: episode: 2071, duration: 5.082s, episode steps: 423, steps per second:  83, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: 0.001716, mae: 0.731897, mean_q: 0.991645, mean_eps: 0.588381\n",
      " 458052/1000000: episode: 2072, duration: 5.649s, episode steps: 485, steps per second:  86, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.001729, mae: 0.737050, mean_q: 0.999137, mean_eps: 0.587973\n",
      " 458271/1000000: episode: 2073, duration: 2.656s, episode steps: 219, steps per second:  82, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.001631, mae: 0.734068, mean_q: 0.993839, mean_eps: 0.587656\n",
      " 458582/1000000: episode: 2074, duration: 3.618s, episode steps: 311, steps per second:  86, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.617 [0.000, 3.000],  loss: 0.001430, mae: 0.743476, mean_q: 1.005846, mean_eps: 0.587417\n",
      " 458776/1000000: episode: 2075, duration: 2.322s, episode steps: 194, steps per second:  84, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.423 [0.000, 3.000],  loss: 0.001586, mae: 0.753884, mean_q: 1.018619, mean_eps: 0.587190\n",
      " 459001/1000000: episode: 2076, duration: 2.703s, episode steps: 225, steps per second:  83, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.671 [0.000, 3.000],  loss: 0.002087, mae: 0.726600, mean_q: 0.982395, mean_eps: 0.587001\n",
      " 459345/1000000: episode: 2077, duration: 4.037s, episode steps: 344, steps per second:  85, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.715 [0.000, 3.000],  loss: 0.001689, mae: 0.738261, mean_q: 0.999234, mean_eps: 0.586743\n",
      " 459733/1000000: episode: 2078, duration: 4.555s, episode steps: 388, steps per second:  85, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.001582, mae: 0.738247, mean_q: 1.000248, mean_eps: 0.586414\n",
      " 460072/1000000: episode: 2079, duration: 3.955s, episode steps: 339, steps per second:  86, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.746 [0.000, 3.000],  loss: 0.001665, mae: 0.739493, mean_q: 1.003040, mean_eps: 0.586088\n",
      " 460490/1000000: episode: 2080, duration: 4.985s, episode steps: 418, steps per second:  84, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.703 [0.000, 3.000],  loss: 0.002078, mae: 0.760402, mean_q: 1.029077, mean_eps: 0.585748\n",
      " 460722/1000000: episode: 2081, duration: 2.766s, episode steps: 232, steps per second:  84, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: 0.001556, mae: 0.755694, mean_q: 1.020423, mean_eps: 0.585455\n",
      " 460980/1000000: episode: 2082, duration: 2.969s, episode steps: 258, steps per second:  87, episode reward:  4.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.001470, mae: 0.758148, mean_q: 1.023635, mean_eps: 0.585235\n",
      " 461325/1000000: episode: 2083, duration: 4.000s, episode steps: 345, steps per second:  86, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.725 [0.000, 3.000],  loss: 0.001774, mae: 0.747623, mean_q: 1.009708, mean_eps: 0.584963\n",
      " 461756/1000000: episode: 2084, duration: 5.191s, episode steps: 431, steps per second:  83, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.606 [0.000, 3.000],  loss: 0.001692, mae: 0.753018, mean_q: 1.017181, mean_eps: 0.584614\n",
      " 462017/1000000: episode: 2085, duration: 3.076s, episode steps: 261, steps per second:  85, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.720 [0.000, 3.000],  loss: 0.001576, mae: 0.746576, mean_q: 1.007519, mean_eps: 0.584303\n",
      " 462334/1000000: episode: 2086, duration: 3.657s, episode steps: 317, steps per second:  87, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: 0.001362, mae: 0.750919, mean_q: 1.012648, mean_eps: 0.584042\n",
      " 462601/1000000: episode: 2087, duration: 3.107s, episode steps: 267, steps per second:  86, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.001585, mae: 0.753252, mean_q: 1.015182, mean_eps: 0.583779\n",
      " 462957/1000000: episode: 2088, duration: 4.338s, episode steps: 356, steps per second:  82, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.528 [0.000, 3.000],  loss: 0.001606, mae: 0.751083, mean_q: 1.013408, mean_eps: 0.583498\n",
      " 463297/1000000: episode: 2089, duration: 4.015s, episode steps: 340, steps per second:  85, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.597 [0.000, 3.000],  loss: 0.001389, mae: 0.753376, mean_q: 1.016133, mean_eps: 0.583185\n",
      " 463569/1000000: episode: 2090, duration: 3.177s, episode steps: 272, steps per second:  86, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.426 [0.000, 3.000],  loss: 0.001763, mae: 0.747873, mean_q: 1.009657, mean_eps: 0.582909\n",
      " 463987/1000000: episode: 2091, duration: 4.866s, episode steps: 418, steps per second:  86, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.648 [0.000, 3.000],  loss: 0.001680, mae: 0.748964, mean_q: 1.010243, mean_eps: 0.582600\n",
      " 464259/1000000: episode: 2092, duration: 3.259s, episode steps: 272, steps per second:  83, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.684 [0.000, 3.000],  loss: 0.001476, mae: 0.758285, mean_q: 1.025511, mean_eps: 0.582290\n",
      " 464590/1000000: episode: 2093, duration: 3.953s, episode steps: 331, steps per second:  84, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 0.001944, mae: 0.751953, mean_q: 1.017323, mean_eps: 0.582018\n",
      " 464973/1000000: episode: 2094, duration: 4.500s, episode steps: 383, steps per second:  85, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.001411, mae: 0.751882, mean_q: 1.015235, mean_eps: 0.581696\n",
      " 465530/1000000: episode: 2095, duration: 6.423s, episode steps: 557, steps per second:  87, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.636 [0.000, 3.000],  loss: 0.001515, mae: 0.750878, mean_q: 1.015261, mean_eps: 0.581273\n",
      " 465924/1000000: episode: 2096, duration: 4.801s, episode steps: 394, steps per second:  82, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.001474, mae: 0.754442, mean_q: 1.019257, mean_eps: 0.580847\n",
      " 466105/1000000: episode: 2097, duration: 2.201s, episode steps: 181, steps per second:  82, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.586 [0.000, 3.000],  loss: 0.001858, mae: 0.754095, mean_q: 1.018288, mean_eps: 0.580587\n",
      " 466663/1000000: episode: 2098, duration: 6.642s, episode steps: 558, steps per second:  84, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.685 [0.000, 3.000],  loss: 0.001457, mae: 0.751664, mean_q: 1.015821, mean_eps: 0.580254\n",
      " 466985/1000000: episode: 2099, duration: 4.100s, episode steps: 322, steps per second:  79, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.680 [0.000, 3.000],  loss: 0.001777, mae: 0.747264, mean_q: 1.009002, mean_eps: 0.579858\n",
      " 467271/1000000: episode: 2100, duration: 3.362s, episode steps: 286, steps per second:  85, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.689 [0.000, 3.000],  loss: 0.002119, mae: 0.750423, mean_q: 1.013709, mean_eps: 0.579585\n",
      " 467689/1000000: episode: 2101, duration: 4.953s, episode steps: 418, steps per second:  84, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.754 [0.000, 3.000],  loss: 0.001556, mae: 0.754449, mean_q: 1.018665, mean_eps: 0.579268\n",
      " 467976/1000000: episode: 2102, duration: 3.363s, episode steps: 287, steps per second:  85, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.652 [0.000, 3.000],  loss: 0.001663, mae: 0.753008, mean_q: 1.015546, mean_eps: 0.578951\n",
      " 468229/1000000: episode: 2103, duration: 3.031s, episode steps: 253, steps per second:  83, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.494 [0.000, 3.000],  loss: 0.001700, mae: 0.749373, mean_q: 1.011800, mean_eps: 0.578708\n",
      " 468543/1000000: episode: 2104, duration: 3.669s, episode steps: 314, steps per second:  86, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.685 [0.000, 3.000],  loss: 0.001722, mae: 0.754451, mean_q: 1.019648, mean_eps: 0.578453\n",
      " 468800/1000000: episode: 2105, duration: 3.064s, episode steps: 257, steps per second:  84, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.572 [0.000, 3.000],  loss: 0.001344, mae: 0.757119, mean_q: 1.023653, mean_eps: 0.578197\n",
      " 469238/1000000: episode: 2106, duration: 5.141s, episode steps: 438, steps per second:  85, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 0.001346, mae: 0.748759, mean_q: 1.010840, mean_eps: 0.577884\n",
      " 469594/1000000: episode: 2107, duration: 4.225s, episode steps: 356, steps per second:  84, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.669 [0.000, 3.000],  loss: 0.001526, mae: 0.751695, mean_q: 1.016083, mean_eps: 0.577526\n",
      " 469786/1000000: episode: 2108, duration: 2.336s, episode steps: 192, steps per second:  82, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.734 [0.000, 3.000],  loss: 0.001506, mae: 0.755773, mean_q: 1.020710, mean_eps: 0.577279\n",
      " 469994/1000000: episode: 2109, duration: 2.469s, episode steps: 208, steps per second:  84, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 0.001955, mae: 0.747990, mean_q: 1.011015, mean_eps: 0.577099\n",
      " 470270/1000000: episode: 2110, duration: 3.246s, episode steps: 276, steps per second:  85, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.707 [0.000, 3.000],  loss: 0.001986, mae: 0.766775, mean_q: 1.032962, mean_eps: 0.576881\n",
      " 470553/1000000: episode: 2111, duration: 3.327s, episode steps: 283, steps per second:  85, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.456 [0.000, 3.000],  loss: 0.001661, mae: 0.766375, mean_q: 1.032664, mean_eps: 0.576629\n",
      " 470810/1000000: episode: 2112, duration: 3.117s, episode steps: 257, steps per second:  82, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.498 [0.000, 3.000],  loss: 0.001220, mae: 0.769408, mean_q: 1.037817, mean_eps: 0.576386\n",
      " 471091/1000000: episode: 2113, duration: 3.488s, episode steps: 281, steps per second:  81, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.573 [0.000, 3.000],  loss: 0.001626, mae: 0.769560, mean_q: 1.037791, mean_eps: 0.576145\n",
      " 471389/1000000: episode: 2114, duration: 3.529s, episode steps: 298, steps per second:  84, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.758 [0.000, 3.000],  loss: 0.001272, mae: 0.760947, mean_q: 1.028567, mean_eps: 0.575884\n",
      " 471590/1000000: episode: 2115, duration: 2.353s, episode steps: 201, steps per second:  85, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.627 [0.000, 3.000],  loss: 0.001790, mae: 0.770737, mean_q: 1.039672, mean_eps: 0.575659\n",
      " 471872/1000000: episode: 2116, duration: 3.352s, episode steps: 282, steps per second:  84, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.613 [0.000, 3.000],  loss: 0.001598, mae: 0.774375, mean_q: 1.042884, mean_eps: 0.575443\n",
      " 472511/1000000: episode: 2117, duration: 7.750s, episode steps: 639, steps per second:  82, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.717 [0.000, 3.000],  loss: 0.001316, mae: 0.766741, mean_q: 1.034099, mean_eps: 0.575029\n",
      " 472902/1000000: episode: 2118, duration: 4.573s, episode steps: 391, steps per second:  85, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.729 [0.000, 3.000],  loss: 0.001571, mae: 0.764875, mean_q: 1.032508, mean_eps: 0.574565\n",
      " 473261/1000000: episode: 2119, duration: 4.174s, episode steps: 359, steps per second:  86, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.760 [0.000, 3.000],  loss: 0.001541, mae: 0.767809, mean_q: 1.035982, mean_eps: 0.574226\n",
      " 473645/1000000: episode: 2120, duration: 4.693s, episode steps: 384, steps per second:  82, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.539 [0.000, 3.000],  loss: 0.001061, mae: 0.773338, mean_q: 1.044516, mean_eps: 0.573891\n",
      " 474077/1000000: episode: 2121, duration: 5.207s, episode steps: 432, steps per second:  83, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.523 [0.000, 3.000],  loss: 0.001407, mae: 0.763276, mean_q: 1.030542, mean_eps: 0.573524\n",
      " 474247/1000000: episode: 2122, duration: 1.993s, episode steps: 170, steps per second:  85, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: 0.001596, mae: 0.768738, mean_q: 1.038101, mean_eps: 0.573254\n",
      " 474480/1000000: episode: 2123, duration: 2.786s, episode steps: 233, steps per second:  84, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.001761, mae: 0.763513, mean_q: 1.031576, mean_eps: 0.573074\n",
      " 474799/1000000: episode: 2124, duration: 3.832s, episode steps: 319, steps per second:  83, episode reward:  6.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.652 [0.000, 3.000],  loss: 0.001839, mae: 0.761437, mean_q: 1.028522, mean_eps: 0.572826\n",
      " 475136/1000000: episode: 2125, duration: 4.208s, episode steps: 337, steps per second:  80, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.001404, mae: 0.765178, mean_q: 1.033383, mean_eps: 0.572531\n",
      " 475458/1000000: episode: 2126, duration: 3.821s, episode steps: 322, steps per second:  84, episode reward:  6.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.742 [0.000, 3.000],  loss: 0.001418, mae: 0.765821, mean_q: 1.034083, mean_eps: 0.572234\n",
      " 475825/1000000: episode: 2127, duration: 4.377s, episode steps: 367, steps per second:  84, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.733 [0.000, 3.000],  loss: 0.001422, mae: 0.771572, mean_q: 1.042177, mean_eps: 0.571922\n",
      " 475955/1000000: episode: 2128, duration: 1.560s, episode steps: 130, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.562 [0.000, 3.000],  loss: 0.001359, mae: 0.770469, mean_q: 1.041165, mean_eps: 0.571699\n",
      " 476277/1000000: episode: 2129, duration: 3.972s, episode steps: 322, steps per second:  81, episode reward:  6.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.537 [0.000, 3.000],  loss: 0.001161, mae: 0.760609, mean_q: 1.028333, mean_eps: 0.571496\n",
      " 476595/1000000: episode: 2130, duration: 3.829s, episode steps: 318, steps per second:  83, episode reward:  6.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.660 [0.000, 3.000],  loss: 0.001580, mae: 0.763502, mean_q: 1.029995, mean_eps: 0.571208\n",
      " 476904/1000000: episode: 2131, duration: 3.698s, episode steps: 309, steps per second:  84, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.537 [0.000, 3.000],  loss: 0.001255, mae: 0.766464, mean_q: 1.034134, mean_eps: 0.570927\n",
      " 477145/1000000: episode: 2132, duration: 2.938s, episode steps: 241, steps per second:  82, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.656 [0.000, 3.000],  loss: 0.001640, mae: 0.765300, mean_q: 1.034852, mean_eps: 0.570678\n",
      " 477667/1000000: episode: 2133, duration: 6.377s, episode steps: 522, steps per second:  82, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 0.001670, mae: 0.768606, mean_q: 1.037120, mean_eps: 0.570335\n",
      " 478109/1000000: episode: 2134, duration: 5.292s, episode steps: 442, steps per second:  84, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.629 [0.000, 3.000],  loss: 0.001534, mae: 0.765729, mean_q: 1.033930, mean_eps: 0.569901\n",
      " 478419/1000000: episode: 2135, duration: 3.644s, episode steps: 310, steps per second:  85, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 0.001436, mae: 0.771107, mean_q: 1.039718, mean_eps: 0.569562\n",
      " 478684/1000000: episode: 2136, duration: 3.180s, episode steps: 265, steps per second:  83, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 0.001381, mae: 0.772617, mean_q: 1.043405, mean_eps: 0.569305\n",
      " 478887/1000000: episode: 2137, duration: 2.523s, episode steps: 203, steps per second:  80, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.700 [0.000, 3.000],  loss: 0.001206, mae: 0.764509, mean_q: 1.032477, mean_eps: 0.569094\n",
      " 479169/1000000: episode: 2138, duration: 3.432s, episode steps: 282, steps per second:  82, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.699 [0.000, 3.000],  loss: 0.001387, mae: 0.774470, mean_q: 1.045507, mean_eps: 0.568875\n",
      " 479474/1000000: episode: 2139, duration: 3.530s, episode steps: 305, steps per second:  86, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.590 [0.000, 3.000],  loss: 0.001295, mae: 0.765205, mean_q: 1.033566, mean_eps: 0.568610\n",
      " 479720/1000000: episode: 2140, duration: 3.005s, episode steps: 246, steps per second:  82, episode reward:  4.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.850 [0.000, 3.000],  loss: 0.001451, mae: 0.757855, mean_q: 1.025652, mean_eps: 0.568364\n",
      " 480227/1000000: episode: 2141, duration: 6.046s, episode steps: 507, steps per second:  84, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.619 [0.000, 3.000],  loss: 0.001825, mae: 0.772637, mean_q: 1.041929, mean_eps: 0.568025\n",
      " 480512/1000000: episode: 2142, duration: 3.512s, episode steps: 285, steps per second:  81, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.684 [0.000, 3.000],  loss: 0.001462, mae: 0.778385, mean_q: 1.047814, mean_eps: 0.567669\n",
      " 480828/1000000: episode: 2143, duration: 3.751s, episode steps: 316, steps per second:  84, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.785 [0.000, 3.000],  loss: 0.001628, mae: 0.773997, mean_q: 1.042431, mean_eps: 0.567399\n",
      " 481109/1000000: episode: 2144, duration: 3.378s, episode steps: 281, steps per second:  83, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.619 [0.000, 3.000],  loss: 0.001688, mae: 0.781655, mean_q: 1.052142, mean_eps: 0.567129\n",
      " 481581/1000000: episode: 2145, duration: 5.616s, episode steps: 472, steps per second:  84, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.470 [0.000, 3.000],  loss: 0.001351, mae: 0.778445, mean_q: 1.047482, mean_eps: 0.566789\n",
      " 482082/1000000: episode: 2146, duration: 5.962s, episode steps: 501, steps per second:  84, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 0.001563, mae: 0.774323, mean_q: 1.042736, mean_eps: 0.566351\n",
      " 482290/1000000: episode: 2147, duration: 2.462s, episode steps: 208, steps per second:  84, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.726 [0.000, 3.000],  loss: 0.001023, mae: 0.778963, mean_q: 1.048363, mean_eps: 0.566033\n",
      " 482574/1000000: episode: 2148, duration: 3.395s, episode steps: 284, steps per second:  84, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.768 [0.000, 3.000],  loss: 0.001358, mae: 0.780684, mean_q: 1.051903, mean_eps: 0.565811\n",
      " 482888/1000000: episode: 2149, duration: 3.829s, episode steps: 314, steps per second:  82, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.755 [0.000, 3.000],  loss: 0.001603, mae: 0.774514, mean_q: 1.044176, mean_eps: 0.565543\n",
      " 483164/1000000: episode: 2150, duration: 3.434s, episode steps: 276, steps per second:  80, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.428 [0.000, 3.000],  loss: 0.001304, mae: 0.781129, mean_q: 1.051881, mean_eps: 0.565278\n",
      " 483420/1000000: episode: 2151, duration: 3.023s, episode steps: 256, steps per second:  85, episode reward:  4.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.730 [0.000, 3.000],  loss: 0.001372, mae: 0.776656, mean_q: 1.047565, mean_eps: 0.565039\n",
      " 483730/1000000: episode: 2152, duration: 3.736s, episode steps: 310, steps per second:  83, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.635 [0.000, 3.000],  loss: 0.001530, mae: 0.768928, mean_q: 1.036469, mean_eps: 0.564783\n",
      " 484257/1000000: episode: 2153, duration: 6.313s, episode steps: 527, steps per second:  83, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.636 [0.000, 3.000],  loss: 0.001324, mae: 0.778326, mean_q: 1.049204, mean_eps: 0.564405\n",
      " 484607/1000000: episode: 2154, duration: 4.259s, episode steps: 350, steps per second:  82, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.001143, mae: 0.775770, mean_q: 1.045183, mean_eps: 0.564011\n",
      " 484931/1000000: episode: 2155, duration: 3.897s, episode steps: 324, steps per second:  83, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.707 [0.000, 3.000],  loss: 0.001348, mae: 0.780029, mean_q: 1.050475, mean_eps: 0.563709\n",
      " 485213/1000000: episode: 2156, duration: 3.431s, episode steps: 282, steps per second:  82, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.645 [0.000, 3.000],  loss: 0.001214, mae: 0.774212, mean_q: 1.043662, mean_eps: 0.563435\n",
      " 485428/1000000: episode: 2157, duration: 2.559s, episode steps: 215, steps per second:  84, episode reward:  3.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 0.001281, mae: 0.781878, mean_q: 1.056028, mean_eps: 0.563212\n",
      " 485657/1000000: episode: 2158, duration: 2.977s, episode steps: 229, steps per second:  77, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.001262, mae: 0.773759, mean_q: 1.043610, mean_eps: 0.563012\n",
      " 486035/1000000: episode: 2159, duration: 4.416s, episode steps: 378, steps per second:  86, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.659 [0.000, 3.000],  loss: 0.001623, mae: 0.771404, mean_q: 1.038733, mean_eps: 0.562739\n",
      " 486350/1000000: episode: 2160, duration: 3.816s, episode steps: 315, steps per second:  83, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.708 [0.000, 3.000],  loss: 0.001166, mae: 0.782795, mean_q: 1.054805, mean_eps: 0.562427\n",
      " 486692/1000000: episode: 2161, duration: 4.039s, episode steps: 342, steps per second:  85, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.620 [0.000, 3.000],  loss: 0.001485, mae: 0.776743, mean_q: 1.047657, mean_eps: 0.562132\n",
      " 487047/1000000: episode: 2162, duration: 4.387s, episode steps: 355, steps per second:  81, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.811 [0.000, 3.000],  loss: 0.001257, mae: 0.786627, mean_q: 1.059585, mean_eps: 0.561819\n",
      " 487280/1000000: episode: 2163, duration: 2.795s, episode steps: 233, steps per second:  83, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.472 [0.000, 3.000],  loss: 0.001382, mae: 0.780718, mean_q: 1.051060, mean_eps: 0.561554\n",
      " 487622/1000000: episode: 2164, duration: 4.069s, episode steps: 342, steps per second:  84, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.635 [0.000, 3.000],  loss: 0.001465, mae: 0.772651, mean_q: 1.044023, mean_eps: 0.561295\n",
      " 488035/1000000: episode: 2165, duration: 4.947s, episode steps: 413, steps per second:  83, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.685 [0.000, 3.000],  loss: 0.001700, mae: 0.772815, mean_q: 1.043077, mean_eps: 0.560955\n",
      " 488438/1000000: episode: 2166, duration: 4.977s, episode steps: 403, steps per second:  81, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.784 [0.000, 3.000],  loss: 0.001513, mae: 0.775990, mean_q: 1.046148, mean_eps: 0.560588\n",
      " 488584/1000000: episode: 2167, duration: 1.755s, episode steps: 146, steps per second:  83, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.452 [0.000, 3.000],  loss: 0.001899, mae: 0.773850, mean_q: 1.043402, mean_eps: 0.560341\n",
      " 488750/1000000: episode: 2168, duration: 2.005s, episode steps: 166, steps per second:  83, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.542 [0.000, 3.000],  loss: 0.001165, mae: 0.776975, mean_q: 1.047565, mean_eps: 0.560201\n",
      " 489080/1000000: episode: 2169, duration: 4.046s, episode steps: 330, steps per second:  82, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.661 [0.000, 3.000],  loss: 0.001233, mae: 0.778321, mean_q: 1.049511, mean_eps: 0.559977\n",
      " 489478/1000000: episode: 2170, duration: 4.950s, episode steps: 398, steps per second:  80, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.706 [0.000, 3.000],  loss: 0.001310, mae: 0.778708, mean_q: 1.049324, mean_eps: 0.559650\n",
      " 489792/1000000: episode: 2171, duration: 3.935s, episode steps: 314, steps per second:  80, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.001463, mae: 0.774246, mean_q: 1.043585, mean_eps: 0.559329\n",
      " 490102/1000000: episode: 2172, duration: 3.735s, episode steps: 310, steps per second:  83, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 0.001368, mae: 0.781356, mean_q: 1.053264, mean_eps: 0.559049\n",
      " 490369/1000000: episode: 2173, duration: 3.180s, episode steps: 267, steps per second:  84, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.809 [0.000, 3.000],  loss: 0.002091, mae: 0.791414, mean_q: 1.065035, mean_eps: 0.558788\n",
      " 490677/1000000: episode: 2174, duration: 3.587s, episode steps: 308, steps per second:  86, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.503 [0.000, 3.000],  loss: 0.001391, mae: 0.798073, mean_q: 1.074389, mean_eps: 0.558528\n",
      " 490881/1000000: episode: 2175, duration: 2.567s, episode steps: 204, steps per second:  79, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: 0.001569, mae: 0.793637, mean_q: 1.069800, mean_eps: 0.558298\n",
      " 491059/1000000: episode: 2176, duration: 2.207s, episode steps: 178, steps per second:  81, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.669 [0.000, 3.000],  loss: 0.001004, mae: 0.797388, mean_q: 1.073371, mean_eps: 0.558127\n",
      " 491337/1000000: episode: 2177, duration: 3.376s, episode steps: 278, steps per second:  82, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.604 [0.000, 3.000],  loss: 0.001522, mae: 0.798561, mean_q: 1.075414, mean_eps: 0.557922\n",
      " 491678/1000000: episode: 2178, duration: 4.089s, episode steps: 341, steps per second:  83, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.651 [0.000, 3.000],  loss: 0.001378, mae: 0.791297, mean_q: 1.065533, mean_eps: 0.557643\n",
      " 492059/1000000: episode: 2179, duration: 5.085s, episode steps: 381, steps per second:  75, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.001443, mae: 0.796432, mean_q: 1.072859, mean_eps: 0.557319\n",
      " 492435/1000000: episode: 2180, duration: 4.612s, episode steps: 376, steps per second:  82, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.001464, mae: 0.795833, mean_q: 1.071231, mean_eps: 0.556979\n",
      " 492686/1000000: episode: 2181, duration: 2.999s, episode steps: 251, steps per second:  84, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.821 [0.000, 3.000],  loss: 0.001403, mae: 0.786170, mean_q: 1.059568, mean_eps: 0.556696\n",
      " 493095/1000000: episode: 2182, duration: 5.018s, episode steps: 409, steps per second:  82, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.638 [0.000, 3.000],  loss: 0.001476, mae: 0.798565, mean_q: 1.074429, mean_eps: 0.556399\n",
      " 493378/1000000: episode: 2183, duration: 3.432s, episode steps: 283, steps per second:  82, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.777 [0.000, 3.000],  loss: 0.001590, mae: 0.794550, mean_q: 1.070134, mean_eps: 0.556088\n",
      " 493559/1000000: episode: 2184, duration: 2.294s, episode steps: 181, steps per second:  79, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.652 [0.000, 3.000],  loss: 0.001540, mae: 0.793897, mean_q: 1.069606, mean_eps: 0.555879\n",
      " 493960/1000000: episode: 2185, duration: 4.877s, episode steps: 401, steps per second:  82, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.594 [0.000, 3.000],  loss: 0.001314, mae: 0.788422, mean_q: 1.061632, mean_eps: 0.555618\n",
      " 494472/1000000: episode: 2186, duration: 6.247s, episode steps: 512, steps per second:  82, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.660 [0.000, 3.000],  loss: 0.001357, mae: 0.794642, mean_q: 1.069079, mean_eps: 0.555207\n",
      " 494721/1000000: episode: 2187, duration: 3.184s, episode steps: 249, steps per second:  78, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.691 [0.000, 3.000],  loss: 0.001297, mae: 0.799556, mean_q: 1.078845, mean_eps: 0.554864\n",
      " 495034/1000000: episode: 2188, duration: 3.915s, episode steps: 313, steps per second:  80, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.735 [0.000, 3.000],  loss: 0.001526, mae: 0.795362, mean_q: 1.071128, mean_eps: 0.554610\n",
      " 495529/1000000: episode: 2189, duration: 5.962s, episode steps: 495, steps per second:  83, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.766 [0.000, 3.000],  loss: 0.001031, mae: 0.796762, mean_q: 1.072701, mean_eps: 0.554246\n",
      " 495963/1000000: episode: 2190, duration: 5.237s, episode steps: 434, steps per second:  83, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.641 [0.000, 3.000],  loss: 0.001278, mae: 0.794915, mean_q: 1.069810, mean_eps: 0.553829\n",
      " 496250/1000000: episode: 2191, duration: 3.579s, episode steps: 287, steps per second:  80, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.728 [0.000, 3.000],  loss: 0.001418, mae: 0.790762, mean_q: 1.065058, mean_eps: 0.553505\n",
      " 496448/1000000: episode: 2192, duration: 2.469s, episode steps: 198, steps per second:  80, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.001152, mae: 0.789873, mean_q: 1.064893, mean_eps: 0.553287\n",
      " 496646/1000000: episode: 2193, duration: 2.447s, episode steps: 198, steps per second:  81, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.702 [0.000, 3.000],  loss: 0.001288, mae: 0.787105, mean_q: 1.061854, mean_eps: 0.553109\n",
      " 497097/1000000: episode: 2194, duration: 5.478s, episode steps: 451, steps per second:  82, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: 0.001335, mae: 0.799984, mean_q: 1.077944, mean_eps: 0.552815\n",
      " 497492/1000000: episode: 2195, duration: 4.859s, episode steps: 395, steps per second:  81, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.770 [0.000, 3.000],  loss: 0.001453, mae: 0.788038, mean_q: 1.063066, mean_eps: 0.552435\n",
      " 497704/1000000: episode: 2196, duration: 2.584s, episode steps: 212, steps per second:  82, episode reward:  3.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.001546, mae: 0.793698, mean_q: 1.070280, mean_eps: 0.552164\n",
      " 498133/1000000: episode: 2197, duration: 5.074s, episode steps: 429, steps per second:  85, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.671 [0.000, 3.000],  loss: 0.001517, mae: 0.796641, mean_q: 1.074107, mean_eps: 0.551874\n",
      " 498531/1000000: episode: 2198, duration: 4.701s, episode steps: 398, steps per second:  85, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.663 [0.000, 3.000],  loss: 0.001349, mae: 0.791014, mean_q: 1.065397, mean_eps: 0.551501\n",
      " 498878/1000000: episode: 2199, duration: 4.397s, episode steps: 347, steps per second:  79, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.001636, mae: 0.792917, mean_q: 1.068021, mean_eps: 0.551166\n",
      " 499280/1000000: episode: 2200, duration: 4.861s, episode steps: 402, steps per second:  83, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.612 [0.000, 3.000],  loss: 0.001350, mae: 0.793026, mean_q: 1.068976, mean_eps: 0.550830\n",
      " 499652/1000000: episode: 2201, duration: 4.514s, episode steps: 372, steps per second:  82, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.656 [0.000, 3.000],  loss: 0.001363, mae: 0.792838, mean_q: 1.068840, mean_eps: 0.550482\n",
      " 499971/1000000: episode: 2202, duration: 3.877s, episode steps: 319, steps per second:  82, episode reward:  6.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.627 [0.000, 3.000],  loss: 0.001482, mae: 0.795134, mean_q: 1.070320, mean_eps: 0.550171\n",
      " 500304/1000000: episode: 2203, duration: 4.194s, episode steps: 333, steps per second:  79, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.514 [0.000, 3.000],  loss: 0.002008, mae: 0.807004, mean_q: 1.084615, mean_eps: 0.549878\n",
      " 500654/1000000: episode: 2204, duration: 4.285s, episode steps: 350, steps per second:  82, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.543 [0.000, 3.000],  loss: 0.001485, mae: 0.802594, mean_q: 1.079209, mean_eps: 0.549570\n",
      " 500836/1000000: episode: 2205, duration: 2.210s, episode steps: 182, steps per second:  82, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.786 [0.000, 3.000],  loss: 0.001304, mae: 0.795588, mean_q: 1.071063, mean_eps: 0.549330\n",
      " 501171/1000000: episode: 2206, duration: 4.104s, episode steps: 335, steps per second:  82, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.746 [0.000, 3.000],  loss: 0.001294, mae: 0.803207, mean_q: 1.080286, mean_eps: 0.549098\n",
      " 501475/1000000: episode: 2207, duration: 3.801s, episode steps: 304, steps per second:  80, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.727 [0.000, 3.000],  loss: 0.001268, mae: 0.804356, mean_q: 1.081526, mean_eps: 0.548810\n",
      " 501776/1000000: episode: 2208, duration: 3.659s, episode steps: 301, steps per second:  82, episode reward:  5.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.545 [0.000, 3.000],  loss: 0.001369, mae: 0.800153, mean_q: 1.077121, mean_eps: 0.548538\n",
      " 502103/1000000: episode: 2209, duration: 3.955s, episode steps: 327, steps per second:  83, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.746 [0.000, 3.000],  loss: 0.001391, mae: 0.800379, mean_q: 1.076657, mean_eps: 0.548256\n",
      " 502431/1000000: episode: 2210, duration: 3.943s, episode steps: 328, steps per second:  83, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 0.001348, mae: 0.800953, mean_q: 1.078117, mean_eps: 0.547961\n",
      " 502832/1000000: episode: 2211, duration: 5.091s, episode steps: 401, steps per second:  79, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.001536, mae: 0.804701, mean_q: 1.081991, mean_eps: 0.547633\n",
      " 503243/1000000: episode: 2212, duration: 4.964s, episode steps: 411, steps per second:  83, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.001058, mae: 0.811603, mean_q: 1.092014, mean_eps: 0.547268\n",
      " 503553/1000000: episode: 2213, duration: 3.769s, episode steps: 310, steps per second:  82, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.552 [0.000, 3.000],  loss: 0.001480, mae: 0.801717, mean_q: 1.078406, mean_eps: 0.546942\n",
      " 503923/1000000: episode: 2214, duration: 4.552s, episode steps: 370, steps per second:  81, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.697 [0.000, 3.000],  loss: 0.001131, mae: 0.806199, mean_q: 1.085518, mean_eps: 0.546636\n",
      " 504256/1000000: episode: 2215, duration: 4.246s, episode steps: 333, steps per second:  78, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.727 [0.000, 3.000],  loss: 0.001320, mae: 0.810869, mean_q: 1.090449, mean_eps: 0.546321\n",
      " 504528/1000000: episode: 2216, duration: 3.304s, episode steps: 272, steps per second:  82, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.555 [0.000, 3.000],  loss: 0.001251, mae: 0.800227, mean_q: 1.077229, mean_eps: 0.546049\n",
      " 504790/1000000: episode: 2217, duration: 3.190s, episode steps: 262, steps per second:  82, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.001135, mae: 0.808774, mean_q: 1.089829, mean_eps: 0.545808\n",
      " 505051/1000000: episode: 2218, duration: 3.166s, episode steps: 261, steps per second:  82, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.000936, mae: 0.804363, mean_q: 1.083195, mean_eps: 0.545572\n",
      " 505502/1000000: episode: 2219, duration: 5.628s, episode steps: 451, steps per second:  80, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.665 [0.000, 3.000],  loss: 0.001265, mae: 0.807421, mean_q: 1.087438, mean_eps: 0.545252\n",
      " 505854/1000000: episode: 2220, duration: 4.187s, episode steps: 352, steps per second:  84, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.520 [0.000, 3.000],  loss: 0.001475, mae: 0.800675, mean_q: 1.077494, mean_eps: 0.544890\n",
      " 506227/1000000: episode: 2221, duration: 4.475s, episode steps: 373, steps per second:  83, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.676 [0.000, 3.000],  loss: 0.001147, mae: 0.805553, mean_q: 1.084495, mean_eps: 0.544564\n",
      " 506672/1000000: episode: 2222, duration: 5.610s, episode steps: 445, steps per second:  79, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.631 [0.000, 3.000],  loss: 0.001291, mae: 0.808636, mean_q: 1.088837, mean_eps: 0.544197\n",
      " 506968/1000000: episode: 2223, duration: 3.554s, episode steps: 296, steps per second:  83, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 0.001370, mae: 0.803095, mean_q: 1.080638, mean_eps: 0.543864\n",
      " 507295/1000000: episode: 2224, duration: 3.948s, episode steps: 327, steps per second:  83, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.636 [0.000, 3.000],  loss: 0.001811, mae: 0.801624, mean_q: 1.079405, mean_eps: 0.543583\n",
      " 507529/1000000: episode: 2225, duration: 2.861s, episode steps: 234, steps per second:  82, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 0.001063, mae: 0.801586, mean_q: 1.078429, mean_eps: 0.543329\n",
      " 507818/1000000: episode: 2226, duration: 3.559s, episode steps: 289, steps per second:  81, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.671 [0.000, 3.000],  loss: 0.001362, mae: 0.804923, mean_q: 1.083200, mean_eps: 0.543093\n",
      " 508330/1000000: episode: 2227, duration: 6.306s, episode steps: 512, steps per second:  81, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.777 [0.000, 3.000],  loss: 0.001637, mae: 0.798969, mean_q: 1.073606, mean_eps: 0.542733\n",
      " 508484/1000000: episode: 2228, duration: 1.874s, episode steps: 154, steps per second:  82, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 0.000988, mae: 0.809568, mean_q: 1.089946, mean_eps: 0.542435\n",
      " 508860/1000000: episode: 2229, duration: 4.614s, episode steps: 376, steps per second:  81, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.902 [0.000, 3.000],  loss: 0.000910, mae: 0.811220, mean_q: 1.091501, mean_eps: 0.542197\n",
      " 509198/1000000: episode: 2230, duration: 4.278s, episode steps: 338, steps per second:  79, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.701 [0.000, 3.000],  loss: 0.001123, mae: 0.805559, mean_q: 1.083575, mean_eps: 0.541875\n",
      " 509441/1000000: episode: 2231, duration: 2.965s, episode steps: 243, steps per second:  82, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001054, mae: 0.805744, mean_q: 1.084290, mean_eps: 0.541612\n",
      " 509807/1000000: episode: 2232, duration: 4.419s, episode steps: 366, steps per second:  83, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.749 [0.000, 3.000],  loss: 0.001466, mae: 0.805667, mean_q: 1.085441, mean_eps: 0.541338\n",
      " 510151/1000000: episode: 2233, duration: 4.109s, episode steps: 344, steps per second:  84, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.773 [0.000, 3.000],  loss: 0.001596, mae: 0.799084, mean_q: 1.074898, mean_eps: 0.541020\n",
      " 510393/1000000: episode: 2234, duration: 3.003s, episode steps: 242, steps per second:  81, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.426 [0.000, 3.000],  loss: 0.001683, mae: 0.805203, mean_q: 1.081403, mean_eps: 0.540755\n",
      " 510818/1000000: episode: 2235, duration: 5.333s, episode steps: 425, steps per second:  80, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.666 [0.000, 3.000],  loss: 0.001489, mae: 0.804050, mean_q: 1.083172, mean_eps: 0.540455\n",
      " 510946/1000000: episode: 2236, duration: 1.585s, episode steps: 128, steps per second:  81, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.797 [0.000, 3.000],  loss: 0.001074, mae: 0.809511, mean_q: 1.089477, mean_eps: 0.540206\n",
      " 511283/1000000: episode: 2237, duration: 4.074s, episode steps: 337, steps per second:  83, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.899 [0.000, 3.000],  loss: 0.001099, mae: 0.810643, mean_q: 1.090648, mean_eps: 0.539997\n",
      " 511696/1000000: episode: 2238, duration: 5.103s, episode steps: 413, steps per second:  81, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.001091, mae: 0.801018, mean_q: 1.077741, mean_eps: 0.539661\n",
      " 512013/1000000: episode: 2239, duration: 3.943s, episode steps: 317, steps per second:  80, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 0.001109, mae: 0.804802, mean_q: 1.083217, mean_eps: 0.539331\n",
      " 512435/1000000: episode: 2240, duration: 5.106s, episode steps: 422, steps per second:  83, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.749 [0.000, 3.000],  loss: 0.001422, mae: 0.812086, mean_q: 1.095002, mean_eps: 0.538998\n",
      " 512762/1000000: episode: 2241, duration: 4.009s, episode steps: 327, steps per second:  82, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.654 [0.000, 3.000],  loss: 0.001576, mae: 0.809449, mean_q: 1.089530, mean_eps: 0.538662\n",
      " 513083/1000000: episode: 2242, duration: 4.074s, episode steps: 321, steps per second:  79, episode reward:  6.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.001295, mae: 0.809445, mean_q: 1.089781, mean_eps: 0.538370\n",
      " 513546/1000000: episode: 2243, duration: 5.612s, episode steps: 463, steps per second:  83, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.001257, mae: 0.800502, mean_q: 1.079272, mean_eps: 0.538017\n",
      " 514028/1000000: episode: 2244, duration: 5.810s, episode steps: 482, steps per second:  83, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.695 [0.000, 3.000],  loss: 0.001130, mae: 0.806181, mean_q: 1.083193, mean_eps: 0.537593\n",
      " 514338/1000000: episode: 2245, duration: 3.962s, episode steps: 310, steps per second:  78, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.661 [0.000, 3.000],  loss: 0.001449, mae: 0.798516, mean_q: 1.073975, mean_eps: 0.537236\n",
      " 514598/1000000: episode: 2246, duration: 3.203s, episode steps: 260, steps per second:  81, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.681 [0.000, 3.000],  loss: 0.001186, mae: 0.809126, mean_q: 1.087946, mean_eps: 0.536979\n",
      " 514965/1000000: episode: 2247, duration: 4.453s, episode steps: 367, steps per second:  82, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.719 [0.000, 3.000],  loss: 0.001384, mae: 0.807556, mean_q: 1.086129, mean_eps: 0.536696\n",
      " 515522/1000000: episode: 2248, duration: 6.810s, episode steps: 557, steps per second:  82, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.657 [0.000, 3.000],  loss: 0.001181, mae: 0.810102, mean_q: 1.090734, mean_eps: 0.536280\n",
      " 515860/1000000: episode: 2249, duration: 4.269s, episode steps: 338, steps per second:  79, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.598 [0.000, 3.000],  loss: 0.001321, mae: 0.811341, mean_q: 1.091881, mean_eps: 0.535879\n",
      " 516093/1000000: episode: 2250, duration: 2.875s, episode steps: 233, steps per second:  81, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: 0.001237, mae: 0.800065, mean_q: 1.078032, mean_eps: 0.535622\n",
      " 516506/1000000: episode: 2251, duration: 5.101s, episode steps: 413, steps per second:  81, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.709 [0.000, 3.000],  loss: 0.001104, mae: 0.803476, mean_q: 1.082642, mean_eps: 0.535330\n",
      " 516924/1000000: episode: 2252, duration: 5.142s, episode steps: 418, steps per second:  81, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 0.001150, mae: 0.805247, mean_q: 1.084413, mean_eps: 0.534957\n",
      " 517376/1000000: episode: 2253, duration: 5.579s, episode steps: 452, steps per second:  81, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.728 [0.000, 3.000],  loss: 0.001029, mae: 0.806595, mean_q: 1.086590, mean_eps: 0.534567\n",
      " 517883/1000000: episode: 2254, duration: 6.155s, episode steps: 507, steps per second:  82, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.716 [0.000, 3.000],  loss: 0.001093, mae: 0.804997, mean_q: 1.084910, mean_eps: 0.534135\n",
      " 518306/1000000: episode: 2255, duration: 5.262s, episode steps: 423, steps per second:  80, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.730 [0.000, 3.000],  loss: 0.001047, mae: 0.804096, mean_q: 1.084461, mean_eps: 0.533715\n",
      " 518646/1000000: episode: 2256, duration: 4.156s, episode steps: 340, steps per second:  82, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.391 [0.000, 3.000],  loss: 0.001129, mae: 0.806095, mean_q: 1.086477, mean_eps: 0.533372\n",
      " 518931/1000000: episode: 2257, duration: 3.420s, episode steps: 285, steps per second:  83, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.800 [0.000, 3.000],  loss: 0.001109, mae: 0.809568, mean_q: 1.091521, mean_eps: 0.533091\n",
      " 519290/1000000: episode: 2258, duration: 4.334s, episode steps: 359, steps per second:  83, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.671 [0.000, 3.000],  loss: 0.001247, mae: 0.810135, mean_q: 1.090977, mean_eps: 0.532801\n",
      " 519832/1000000: episode: 2259, duration: 6.763s, episode steps: 542, steps per second:  80, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.001173, mae: 0.807344, mean_q: 1.087514, mean_eps: 0.532396\n",
      " 520314/1000000: episode: 2260, duration: 5.899s, episode steps: 482, steps per second:  82, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.562 [0.000, 3.000],  loss: 0.001533, mae: 0.811084, mean_q: 1.090507, mean_eps: 0.531935\n",
      " 520797/1000000: episode: 2261, duration: 5.910s, episode steps: 483, steps per second:  82, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.706 [0.000, 3.000],  loss: 0.001221, mae: 0.822990, mean_q: 1.106089, mean_eps: 0.531500\n",
      " 521074/1000000: episode: 2262, duration: 3.462s, episode steps: 277, steps per second:  80, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.686 [0.000, 3.000],  loss: 0.001292, mae: 0.821284, mean_q: 1.105029, mean_eps: 0.531158\n",
      " 521378/1000000: episode: 2263, duration: 3.718s, episode steps: 304, steps per second:  82, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 0.000985, mae: 0.819056, mean_q: 1.101666, mean_eps: 0.530897\n",
      " 521954/1000000: episode: 2264, duration: 6.988s, episode steps: 576, steps per second:  82, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.001222, mae: 0.818844, mean_q: 1.102008, mean_eps: 0.530501\n",
      " 522208/1000000: episode: 2265, duration: 3.253s, episode steps: 254, steps per second:  78, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.673 [0.000, 3.000],  loss: 0.001223, mae: 0.810980, mean_q: 1.091492, mean_eps: 0.530128\n",
      " 522739/1000000: episode: 2266, duration: 6.496s, episode steps: 531, steps per second:  82, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 0.001555, mae: 0.817449, mean_q: 1.100557, mean_eps: 0.529775\n",
      " 523036/1000000: episode: 2267, duration: 3.634s, episode steps: 297, steps per second:  82, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.613 [0.000, 3.000],  loss: 0.001060, mae: 0.822898, mean_q: 1.107464, mean_eps: 0.529403\n",
      " 523428/1000000: episode: 2268, duration: 4.895s, episode steps: 392, steps per second:  80, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.653 [0.000, 3.000],  loss: 0.001046, mae: 0.814748, mean_q: 1.096934, mean_eps: 0.529093\n",
      " 523719/1000000: episode: 2269, duration: 3.676s, episode steps: 291, steps per second:  79, episode reward:  5.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.718 [0.000, 3.000],  loss: 0.001236, mae: 0.823283, mean_q: 1.109464, mean_eps: 0.528785\n",
      " 523958/1000000: episode: 2270, duration: 2.968s, episode steps: 239, steps per second:  81, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.682 [0.000, 3.000],  loss: 0.001272, mae: 0.817923, mean_q: 1.100066, mean_eps: 0.528546\n",
      " 524353/1000000: episode: 2271, duration: 4.787s, episode steps: 395, steps per second:  83, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.686 [0.000, 3.000],  loss: 0.001239, mae: 0.813289, mean_q: 1.094276, mean_eps: 0.528260\n",
      " 524761/1000000: episode: 2272, duration: 5.129s, episode steps: 408, steps per second:  80, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 0.001358, mae: 0.814204, mean_q: 1.095512, mean_eps: 0.527898\n",
      " 524977/1000000: episode: 2273, duration: 2.692s, episode steps: 216, steps per second:  80, episode reward:  3.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.542 [0.000, 3.000],  loss: 0.001209, mae: 0.818511, mean_q: 1.101003, mean_eps: 0.527617\n",
      " 525467/1000000: episode: 2274, duration: 5.993s, episode steps: 490, steps per second:  82, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.433 [0.000, 3.000],  loss: 0.001321, mae: 0.821578, mean_q: 1.104560, mean_eps: 0.527300\n",
      " 525656/1000000: episode: 2275, duration: 2.352s, episode steps: 189, steps per second:  80, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.661 [0.000, 3.000],  loss: 0.001029, mae: 0.815679, mean_q: 1.097764, mean_eps: 0.526996\n",
      " 526100/1000000: episode: 2276, duration: 5.622s, episode steps: 444, steps per second:  79, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.689 [0.000, 3.000],  loss: 0.001213, mae: 0.810831, mean_q: 1.091828, mean_eps: 0.526712\n",
      " 526538/1000000: episode: 2277, duration: 5.390s, episode steps: 438, steps per second:  81, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.001106, mae: 0.822681, mean_q: 1.107939, mean_eps: 0.526314\n",
      " 526749/1000000: episode: 2278, duration: 2.595s, episode steps: 211, steps per second:  81, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.659 [0.000, 3.000],  loss: 0.001441, mae: 0.814699, mean_q: 1.097596, mean_eps: 0.526020\n",
      " 527240/1000000: episode: 2279, duration: 6.002s, episode steps: 491, steps per second:  82, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.825 [0.000, 3.000],  loss: 0.000883, mae: 0.812044, mean_q: 1.092953, mean_eps: 0.525705\n",
      " 527599/1000000: episode: 2280, duration: 4.800s, episode steps: 359, steps per second:  75, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.632 [0.000, 3.000],  loss: 0.001169, mae: 0.812257, mean_q: 1.091507, mean_eps: 0.525324\n",
      " 527946/1000000: episode: 2281, duration: 4.497s, episode steps: 347, steps per second:  77, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.001093, mae: 0.822706, mean_q: 1.106368, mean_eps: 0.525005\n",
      " 528145/1000000: episode: 2282, duration: 2.738s, episode steps: 199, steps per second:  73, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.603 [0.000, 3.000],  loss: 0.001326, mae: 0.811407, mean_q: 1.092810, mean_eps: 0.524759\n",
      " 528668/1000000: episode: 2283, duration: 6.594s, episode steps: 523, steps per second:  79, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.679 [0.000, 3.000],  loss: 0.000997, mae: 0.816751, mean_q: 1.099321, mean_eps: 0.524435\n",
      " 529079/1000000: episode: 2284, duration: 5.199s, episode steps: 411, steps per second:  79, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.749 [0.000, 3.000],  loss: 0.001167, mae: 0.816550, mean_q: 1.098973, mean_eps: 0.524015\n",
      " 529399/1000000: episode: 2285, duration: 3.963s, episode steps: 320, steps per second:  81, episode reward:  6.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.822 [0.000, 3.000],  loss: 0.001133, mae: 0.816049, mean_q: 1.099339, mean_eps: 0.523686\n",
      " 529765/1000000: episode: 2286, duration: 4.646s, episode steps: 366, steps per second:  79, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.000927, mae: 0.817816, mean_q: 1.100768, mean_eps: 0.523376\n",
      " 529997/1000000: episode: 2287, duration: 2.998s, episode steps: 232, steps per second:  77, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.444 [0.000, 3.000],  loss: 0.001052, mae: 0.817036, mean_q: 1.099314, mean_eps: 0.523106\n",
      " 530419/1000000: episode: 2288, duration: 5.208s, episode steps: 422, steps per second:  81, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.001428, mae: 0.832870, mean_q: 1.119875, mean_eps: 0.522813\n",
      " 530997/1000000: episode: 2289, duration: 7.208s, episode steps: 578, steps per second:  80, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.815 [0.000, 3.000],  loss: 0.001153, mae: 0.830946, mean_q: 1.117550, mean_eps: 0.522363\n",
      " 531302/1000000: episode: 2290, duration: 3.827s, episode steps: 305, steps per second:  80, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: 0.001541, mae: 0.824187, mean_q: 1.108846, mean_eps: 0.521965\n",
      " 531634/1000000: episode: 2291, duration: 3.988s, episode steps: 332, steps per second:  83, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.660 [0.000, 3.000],  loss: 0.001362, mae: 0.826525, mean_q: 1.111284, mean_eps: 0.521679\n",
      " 531972/1000000: episode: 2292, duration: 4.095s, episode steps: 338, steps per second:  83, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 0.001335, mae: 0.826872, mean_q: 1.111775, mean_eps: 0.521378\n",
      " 532243/1000000: episode: 2293, duration: 3.358s, episode steps: 271, steps per second:  81, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.782 [0.000, 3.000],  loss: 0.001133, mae: 0.832370, mean_q: 1.120581, mean_eps: 0.521105\n",
      " 532580/1000000: episode: 2294, duration: 4.258s, episode steps: 337, steps per second:  79, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.674 [0.000, 3.000],  loss: 0.001667, mae: 0.830283, mean_q: 1.116246, mean_eps: 0.520831\n",
      " 532890/1000000: episode: 2295, duration: 3.790s, episode steps: 310, steps per second:  82, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.710 [0.000, 3.000],  loss: 0.001268, mae: 0.828560, mean_q: 1.114381, mean_eps: 0.520539\n",
      " 533185/1000000: episode: 2296, duration: 3.670s, episode steps: 295, steps per second:  80, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.756 [0.000, 3.000],  loss: 0.001218, mae: 0.823421, mean_q: 1.107153, mean_eps: 0.520266\n",
      " 533640/1000000: episode: 2297, duration: 5.724s, episode steps: 455, steps per second:  79, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.001199, mae: 0.822589, mean_q: 1.106118, mean_eps: 0.519929\n",
      " 534248/1000000: episode: 2298, duration: 8.240s, episode steps: 608, steps per second:  74, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.714 [0.000, 3.000],  loss: 0.001170, mae: 0.824917, mean_q: 1.108500, mean_eps: 0.519452\n",
      " 534602/1000000: episode: 2299, duration: 4.440s, episode steps: 354, steps per second:  80, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: 0.001014, mae: 0.821711, mean_q: 1.105664, mean_eps: 0.519018\n",
      " 534862/1000000: episode: 2300, duration: 3.307s, episode steps: 260, steps per second:  79, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.704 [0.000, 3.000],  loss: 0.001447, mae: 0.828666, mean_q: 1.113747, mean_eps: 0.518741\n",
      " 535268/1000000: episode: 2301, duration: 5.158s, episode steps: 406, steps per second:  79, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.611 [0.000, 3.000],  loss: 0.001121, mae: 0.829345, mean_q: 1.115216, mean_eps: 0.518442\n",
      " 535569/1000000: episode: 2302, duration: 3.791s, episode steps: 301, steps per second:  79, episode reward:  5.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.804 [0.000, 3.000],  loss: 0.001103, mae: 0.816116, mean_q: 1.098553, mean_eps: 0.518124\n",
      " 535851/1000000: episode: 2303, duration: 3.487s, episode steps: 282, steps per second:  81, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.699 [0.000, 3.000],  loss: 0.001230, mae: 0.838601, mean_q: 1.128823, mean_eps: 0.517861\n",
      " 536596/1000000: episode: 2304, duration: 9.210s, episode steps: 745, steps per second:  81, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.774 [0.000, 3.000],  loss: 0.001223, mae: 0.828852, mean_q: 1.115755, mean_eps: 0.517400\n",
      " 536932/1000000: episode: 2305, duration: 4.225s, episode steps: 336, steps per second:  80, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001385, mae: 0.835082, mean_q: 1.123566, mean_eps: 0.516914\n",
      " 537393/1000000: episode: 2306, duration: 5.695s, episode steps: 461, steps per second:  81, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.001325, mae: 0.829050, mean_q: 1.116353, mean_eps: 0.516554\n",
      " 537819/1000000: episode: 2307, duration: 5.533s, episode steps: 426, steps per second:  77, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.676 [0.000, 3.000],  loss: 0.001170, mae: 0.821764, mean_q: 1.105152, mean_eps: 0.516155\n",
      " 538532/1000000: episode: 2308, duration: 8.922s, episode steps: 713, steps per second:  80, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.620 [0.000, 3.000],  loss: 0.001241, mae: 0.830398, mean_q: 1.117863, mean_eps: 0.515643\n",
      " 538785/1000000: episode: 2309, duration: 3.404s, episode steps: 253, steps per second:  74, episode reward:  4.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.664 [0.000, 3.000],  loss: 0.001199, mae: 0.841959, mean_q: 1.133209, mean_eps: 0.515208\n",
      " 539193/1000000: episode: 2310, duration: 5.040s, episode steps: 408, steps per second:  81, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 0.001369, mae: 0.829514, mean_q: 1.118014, mean_eps: 0.514909\n",
      " 539833/1000000: episode: 2311, duration: 7.896s, episode steps: 640, steps per second:  81, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.636 [0.000, 3.000],  loss: 0.001024, mae: 0.823217, mean_q: 1.108204, mean_eps: 0.514437\n",
      " 540170/1000000: episode: 2312, duration: 4.362s, episode steps: 337, steps per second:  77, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 0.001239, mae: 0.828410, mean_q: 1.113936, mean_eps: 0.513998\n",
      " 540895/1000000: episode: 2313, duration: 9.430s, episode steps: 725, steps per second:  77, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.659 [0.000, 3.000],  loss: 0.001328, mae: 0.843037, mean_q: 1.132544, mean_eps: 0.513521\n",
      " 541343/1000000: episode: 2314, duration: 5.711s, episode steps: 448, steps per second:  78, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.656 [0.000, 3.000],  loss: 0.001146, mae: 0.839407, mean_q: 1.128465, mean_eps: 0.512994\n",
      " 541842/1000000: episode: 2315, duration: 6.210s, episode steps: 499, steps per second:  80, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.475 [0.000, 3.000],  loss: 0.001106, mae: 0.834386, mean_q: 1.122559, mean_eps: 0.512567\n",
      " 542189/1000000: episode: 2316, duration: 4.379s, episode steps: 347, steps per second:  79, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.669 [0.000, 3.000],  loss: 0.001619, mae: 0.826305, mean_q: 1.112546, mean_eps: 0.512186\n",
      " 542539/1000000: episode: 2317, duration: 4.553s, episode steps: 350, steps per second:  77, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.680 [0.000, 3.000],  loss: 0.001278, mae: 0.832098, mean_q: 1.121162, mean_eps: 0.511872\n",
      " 542843/1000000: episode: 2318, duration: 3.776s, episode steps: 304, steps per second:  81, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.806 [0.000, 3.000],  loss: 0.001200, mae: 0.841074, mean_q: 1.130890, mean_eps: 0.511579\n",
      " 543231/1000000: episode: 2319, duration: 4.856s, episode steps: 388, steps per second:  80, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.376 [0.000, 3.000],  loss: 0.001234, mae: 0.839781, mean_q: 1.127814, mean_eps: 0.511268\n",
      " 543498/1000000: episode: 2320, duration: 3.416s, episode steps: 267, steps per second:  78, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.768 [0.000, 3.000],  loss: 0.001375, mae: 0.830225, mean_q: 1.117095, mean_eps: 0.510972\n",
      " 543720/1000000: episode: 2321, duration: 2.864s, episode steps: 222, steps per second:  78, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: 0.001397, mae: 0.838924, mean_q: 1.126859, mean_eps: 0.510753\n",
      " 544156/1000000: episode: 2322, duration: 5.591s, episode steps: 436, steps per second:  78, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.711 [0.000, 3.000],  loss: 0.000917, mae: 0.842537, mean_q: 1.133109, mean_eps: 0.510458\n",
      " 544407/1000000: episode: 2323, duration: 3.231s, episode steps: 251, steps per second:  78, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 0.001399, mae: 0.836834, mean_q: 1.124645, mean_eps: 0.510148\n",
      " 544768/1000000: episode: 2324, duration: 4.476s, episode steps: 361, steps per second:  81, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.360 [0.000, 3.000],  loss: 0.001174, mae: 0.841678, mean_q: 1.131703, mean_eps: 0.509873\n",
      " 545208/1000000: episode: 2325, duration: 5.595s, episode steps: 440, steps per second:  79, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.714 [0.000, 3.000],  loss: 0.000920, mae: 0.828385, mean_q: 1.114451, mean_eps: 0.509513\n",
      " 545427/1000000: episode: 2326, duration: 2.795s, episode steps: 219, steps per second:  78, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.694 [0.000, 3.000],  loss: 0.001391, mae: 0.834899, mean_q: 1.123303, mean_eps: 0.509216\n",
      " 545723/1000000: episode: 2327, duration: 3.676s, episode steps: 296, steps per second:  81, episode reward:  5.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.385 [0.000, 3.000],  loss: 0.000944, mae: 0.840577, mean_q: 1.131061, mean_eps: 0.508983\n",
      " 546189/1000000: episode: 2328, duration: 5.722s, episode steps: 466, steps per second:  81, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.712 [0.000, 3.000],  loss: 0.001216, mae: 0.838174, mean_q: 1.127653, mean_eps: 0.508640\n",
      " 546460/1000000: episode: 2329, duration: 3.482s, episode steps: 271, steps per second:  78, episode reward:  3.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: 0.001089, mae: 0.839797, mean_q: 1.128727, mean_eps: 0.508308\n",
      " 546850/1000000: episode: 2330, duration: 4.838s, episode steps: 390, steps per second:  81, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.001020, mae: 0.830898, mean_q: 1.116681, mean_eps: 0.508011\n",
      " 547020/1000000: episode: 2331, duration: 2.151s, episode steps: 170, steps per second:  79, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.388 [0.000, 3.000],  loss: 0.001184, mae: 0.833677, mean_q: 1.122577, mean_eps: 0.507759\n",
      " 547407/1000000: episode: 2332, duration: 4.883s, episode steps: 387, steps per second:  79, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.419 [0.000, 3.000],  loss: 0.001269, mae: 0.840235, mean_q: 1.129327, mean_eps: 0.507509\n",
      " 547802/1000000: episode: 2333, duration: 5.001s, episode steps: 395, steps per second:  79, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.001006, mae: 0.832857, mean_q: 1.120950, mean_eps: 0.507156\n",
      " 548394/1000000: episode: 2334, duration: 7.294s, episode steps: 592, steps per second:  81, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.657 [0.000, 3.000],  loss: 0.001426, mae: 0.834829, mean_q: 1.123276, mean_eps: 0.506712\n",
      " 548718/1000000: episode: 2335, duration: 4.054s, episode steps: 324, steps per second:  80, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.423 [0.000, 3.000],  loss: 0.001148, mae: 0.839001, mean_q: 1.128598, mean_eps: 0.506300\n",
      " 549102/1000000: episode: 2336, duration: 4.950s, episode steps: 384, steps per second:  78, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.612 [0.000, 3.000],  loss: 0.001110, mae: 0.834751, mean_q: 1.123861, mean_eps: 0.505981\n",
      " 549540/1000000: episode: 2337, duration: 5.479s, episode steps: 438, steps per second:  80, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: 0.001018, mae: 0.833432, mean_q: 1.121784, mean_eps: 0.505612\n",
      " 549712/1000000: episode: 2338, duration: 2.292s, episode steps: 172, steps per second:  75, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.692 [0.000, 3.000],  loss: 0.000911, mae: 0.844087, mean_q: 1.134744, mean_eps: 0.505338\n",
      " 549958/1000000: episode: 2339, duration: 3.188s, episode steps: 246, steps per second:  77, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.549 [0.000, 3.000],  loss: 0.001295, mae: 0.835536, mean_q: 1.125310, mean_eps: 0.505149\n",
      " 550271/1000000: episode: 2340, duration: 4.124s, episode steps: 313, steps per second:  76, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.789 [0.000, 3.000],  loss: 0.001425, mae: 0.843353, mean_q: 1.134009, mean_eps: 0.504897\n",
      " 550587/1000000: episode: 2341, duration: 3.962s, episode steps: 316, steps per second:  80, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.001363, mae: 0.846292, mean_q: 1.136448, mean_eps: 0.504615\n",
      " 550958/1000000: episode: 2342, duration: 4.629s, episode steps: 371, steps per second:  80, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.698 [0.000, 3.000],  loss: 0.001048, mae: 0.847444, mean_q: 1.138293, mean_eps: 0.504305\n",
      " 551361/1000000: episode: 2343, duration: 5.070s, episode steps: 403, steps per second:  79, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.001215, mae: 0.848047, mean_q: 1.139346, mean_eps: 0.503956\n",
      " 551832/1000000: episode: 2344, duration: 5.885s, episode steps: 471, steps per second:  80, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.728 [0.000, 3.000],  loss: 0.001268, mae: 0.847340, mean_q: 1.140572, mean_eps: 0.503564\n",
      " 552202/1000000: episode: 2345, duration: 4.619s, episode steps: 370, steps per second:  80, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.916 [0.000, 3.000],  loss: 0.001172, mae: 0.844557, mean_q: 1.136252, mean_eps: 0.503186\n",
      " 552604/1000000: episode: 2346, duration: 5.108s, episode steps: 402, steps per second:  79, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.001520, mae: 0.847375, mean_q: 1.138953, mean_eps: 0.502838\n",
      " 553011/1000000: episode: 2347, duration: 5.185s, episode steps: 407, steps per second:  78, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.732 [0.000, 3.000],  loss: 0.001682, mae: 0.847685, mean_q: 1.139441, mean_eps: 0.502475\n",
      " 553415/1000000: episode: 2348, duration: 5.077s, episode steps: 404, steps per second:  80, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.693 [0.000, 3.000],  loss: 0.001015, mae: 0.850611, mean_q: 1.143349, mean_eps: 0.502109\n",
      " 553754/1000000: episode: 2349, duration: 4.279s, episode steps: 339, steps per second:  79, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.855 [0.000, 3.000],  loss: 0.001350, mae: 0.847822, mean_q: 1.140027, mean_eps: 0.501774\n",
      " 554158/1000000: episode: 2350, duration: 5.166s, episode steps: 404, steps per second:  78, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.601 [0.000, 3.000],  loss: 0.001409, mae: 0.852395, mean_q: 1.146826, mean_eps: 0.501440\n",
      " 554600/1000000: episode: 2351, duration: 5.538s, episode steps: 442, steps per second:  80, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.629 [0.000, 3.000],  loss: 0.001180, mae: 0.852867, mean_q: 1.147039, mean_eps: 0.501060\n",
      " 554948/1000000: episode: 2352, duration: 4.423s, episode steps: 348, steps per second:  79, episode reward:  7.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.707 [0.000, 3.000],  loss: 0.001119, mae: 0.842749, mean_q: 1.133640, mean_eps: 0.500705\n",
      " 555204/1000000: episode: 2353, duration: 3.412s, episode steps: 256, steps per second:  75, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.727 [0.000, 3.000],  loss: 0.001131, mae: 0.843197, mean_q: 1.133459, mean_eps: 0.500433\n",
      " 555517/1000000: episode: 2354, duration: 3.867s, episode steps: 313, steps per second:  81, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.540 [0.000, 3.000],  loss: 0.001208, mae: 0.836693, mean_q: 1.124286, mean_eps: 0.500176\n",
      " 555963/1000000: episode: 2355, duration: 5.521s, episode steps: 446, steps per second:  81, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 0.001570, mae: 0.845465, mean_q: 1.135662, mean_eps: 0.499834\n",
      " 556454/1000000: episode: 2356, duration: 6.291s, episode steps: 491, steps per second:  78, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.001232, mae: 0.846796, mean_q: 1.137450, mean_eps: 0.499413\n",
      " 556933/1000000: episode: 2357, duration: 6.093s, episode steps: 479, steps per second:  79, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.029 [0.000, 3.000],  loss: 0.001260, mae: 0.844857, mean_q: 1.136115, mean_eps: 0.498975\n",
      " 557194/1000000: episode: 2358, duration: 3.256s, episode steps: 261, steps per second:  80, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.617 [0.000, 3.000],  loss: 0.001081, mae: 0.843813, mean_q: 1.134017, mean_eps: 0.498642\n",
      " 557470/1000000: episode: 2359, duration: 3.523s, episode steps: 276, steps per second:  78, episode reward:  5.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.819 [0.000, 3.000],  loss: 0.001291, mae: 0.857718, mean_q: 1.153089, mean_eps: 0.498401\n",
      " 557879/1000000: episode: 2360, duration: 5.330s, episode steps: 409, steps per second:  77, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: 0.001234, mae: 0.852134, mean_q: 1.144890, mean_eps: 0.498093\n",
      " 558212/1000000: episode: 2361, duration: 4.187s, episode steps: 333, steps per second:  80, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.718 [0.000, 3.000],  loss: 0.001565, mae: 0.850120, mean_q: 1.142626, mean_eps: 0.497760\n",
      " 558485/1000000: episode: 2362, duration: 3.483s, episode steps: 273, steps per second:  78, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.370 [0.000, 3.000],  loss: 0.001295, mae: 0.854060, mean_q: 1.148279, mean_eps: 0.497487\n",
      " 558852/1000000: episode: 2363, duration: 4.635s, episode steps: 367, steps per second:  79, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.741 [0.000, 3.000],  loss: 0.001147, mae: 0.852199, mean_q: 1.145453, mean_eps: 0.497199\n",
      " 559518/1000000: episode: 2364, duration: 8.527s, episode steps: 666, steps per second:  78, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: 0.001074, mae: 0.848723, mean_q: 1.140165, mean_eps: 0.496734\n",
      " 559754/1000000: episode: 2365, duration: 2.978s, episode steps: 236, steps per second:  79, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.814 [0.000, 3.000],  loss: 0.000883, mae: 0.849679, mean_q: 1.141947, mean_eps: 0.496328\n",
      " 560074/1000000: episode: 2366, duration: 4.006s, episode steps: 320, steps per second:  80, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.456 [0.000, 3.000],  loss: 0.001279, mae: 0.845605, mean_q: 1.134960, mean_eps: 0.496077\n",
      " 560639/1000000: episode: 2367, duration: 7.325s, episode steps: 565, steps per second:  77, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.789 [0.000, 3.000],  loss: 0.001557, mae: 0.857394, mean_q: 1.152347, mean_eps: 0.495680\n",
      " 560978/1000000: episode: 2368, duration: 4.272s, episode steps: 339, steps per second:  79, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 0.001388, mae: 0.858468, mean_q: 1.152961, mean_eps: 0.495273\n",
      " 561287/1000000: episode: 2369, duration: 3.914s, episode steps: 309, steps per second:  79, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.699 [0.000, 3.000],  loss: 0.001418, mae: 0.857427, mean_q: 1.151249, mean_eps: 0.494981\n",
      " 561915/1000000: episode: 2370, duration: 8.113s, episode steps: 628, steps per second:  77, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.001121, mae: 0.851307, mean_q: 1.143617, mean_eps: 0.494560\n",
      " 562503/1000000: episode: 2371, duration: 7.385s, episode steps: 588, steps per second:  80, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 0.001226, mae: 0.858867, mean_q: 1.154569, mean_eps: 0.494013\n",
      " 562980/1000000: episode: 2372, duration: 6.232s, episode steps: 477, steps per second:  77, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.696 [0.000, 3.000],  loss: 0.001376, mae: 0.853023, mean_q: 1.146925, mean_eps: 0.493534\n",
      " 563416/1000000: episode: 2373, duration: 5.519s, episode steps: 436, steps per second:  79, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.450 [0.000, 3.000],  loss: 0.001318, mae: 0.850021, mean_q: 1.143524, mean_eps: 0.493124\n",
      " 563979/1000000: episode: 2374, duration: 7.185s, episode steps: 563, steps per second:  78, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.872 [0.000, 3.000],  loss: 0.001187, mae: 0.854803, mean_q: 1.149921, mean_eps: 0.492674\n",
      " 564263/1000000: episode: 2375, duration: 3.650s, episode steps: 284, steps per second:  78, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.001108, mae: 0.852336, mean_q: 1.145756, mean_eps: 0.492292\n",
      " 564621/1000000: episode: 2376, duration: 4.534s, episode steps: 358, steps per second:  79, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.760 [0.000, 3.000],  loss: 0.000999, mae: 0.855583, mean_q: 1.149794, mean_eps: 0.492002\n",
      " 564995/1000000: episode: 2377, duration: 4.764s, episode steps: 374, steps per second:  79, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.650 [0.000, 3.000],  loss: 0.001299, mae: 0.857748, mean_q: 1.152561, mean_eps: 0.491673\n",
      " 565582/1000000: episode: 2378, duration: 7.608s, episode steps: 587, steps per second:  77, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.612 [0.000, 3.000],  loss: 0.001397, mae: 0.854184, mean_q: 1.148743, mean_eps: 0.491241\n",
      " 566044/1000000: episode: 2379, duration: 5.782s, episode steps: 462, steps per second:  80, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 0.001240, mae: 0.853742, mean_q: 1.149539, mean_eps: 0.490769\n",
      " 566424/1000000: episode: 2380, duration: 4.946s, episode steps: 380, steps per second:  77, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.463 [0.000, 3.000],  loss: 0.001487, mae: 0.854964, mean_q: 1.150428, mean_eps: 0.490391\n",
      " 566953/1000000: episode: 2381, duration: 6.840s, episode steps: 529, steps per second:  77, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.679 [0.000, 3.000],  loss: 0.001244, mae: 0.861673, mean_q: 1.159716, mean_eps: 0.489981\n",
      " 567388/1000000: episode: 2382, duration: 5.452s, episode steps: 435, steps per second:  80, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.625 [0.000, 3.000],  loss: 0.001139, mae: 0.851363, mean_q: 1.145663, mean_eps: 0.489547\n",
      " 567702/1000000: episode: 2383, duration: 4.087s, episode steps: 314, steps per second:  77, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.697 [0.000, 3.000],  loss: 0.001327, mae: 0.857770, mean_q: 1.154265, mean_eps: 0.489210\n",
      " 568020/1000000: episode: 2384, duration: 4.104s, episode steps: 318, steps per second:  77, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.487 [0.000, 3.000],  loss: 0.001317, mae: 0.861211, mean_q: 1.158215, mean_eps: 0.488926\n",
      " 568555/1000000: episode: 2385, duration: 6.720s, episode steps: 535, steps per second:  80, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.001085, mae: 0.861580, mean_q: 1.157223, mean_eps: 0.488543\n",
      " 568804/1000000: episode: 2386, duration: 3.274s, episode steps: 249, steps per second:  76, episode reward:  4.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.683 [0.000, 3.000],  loss: 0.001282, mae: 0.855775, mean_q: 1.151530, mean_eps: 0.488190\n",
      " 569285/1000000: episode: 2387, duration: 6.231s, episode steps: 481, steps per second:  77, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.778 [0.000, 3.000],  loss: 0.001253, mae: 0.849729, mean_q: 1.141902, mean_eps: 0.487860\n",
      " 569627/1000000: episode: 2388, duration: 4.288s, episode steps: 342, steps per second:  80, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.635 [0.000, 3.000],  loss: 0.001328, mae: 0.854604, mean_q: 1.148154, mean_eps: 0.487490\n",
      " 570013/1000000: episode: 2389, duration: 4.993s, episode steps: 386, steps per second:  77, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.821 [0.000, 3.000],  loss: 0.001255, mae: 0.852955, mean_q: 1.146987, mean_eps: 0.487162\n",
      " 570391/1000000: episode: 2390, duration: 4.959s, episode steps: 378, steps per second:  76, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.738 [0.000, 3.000],  loss: 0.001507, mae: 0.867194, mean_q: 1.164274, mean_eps: 0.486818\n",
      " 570935/1000000: episode: 2391, duration: 6.892s, episode steps: 544, steps per second:  79, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.684 [0.000, 3.000],  loss: 0.001642, mae: 0.857581, mean_q: 1.153198, mean_eps: 0.486404\n",
      " 571183/1000000: episode: 2392, duration: 3.201s, episode steps: 248, steps per second:  77, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: 0.001487, mae: 0.863168, mean_q: 1.159227, mean_eps: 0.486048\n",
      " 571500/1000000: episode: 2393, duration: 4.255s, episode steps: 317, steps per second:  74, episode reward:  6.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.644 [0.000, 3.000],  loss: 0.000864, mae: 0.866689, mean_q: 1.164381, mean_eps: 0.485794\n",
      " 571828/1000000: episode: 2394, duration: 4.216s, episode steps: 328, steps per second:  78, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.573 [0.000, 3.000],  loss: 0.001182, mae: 0.864435, mean_q: 1.161946, mean_eps: 0.485504\n",
      " 572061/1000000: episode: 2395, duration: 3.065s, episode steps: 233, steps per second:  76, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.528 [0.000, 3.000],  loss: 0.001164, mae: 0.858414, mean_q: 1.154308, mean_eps: 0.485250\n",
      " 572407/1000000: episode: 2396, duration: 4.460s, episode steps: 346, steps per second:  78, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.673 [0.000, 3.000],  loss: 0.001457, mae: 0.867795, mean_q: 1.166591, mean_eps: 0.484989\n",
      " 572623/1000000: episode: 2397, duration: 2.843s, episode steps: 216, steps per second:  76, episode reward:  3.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.722 [0.000, 3.000],  loss: 0.001265, mae: 0.865015, mean_q: 1.163085, mean_eps: 0.484737\n",
      " 572871/1000000: episode: 2398, duration: 3.276s, episode steps: 248, steps per second:  76, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.673 [0.000, 3.000],  loss: 0.001055, mae: 0.862399, mean_q: 1.159815, mean_eps: 0.484529\n",
      " 573320/1000000: episode: 2399, duration: 5.774s, episode steps: 449, steps per second:  78, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.693 [0.000, 3.000],  loss: 0.001179, mae: 0.863189, mean_q: 1.160115, mean_eps: 0.484215\n",
      " 573743/1000000: episode: 2400, duration: 5.451s, episode steps: 423, steps per second:  78, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.799 [0.000, 3.000],  loss: 0.001516, mae: 0.858579, mean_q: 1.153888, mean_eps: 0.483823\n",
      " 574061/1000000: episode: 2401, duration: 4.202s, episode steps: 318, steps per second:  76, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.648 [0.000, 3.000],  loss: 0.001360, mae: 0.866341, mean_q: 1.163789, mean_eps: 0.483488\n",
      " 574645/1000000: episode: 2402, duration: 7.448s, episode steps: 584, steps per second:  78, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.714 [0.000, 3.000],  loss: 0.001133, mae: 0.863300, mean_q: 1.161546, mean_eps: 0.483081\n",
      " 574846/1000000: episode: 2403, duration: 2.534s, episode steps: 201, steps per second:  79, episode reward:  2.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.458 [0.000, 3.000],  loss: 0.001012, mae: 0.870123, mean_q: 1.169763, mean_eps: 0.482729\n",
      " 575336/1000000: episode: 2404, duration: 6.412s, episode steps: 490, steps per second:  76, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.001231, mae: 0.859424, mean_q: 1.155323, mean_eps: 0.482419\n",
      " 576123/1000000: episode: 2405, duration: 9.952s, episode steps: 787, steps per second:  79, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.712 [0.000, 3.000],  loss: 0.001102, mae: 0.865354, mean_q: 1.164084, mean_eps: 0.481845\n",
      " 576278/1000000: episode: 2406, duration: 1.991s, episode steps: 155, steps per second:  78, episode reward:  1.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: 0.001865, mae: 0.859643, mean_q: 1.155634, mean_eps: 0.481420\n",
      " 576850/1000000: episode: 2407, duration: 7.467s, episode steps: 572, steps per second:  77, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.640 [0.000, 3.000],  loss: 0.001290, mae: 0.859831, mean_q: 1.157595, mean_eps: 0.481092\n",
      " 577320/1000000: episode: 2408, duration: 5.887s, episode steps: 470, steps per second:  80, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.602 [0.000, 3.000],  loss: 0.001219, mae: 0.855730, mean_q: 1.151421, mean_eps: 0.480624\n",
      " 577676/1000000: episode: 2409, duration: 4.779s, episode steps: 356, steps per second:  74, episode reward:  7.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.455 [0.000, 3.000],  loss: 0.001224, mae: 0.863544, mean_q: 1.162751, mean_eps: 0.480254\n",
      " 578052/1000000: episode: 2410, duration: 4.953s, episode steps: 376, steps per second:  76, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.561 [0.000, 3.000],  loss: 0.001439, mae: 0.865121, mean_q: 1.164501, mean_eps: 0.479924\n",
      " 578327/1000000: episode: 2411, duration: 3.534s, episode steps: 275, steps per second:  78, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.658 [0.000, 3.000],  loss: 0.000914, mae: 0.859941, mean_q: 1.155918, mean_eps: 0.479631\n",
      " 578858/1000000: episode: 2412, duration: 6.735s, episode steps: 531, steps per second:  79, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.691 [0.000, 3.000],  loss: 0.001039, mae: 0.873103, mean_q: 1.173646, mean_eps: 0.479267\n",
      " 579120/1000000: episode: 2413, duration: 3.499s, episode steps: 262, steps per second:  75, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.653 [0.000, 3.000],  loss: 0.000850, mae: 0.857241, mean_q: 1.152292, mean_eps: 0.478911\n",
      " 579478/1000000: episode: 2414, duration: 4.587s, episode steps: 358, steps per second:  78, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.740 [0.000, 3.000],  loss: 0.001515, mae: 0.864628, mean_q: 1.162984, mean_eps: 0.478632\n",
      " 579962/1000000: episode: 2415, duration: 6.299s, episode steps: 484, steps per second:  77, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.001399, mae: 0.868838, mean_q: 1.169253, mean_eps: 0.478252\n",
      " 580389/1000000: episode: 2416, duration: 5.677s, episode steps: 427, steps per second:  75, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.674 [0.000, 3.000],  loss: 0.001484, mae: 0.862842, mean_q: 1.159701, mean_eps: 0.477842\n",
      " 580738/1000000: episode: 2417, duration: 4.509s, episode steps: 349, steps per second:  77, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.642 [0.000, 3.000],  loss: 0.001414, mae: 0.865499, mean_q: 1.162433, mean_eps: 0.477492\n",
      " 581370/1000000: episode: 2418, duration: 8.245s, episode steps: 632, steps per second:  77, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: 0.001231, mae: 0.867396, mean_q: 1.165476, mean_eps: 0.477051\n",
      " 581730/1000000: episode: 2419, duration: 4.729s, episode steps: 360, steps per second:  76, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 0.001091, mae: 0.868771, mean_q: 1.166052, mean_eps: 0.476605\n",
      " 582275/1000000: episode: 2420, duration: 7.013s, episode steps: 545, steps per second:  78, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.809 [0.000, 3.000],  loss: 0.001091, mae: 0.861659, mean_q: 1.159559, mean_eps: 0.476198\n",
      " 582843/1000000: episode: 2421, duration: 7.513s, episode steps: 568, steps per second:  76, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.001221, mae: 0.869338, mean_q: 1.170198, mean_eps: 0.475698\n",
      " 583278/1000000: episode: 2422, duration: 5.637s, episode steps: 435, steps per second:  77, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.517 [0.000, 3.000],  loss: 0.001316, mae: 0.869947, mean_q: 1.168821, mean_eps: 0.475246\n",
      " 583817/1000000: episode: 2423, duration: 6.977s, episode steps: 539, steps per second:  77, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.001409, mae: 0.869396, mean_q: 1.170460, mean_eps: 0.474807\n",
      " 584230/1000000: episode: 2424, duration: 5.262s, episode steps: 413, steps per second:  78, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.751 [0.000, 3.000],  loss: 0.001123, mae: 0.870605, mean_q: 1.170762, mean_eps: 0.474378\n",
      " 584543/1000000: episode: 2425, duration: 4.025s, episode steps: 313, steps per second:  78, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.575 [0.000, 3.000],  loss: 0.001301, mae: 0.870045, mean_q: 1.168053, mean_eps: 0.474053\n",
      " 585036/1000000: episode: 2426, duration: 6.614s, episode steps: 493, steps per second:  75, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.645 [0.000, 3.000],  loss: 0.001705, mae: 0.866268, mean_q: 1.165081, mean_eps: 0.473691\n",
      " 585375/1000000: episode: 2427, duration: 4.368s, episode steps: 339, steps per second:  78, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.640 [0.000, 3.000],  loss: 0.001612, mae: 0.871386, mean_q: 1.173183, mean_eps: 0.473316\n",
      " 585961/1000000: episode: 2428, duration: 7.621s, episode steps: 586, steps per second:  77, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.001142, mae: 0.871392, mean_q: 1.171734, mean_eps: 0.472899\n",
      " 586432/1000000: episode: 2429, duration: 6.252s, episode steps: 471, steps per second:  75, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.620 [0.000, 3.000],  loss: 0.001045, mae: 0.869823, mean_q: 1.169207, mean_eps: 0.472424\n",
      " 586734/1000000: episode: 2430, duration: 3.875s, episode steps: 302, steps per second:  78, episode reward:  6.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 0.001255, mae: 0.863971, mean_q: 1.163399, mean_eps: 0.472076\n",
      " 587155/1000000: episode: 2431, duration: 5.412s, episode steps: 421, steps per second:  78, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.629 [0.000, 3.000],  loss: 0.001498, mae: 0.862857, mean_q: 1.161880, mean_eps: 0.471750\n",
      " 587541/1000000: episode: 2432, duration: 5.172s, episode steps: 386, steps per second:  75, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.819 [0.000, 3.000],  loss: 0.001401, mae: 0.866725, mean_q: 1.166604, mean_eps: 0.471387\n",
      " 587849/1000000: episode: 2433, duration: 3.947s, episode steps: 308, steps per second:  78, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 0.001252, mae: 0.867907, mean_q: 1.165865, mean_eps: 0.471074\n",
      " 588201/1000000: episode: 2434, duration: 4.629s, episode steps: 352, steps per second:  76, episode reward:  7.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.489 [0.000, 3.000],  loss: 0.001482, mae: 0.871052, mean_q: 1.171449, mean_eps: 0.470777\n",
      " 588430/1000000: episode: 2435, duration: 2.863s, episode steps: 229, steps per second:  80, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.821 [0.000, 3.000],  loss: 0.001171, mae: 0.875345, mean_q: 1.178357, mean_eps: 0.470516\n",
      " 588992/1000000: episode: 2436, duration: 7.458s, episode steps: 562, steps per second:  75, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.593 [0.000, 3.000],  loss: 0.001167, mae: 0.861395, mean_q: 1.159409, mean_eps: 0.470161\n",
      " 589219/1000000: episode: 2437, duration: 3.031s, episode steps: 227, steps per second:  75, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 0.001145, mae: 0.884148, mean_q: 1.188955, mean_eps: 0.469806\n",
      " 589466/1000000: episode: 2438, duration: 3.229s, episode steps: 247, steps per second:  76, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.001216, mae: 0.853282, mean_q: 1.150157, mean_eps: 0.469592\n",
      " 589730/1000000: episode: 2439, duration: 3.384s, episode steps: 264, steps per second:  78, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.754 [0.000, 3.000],  loss: 0.000928, mae: 0.875938, mean_q: 1.179805, mean_eps: 0.469362\n",
      " 590123/1000000: episode: 2440, duration: 5.295s, episode steps: 393, steps per second:  74, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.001221, mae: 0.870280, mean_q: 1.171699, mean_eps: 0.469067\n",
      " 590607/1000000: episode: 2441, duration: 6.218s, episode steps: 484, steps per second:  78, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.318 [0.000, 3.000],  loss: 0.001933, mae: 0.871652, mean_q: 1.170943, mean_eps: 0.468672\n",
      " 590910/1000000: episode: 2442, duration: 3.868s, episode steps: 303, steps per second:  78, episode reward:  5.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.703 [0.000, 3.000],  loss: 0.001331, mae: 0.881140, mean_q: 1.186321, mean_eps: 0.468318\n",
      " 591288/1000000: episode: 2443, duration: 5.151s, episode steps: 378, steps per second:  73, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: 0.001801, mae: 0.891535, mean_q: 1.198230, mean_eps: 0.468012\n",
      " 591802/1000000: episode: 2444, duration: 6.685s, episode steps: 514, steps per second:  77, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001664, mae: 0.874821, mean_q: 1.176461, mean_eps: 0.467610\n",
      " 592601/1000000: episode: 2445, duration: 10.486s, episode steps: 799, steps per second:  76, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.642 [0.000, 3.000],  loss: 0.001454, mae: 0.876329, mean_q: 1.178231, mean_eps: 0.467018\n",
      " 593136/1000000: episode: 2446, duration: 6.999s, episode steps: 535, steps per second:  76, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.677 [0.000, 3.000],  loss: 0.001419, mae: 0.880908, mean_q: 1.185315, mean_eps: 0.466419\n",
      " 593395/1000000: episode: 2447, duration: 3.372s, episode steps: 259, steps per second:  77, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: 0.002301, mae: 0.888779, mean_q: 1.198054, mean_eps: 0.466062\n",
      " 593777/1000000: episode: 2448, duration: 5.136s, episode steps: 382, steps per second:  74, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.665 [0.000, 3.000],  loss: 0.001472, mae: 0.873196, mean_q: 1.174259, mean_eps: 0.465773\n",
      " 594140/1000000: episode: 2449, duration: 4.761s, episode steps: 363, steps per second:  76, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.694 [0.000, 3.000],  loss: 0.001395, mae: 0.876123, mean_q: 1.179222, mean_eps: 0.465438\n",
      " 594761/1000000: episode: 2450, duration: 8.196s, episode steps: 621, steps per second:  76, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.001640, mae: 0.873009, mean_q: 1.174011, mean_eps: 0.464995\n",
      " 595198/1000000: episode: 2451, duration: 5.674s, episode steps: 437, steps per second:  77, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.716 [0.000, 3.000],  loss: 0.001522, mae: 0.887860, mean_q: 1.193057, mean_eps: 0.464518\n",
      " 595833/1000000: episode: 2452, duration: 8.197s, episode steps: 635, steps per second:  77, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.001409, mae: 0.886511, mean_q: 1.192863, mean_eps: 0.464036\n",
      " 596285/1000000: episode: 2453, duration: 6.090s, episode steps: 452, steps per second:  74, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: 0.001326, mae: 0.872194, mean_q: 1.172377, mean_eps: 0.463546\n",
      " 596889/1000000: episode: 2454, duration: 7.651s, episode steps: 604, steps per second:  79, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.603 [0.000, 3.000],  loss: 0.001505, mae: 0.881654, mean_q: 1.185855, mean_eps: 0.463071\n",
      " 597155/1000000: episode: 2455, duration: 3.552s, episode steps: 266, steps per second:  75, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.432 [0.000, 3.000],  loss: 0.001308, mae: 0.880466, mean_q: 1.184009, mean_eps: 0.462680\n",
      " 597793/1000000: episode: 2456, duration: 8.435s, episode steps: 638, steps per second:  76, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.491 [0.000, 3.000],  loss: 0.001360, mae: 0.884915, mean_q: 1.190566, mean_eps: 0.462273\n",
      " 598189/1000000: episode: 2457, duration: 5.088s, episode steps: 396, steps per second:  78, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.657 [0.000, 3.000],  loss: 0.001885, mae: 0.883861, mean_q: 1.187913, mean_eps: 0.461807\n",
      " 598719/1000000: episode: 2458, duration: 7.047s, episode steps: 530, steps per second:  75, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.745 [0.000, 3.000],  loss: 0.001364, mae: 0.878181, mean_q: 1.180461, mean_eps: 0.461391\n",
      " 599097/1000000: episode: 2459, duration: 4.903s, episode steps: 378, steps per second:  77, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.001334, mae: 0.879825, mean_q: 1.184168, mean_eps: 0.460983\n",
      " 599341/1000000: episode: 2460, duration: 3.178s, episode steps: 244, steps per second:  77, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.357 [0.000, 3.000],  loss: 0.001177, mae: 0.877713, mean_q: 1.181470, mean_eps: 0.460702\n",
      " 599732/1000000: episode: 2461, duration: 5.329s, episode steps: 391, steps per second:  73, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.721 [0.000, 3.000],  loss: 0.001412, mae: 0.874609, mean_q: 1.177614, mean_eps: 0.460418\n",
      " 600097/1000000: episode: 2462, duration: 4.726s, episode steps: 365, steps per second:  77, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.729 [0.000, 3.000],  loss: 0.001840, mae: 0.877631, mean_q: 1.180403, mean_eps: 0.460077\n",
      " 600342/1000000: episode: 2463, duration: 3.162s, episode steps: 245, steps per second:  77, episode reward:  4.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.890 [0.000, 3.000],  loss: 0.001824, mae: 0.880928, mean_q: 1.185271, mean_eps: 0.459802\n",
      " 600878/1000000: episode: 2464, duration: 7.153s, episode steps: 536, steps per second:  75, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.001794, mae: 0.877761, mean_q: 1.180913, mean_eps: 0.459451\n",
      " 601173/1000000: episode: 2465, duration: 3.909s, episode steps: 295, steps per second:  75, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.749 [0.000, 3.000],  loss: 0.001772, mae: 0.885260, mean_q: 1.191520, mean_eps: 0.459077\n",
      " 601659/1000000: episode: 2466, duration: 6.295s, episode steps: 486, steps per second:  77, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.770 [0.000, 3.000],  loss: 0.001582, mae: 0.884607, mean_q: 1.190179, mean_eps: 0.458726\n",
      " 602067/1000000: episode: 2467, duration: 5.480s, episode steps: 408, steps per second:  74, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.826 [0.000, 3.000],  loss: 0.001617, mae: 0.876627, mean_q: 1.179536, mean_eps: 0.458324\n",
      " 602331/1000000: episode: 2468, duration: 3.520s, episode steps: 264, steps per second:  75, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.716 [0.000, 3.000],  loss: 0.001735, mae: 0.884697, mean_q: 1.194151, mean_eps: 0.458022\n",
      " 602792/1000000: episode: 2469, duration: 5.935s, episode steps: 461, steps per second:  78, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.688 [0.000, 3.000],  loss: 0.001691, mae: 0.886388, mean_q: 1.193472, mean_eps: 0.457696\n",
      " 603415/1000000: episode: 2470, duration: 8.356s, episode steps: 623, steps per second:  75, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.706 [0.000, 3.000],  loss: 0.001414, mae: 0.886110, mean_q: 1.193445, mean_eps: 0.457208\n",
      " 603920/1000000: episode: 2471, duration: 6.603s, episode steps: 505, steps per second:  76, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.687 [0.000, 3.000],  loss: 0.001712, mae: 0.879155, mean_q: 1.184122, mean_eps: 0.456701\n",
      " 604180/1000000: episode: 2472, duration: 3.444s, episode steps: 260, steps per second:  75, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.715 [0.000, 3.000],  loss: 0.001580, mae: 0.870934, mean_q: 1.172752, mean_eps: 0.456357\n",
      " 604524/1000000: episode: 2473, duration: 4.734s, episode steps: 344, steps per second:  73, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.642 [0.000, 3.000],  loss: 0.001267, mae: 0.876373, mean_q: 1.180177, mean_eps: 0.456085\n",
      " 604794/1000000: episode: 2474, duration: 3.680s, episode steps: 270, steps per second:  73, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.611 [0.000, 3.000],  loss: 0.001534, mae: 0.887706, mean_q: 1.195056, mean_eps: 0.455808\n",
      " 605077/1000000: episode: 2475, duration: 3.742s, episode steps: 283, steps per second:  76, episode reward:  5.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.484 [0.000, 3.000],  loss: 0.001347, mae: 0.881715, mean_q: 1.187131, mean_eps: 0.455558\n",
      " 605444/1000000: episode: 2476, duration: 4.826s, episode steps: 367, steps per second:  76, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.757 [0.000, 3.000],  loss: 0.001260, mae: 0.885263, mean_q: 1.192061, mean_eps: 0.455266\n",
      " 605835/1000000: episode: 2477, duration: 5.412s, episode steps: 391, steps per second:  72, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.714 [0.000, 3.000],  loss: 0.001430, mae: 0.876002, mean_q: 1.178633, mean_eps: 0.454926\n",
      " 606130/1000000: episode: 2478, duration: 3.905s, episode steps: 295, steps per second:  76, episode reward:  5.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.651 [0.000, 3.000],  loss: 0.001412, mae: 0.879811, mean_q: 1.181906, mean_eps: 0.454616\n",
      " 606428/1000000: episode: 2479, duration: 3.981s, episode steps: 298, steps per second:  75, episode reward:  5.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.668 [0.000, 3.000],  loss: 0.001169, mae: 0.894267, mean_q: 1.202980, mean_eps: 0.454350\n",
      " 606929/1000000: episode: 2480, duration: 6.659s, episode steps: 501, steps per second:  75, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.717 [0.000, 3.000],  loss: 0.001547, mae: 0.883743, mean_q: 1.189380, mean_eps: 0.453990\n",
      " 607394/1000000: episode: 2481, duration: 6.277s, episode steps: 465, steps per second:  74, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: 0.001474, mae: 0.888395, mean_q: 1.195374, mean_eps: 0.453554\n",
      " 607772/1000000: episode: 2482, duration: 4.983s, episode steps: 378, steps per second:  76, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.743 [0.000, 3.000],  loss: 0.001243, mae: 0.896521, mean_q: 1.205997, mean_eps: 0.453176\n",
      " 608489/1000000: episode: 2483, duration: 9.632s, episode steps: 717, steps per second:  74, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.601 [0.000, 3.000],  loss: 0.001682, mae: 0.879962, mean_q: 1.184819, mean_eps: 0.452683\n",
      " 608670/1000000: episode: 2484, duration: 2.365s, episode steps: 181, steps per second:  77, episode reward:  2.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.613 [0.000, 3.000],  loss: 0.001297, mae: 0.893223, mean_q: 1.200762, mean_eps: 0.452278\n",
      " 609226/1000000: episode: 2485, duration: 7.324s, episode steps: 556, steps per second:  76, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: 0.001907, mae: 0.887395, mean_q: 1.195155, mean_eps: 0.451947\n",
      " 609859/1000000: episode: 2486, duration: 8.491s, episode steps: 633, steps per second:  75, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.712 [0.000, 3.000],  loss: 0.001538, mae: 0.889634, mean_q: 1.197224, mean_eps: 0.451412\n",
      " 610087/1000000: episode: 2487, duration: 2.958s, episode steps: 228, steps per second:  77, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.430 [0.000, 3.000],  loss: 0.001752, mae: 0.888093, mean_q: 1.195948, mean_eps: 0.451025\n",
      " 610534/1000000: episode: 2488, duration: 6.521s, episode steps: 447, steps per second:  69, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.575 [0.000, 3.000],  loss: 0.002004, mae: 0.903136, mean_q: 1.215140, mean_eps: 0.450721\n",
      " 610923/1000000: episode: 2489, duration: 5.242s, episode steps: 389, steps per second:  74, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.602 [0.000, 3.000],  loss: 0.001597, mae: 0.888915, mean_q: 1.197562, mean_eps: 0.450345\n",
      " 611506/1000000: episode: 2490, duration: 7.669s, episode steps: 583, steps per second:  76, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.815 [0.000, 3.000],  loss: 0.001709, mae: 0.910680, mean_q: 1.226180, mean_eps: 0.449907\n",
      " 611866/1000000: episode: 2491, duration: 4.856s, episode steps: 360, steps per second:  74, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 0.001714, mae: 0.896899, mean_q: 1.208001, mean_eps: 0.449483\n",
      " 612433/1000000: episode: 2492, duration: 7.491s, episode steps: 567, steps per second:  76, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.531 [0.000, 3.000],  loss: 0.001972, mae: 0.899753, mean_q: 1.212462, mean_eps: 0.449065\n",
      " 612900/1000000: episode: 2493, duration: 6.227s, episode steps: 467, steps per second:  75, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.647 [0.000, 3.000],  loss: 0.001626, mae: 0.895045, mean_q: 1.206332, mean_eps: 0.448601\n",
      " 613368/1000000: episode: 2494, duration: 6.190s, episode steps: 468, steps per second:  76, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.001758, mae: 0.904164, mean_q: 1.217769, mean_eps: 0.448181\n",
      " 613756/1000000: episode: 2495, duration: 5.245s, episode steps: 388, steps per second:  74, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.691 [0.000, 3.000],  loss: 0.001487, mae: 0.893333, mean_q: 1.203992, mean_eps: 0.447796\n",
      " 614138/1000000: episode: 2496, duration: 5.234s, episode steps: 382, steps per second:  73, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.482 [0.000, 3.000],  loss: 0.001566, mae: 0.902920, mean_q: 1.214755, mean_eps: 0.447449\n",
      " 614544/1000000: episode: 2497, duration: 5.440s, episode steps: 406, steps per second:  75, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.406 [0.000, 3.000],  loss: 0.001589, mae: 0.904981, mean_q: 1.219584, mean_eps: 0.447094\n",
      " 615123/1000000: episode: 2498, duration: 7.628s, episode steps: 579, steps per second:  76, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.712 [0.000, 3.000],  loss: 0.001683, mae: 0.898224, mean_q: 1.209908, mean_eps: 0.446651\n",
      " 615630/1000000: episode: 2499, duration: 6.890s, episode steps: 507, steps per second:  74, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.001788, mae: 0.886925, mean_q: 1.196482, mean_eps: 0.446162\n",
      " 616072/1000000: episode: 2500, duration: 5.881s, episode steps: 442, steps per second:  75, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.001803, mae: 0.897687, mean_q: 1.208957, mean_eps: 0.445735\n",
      " 616731/1000000: episode: 2501, duration: 8.829s, episode steps: 659, steps per second:  75, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.725 [0.000, 3.000],  loss: 0.001727, mae: 0.895190, mean_q: 1.204775, mean_eps: 0.445240\n",
      " 617175/1000000: episode: 2502, duration: 5.878s, episode steps: 444, steps per second:  76, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.604 [0.000, 3.000],  loss: 0.001575, mae: 0.892764, mean_q: 1.202970, mean_eps: 0.444743\n",
      " 617565/1000000: episode: 2503, duration: 5.248s, episode steps: 390, steps per second:  74, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.603 [0.000, 3.000],  loss: 0.001206, mae: 0.910868, mean_q: 1.225953, mean_eps: 0.444367\n",
      " 618162/1000000: episode: 2504, duration: 8.030s, episode steps: 597, steps per second:  74, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.414 [0.000, 3.000],  loss: 0.001535, mae: 0.907235, mean_q: 1.220659, mean_eps: 0.443922\n",
      " 618667/1000000: episode: 2505, duration: 6.622s, episode steps: 505, steps per second:  76, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.001354, mae: 0.899748, mean_q: 1.211775, mean_eps: 0.443427\n",
      " 618990/1000000: episode: 2506, duration: 4.458s, episode steps: 323, steps per second:  72, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.548 [0.000, 3.000],  loss: 0.001279, mae: 0.893730, mean_q: 1.203115, mean_eps: 0.443055\n",
      " 619403/1000000: episode: 2507, duration: 5.490s, episode steps: 413, steps per second:  75, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.001753, mae: 0.898633, mean_q: 1.210186, mean_eps: 0.442724\n",
      " 619730/1000000: episode: 2508, duration: 4.364s, episode steps: 327, steps per second:  75, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: 0.001696, mae: 0.897069, mean_q: 1.209077, mean_eps: 0.442391\n",
      " 620258/1000000: episode: 2509, duration: 7.453s, episode steps: 528, steps per second:  71, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.659 [0.000, 3.000],  loss: 0.001915, mae: 0.904968, mean_q: 1.218746, mean_eps: 0.442005\n",
      " 621040/1000000: episode: 2510, duration: 10.546s, episode steps: 782, steps per second:  74, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 0.002296, mae: 0.918367, mean_q: 1.236007, mean_eps: 0.441417\n",
      " 621664/1000000: episode: 2511, duration: 8.420s, episode steps: 624, steps per second:  74, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.644 [0.000, 3.000],  loss: 0.002022, mae: 0.916982, mean_q: 1.233727, mean_eps: 0.440785\n",
      " 622020/1000000: episode: 2512, duration: 4.778s, episode steps: 356, steps per second:  75, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.001998, mae: 0.921229, mean_q: 1.243365, mean_eps: 0.440344\n",
      " 622278/1000000: episode: 2513, duration: 3.492s, episode steps: 258, steps per second:  74, episode reward:  4.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.001709, mae: 0.923098, mean_q: 1.242516, mean_eps: 0.440067\n",
      " 622571/1000000: episode: 2514, duration: 4.005s, episode steps: 293, steps per second:  73, episode reward:  5.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.706 [0.000, 3.000],  loss: 0.001853, mae: 0.917432, mean_q: 1.236932, mean_eps: 0.439818\n",
      " 622799/1000000: episode: 2515, duration: 3.069s, episode steps: 228, steps per second:  74, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 0.001632, mae: 0.920948, mean_q: 1.242894, mean_eps: 0.439584\n",
      " 623200/1000000: episode: 2516, duration: 5.348s, episode steps: 401, steps per second:  75, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.501 [0.000, 3.000],  loss: 0.001617, mae: 0.910914, mean_q: 1.227113, mean_eps: 0.439302\n",
      " 623640/1000000: episode: 2517, duration: 6.076s, episode steps: 440, steps per second:  72, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.930 [0.000, 3.000],  loss: 0.001420, mae: 0.925105, mean_q: 1.246619, mean_eps: 0.438924\n",
      " 624101/1000000: episode: 2518, duration: 6.172s, episode steps: 461, steps per second:  75, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.603 [0.000, 3.000],  loss: 0.002002, mae: 0.915733, mean_q: 1.233457, mean_eps: 0.438517\n",
      " 624406/1000000: episode: 2519, duration: 4.021s, episode steps: 305, steps per second:  76, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.475 [0.000, 3.000],  loss: 0.001990, mae: 0.913082, mean_q: 1.229490, mean_eps: 0.438171\n",
      " 624920/1000000: episode: 2520, duration: 6.982s, episode steps: 514, steps per second:  74, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.679 [0.000, 3.000],  loss: 0.001686, mae: 0.922595, mean_q: 1.243437, mean_eps: 0.437804\n",
      " 625307/1000000: episode: 2521, duration: 5.255s, episode steps: 387, steps per second:  74, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.625 [0.000, 3.000],  loss: 0.001544, mae: 0.921059, mean_q: 1.240226, mean_eps: 0.437399\n",
      " 625622/1000000: episode: 2522, duration: 4.215s, episode steps: 315, steps per second:  75, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.756 [0.000, 3.000],  loss: 0.001447, mae: 0.920702, mean_q: 1.239175, mean_eps: 0.437082\n",
      " 625990/1000000: episode: 2523, duration: 5.033s, episode steps: 368, steps per second:  73, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.565 [0.000, 3.000],  loss: 0.001510, mae: 0.917497, mean_q: 1.234977, mean_eps: 0.436775\n",
      " 626533/1000000: episode: 2524, duration: 7.315s, episode steps: 543, steps per second:  74, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 0.001973, mae: 0.914808, mean_q: 1.232764, mean_eps: 0.436364\n",
      " 627081/1000000: episode: 2525, duration: 7.359s, episode steps: 548, steps per second:  74, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.668 [0.000, 3.000],  loss: 0.001647, mae: 0.915165, mean_q: 1.233015, mean_eps: 0.435873\n",
      " 627850/1000000: episode: 2526, duration: 10.249s, episode steps: 769, steps per second:  75, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 0.001592, mae: 0.919348, mean_q: 1.237940, mean_eps: 0.435281\n",
      " 628066/1000000: episode: 2527, duration: 2.930s, episode steps: 216, steps per second:  74, episode reward:  2.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.597 [0.000, 3.000],  loss: 0.001302, mae: 0.926219, mean_q: 1.247168, mean_eps: 0.434838\n",
      " 628274/1000000: episode: 2528, duration: 2.859s, episode steps: 208, steps per second:  73, episode reward:  3.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.505 [0.000, 3.000],  loss: 0.002002, mae: 0.931985, mean_q: 1.254758, mean_eps: 0.434647\n",
      " 628730/1000000: episode: 2529, duration: 6.190s, episode steps: 456, steps per second:  74, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.785 [0.000, 3.000],  loss: 0.001750, mae: 0.915405, mean_q: 1.233224, mean_eps: 0.434348\n",
      " 629095/1000000: episode: 2530, duration: 4.950s, episode steps: 365, steps per second:  74, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.655 [0.000, 3.000],  loss: 0.001670, mae: 0.918072, mean_q: 1.238345, mean_eps: 0.433979\n",
      " 629611/1000000: episode: 2531, duration: 7.047s, episode steps: 516, steps per second:  73, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.802 [0.000, 3.000],  loss: 0.001734, mae: 0.921908, mean_q: 1.241718, mean_eps: 0.433583\n",
      " 629943/1000000: episode: 2532, duration: 4.601s, episode steps: 332, steps per second:  72, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.355 [0.000, 3.000],  loss: 0.002026, mae: 0.913455, mean_q: 1.230355, mean_eps: 0.433202\n",
      " 630311/1000000: episode: 2533, duration: 4.977s, episode steps: 368, steps per second:  74, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: 0.002229, mae: 0.928509, mean_q: 1.249614, mean_eps: 0.432887\n",
      " 630624/1000000: episode: 2534, duration: 4.347s, episode steps: 313, steps per second:  72, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.604 [0.000, 3.000],  loss: 0.002077, mae: 0.927123, mean_q: 1.247883, mean_eps: 0.432581\n",
      " 630963/1000000: episode: 2535, duration: 4.607s, episode steps: 339, steps per second:  74, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.655 [0.000, 3.000],  loss: 0.002571, mae: 0.916343, mean_q: 1.236817, mean_eps: 0.432287\n",
      " 631402/1000000: episode: 2536, duration: 5.904s, episode steps: 439, steps per second:  74, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.001562, mae: 0.919695, mean_q: 1.239348, mean_eps: 0.431936\n",
      " 631718/1000000: episode: 2537, duration: 4.216s, episode steps: 316, steps per second:  75, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 0.001948, mae: 0.916346, mean_q: 1.235313, mean_eps: 0.431596\n",
      " 632272/1000000: episode: 2538, duration: 7.684s, episode steps: 554, steps per second:  72, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.810 [0.000, 3.000],  loss: 0.001603, mae: 0.928741, mean_q: 1.252061, mean_eps: 0.431205\n",
      " 632708/1000000: episode: 2539, duration: 5.876s, episode steps: 436, steps per second:  74, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.002373, mae: 0.920405, mean_q: 1.240059, mean_eps: 0.430761\n",
      " 633040/1000000: episode: 2540, duration: 4.638s, episode steps: 332, steps per second:  72, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.825 [0.000, 3.000],  loss: 0.001945, mae: 0.923342, mean_q: 1.244523, mean_eps: 0.430415\n",
      " 633580/1000000: episode: 2541, duration: 7.229s, episode steps: 540, steps per second:  75, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.613 [0.000, 3.000],  loss: 0.002153, mae: 0.928695, mean_q: 1.250649, mean_eps: 0.430023\n",
      " 634063/1000000: episode: 2542, duration: 6.499s, episode steps: 483, steps per second:  74, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.733 [0.000, 3.000],  loss: 0.001713, mae: 0.935029, mean_q: 1.259926, mean_eps: 0.429562\n",
      " 634596/1000000: episode: 2543, duration: 7.288s, episode steps: 533, steps per second:  73, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.659 [0.000, 3.000],  loss: 0.002015, mae: 0.925472, mean_q: 1.247504, mean_eps: 0.429105\n",
      " 635202/1000000: episode: 2544, duration: 8.070s, episode steps: 606, steps per second:  75, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.001896, mae: 0.926583, mean_q: 1.248848, mean_eps: 0.428592\n",
      " 635509/1000000: episode: 2545, duration: 4.294s, episode steps: 307, steps per second:  71, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.759 [0.000, 3.000],  loss: 0.002140, mae: 0.924040, mean_q: 1.246718, mean_eps: 0.428180\n",
      " 635800/1000000: episode: 2546, duration: 3.864s, episode steps: 291, steps per second:  75, episode reward:  5.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.601 [0.000, 3.000],  loss: 0.001829, mae: 0.932475, mean_q: 1.256294, mean_eps: 0.427911\n",
      " 636343/1000000: episode: 2547, duration: 7.306s, episode steps: 543, steps per second:  74, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.597 [0.000, 3.000],  loss: 0.001605, mae: 0.929436, mean_q: 1.253117, mean_eps: 0.427537\n",
      " 636737/1000000: episode: 2548, duration: 5.556s, episode steps: 394, steps per second:  71, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.670 [0.000, 3.000],  loss: 0.001567, mae: 0.927171, mean_q: 1.250298, mean_eps: 0.427114\n",
      " 637381/1000000: episode: 2549, duration: 8.687s, episode steps: 644, steps per second:  74, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.839 [0.000, 3.000],  loss: 0.001806, mae: 0.930418, mean_q: 1.253890, mean_eps: 0.426646\n",
      " 637872/1000000: episode: 2550, duration: 6.863s, episode steps: 491, steps per second:  72, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.768 [0.000, 3.000],  loss: 0.001793, mae: 0.922133, mean_q: 1.243435, mean_eps: 0.426137\n",
      " 638197/1000000: episode: 2551, duration: 4.435s, episode steps: 325, steps per second:  73, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 0.001869, mae: 0.935594, mean_q: 1.261313, mean_eps: 0.425769\n",
      " 638685/1000000: episode: 2552, duration: 6.491s, episode steps: 488, steps per second:  75, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.001741, mae: 0.927192, mean_q: 1.251206, mean_eps: 0.425402\n",
      " 639289/1000000: episode: 2553, duration: 8.367s, episode steps: 604, steps per second:  72, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: 0.001522, mae: 0.925370, mean_q: 1.246177, mean_eps: 0.424911\n",
      " 639718/1000000: episode: 2554, duration: 5.804s, episode steps: 429, steps per second:  74, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.774 [0.000, 3.000],  loss: 0.001871, mae: 0.931924, mean_q: 1.256830, mean_eps: 0.424446\n",
      " 640311/1000000: episode: 2555, duration: 8.088s, episode steps: 593, steps per second:  73, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.720 [0.000, 3.000],  loss: 0.001997, mae: 0.930690, mean_q: 1.253280, mean_eps: 0.423987\n",
      " 640602/1000000: episode: 2556, duration: 3.992s, episode steps: 291, steps per second:  73, episode reward:  5.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.509 [0.000, 3.000],  loss: 0.001767, mae: 0.941507, mean_q: 1.268824, mean_eps: 0.423590\n",
      " 641139/1000000: episode: 2557, duration: 7.213s, episode steps: 537, steps per second:  74, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.002362, mae: 0.951699, mean_q: 1.280504, mean_eps: 0.423217\n",
      " 641575/1000000: episode: 2558, duration: 6.102s, episode steps: 436, steps per second:  71, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.775 [0.000, 3.000],  loss: 0.001770, mae: 0.945725, mean_q: 1.273901, mean_eps: 0.422780\n",
      " 642176/1000000: episode: 2559, duration: 8.075s, episode steps: 601, steps per second:  74, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.727 [0.000, 3.000],  loss: 0.001901, mae: 0.947188, mean_q: 1.276919, mean_eps: 0.422313\n",
      " 642749/1000000: episode: 2560, duration: 7.970s, episode steps: 573, steps per second:  72, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.637 [0.000, 3.000],  loss: 0.002129, mae: 0.948552, mean_q: 1.278822, mean_eps: 0.421784\n",
      " 643158/1000000: episode: 2561, duration: 5.457s, episode steps: 409, steps per second:  75, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.002188, mae: 0.939925, mean_q: 1.268142, mean_eps: 0.421341\n",
      " 643474/1000000: episode: 2562, duration: 4.310s, episode steps: 316, steps per second:  73, episode reward:  6.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.627 [0.000, 3.000],  loss: 0.001583, mae: 0.936751, mean_q: 1.262790, mean_eps: 0.421016\n",
      " 643760/1000000: episode: 2563, duration: 4.169s, episode steps: 286, steps per second:  69, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.720 [0.000, 3.000],  loss: 0.001823, mae: 0.950665, mean_q: 1.283271, mean_eps: 0.420746\n",
      " 644191/1000000: episode: 2564, duration: 5.815s, episode steps: 431, steps per second:  74, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.675 [0.000, 3.000],  loss: 0.001747, mae: 0.939599, mean_q: 1.268013, mean_eps: 0.420423\n",
      " 644674/1000000: episode: 2565, duration: 6.565s, episode steps: 483, steps per second:  74, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.754 [0.000, 3.000],  loss: 0.001849, mae: 0.943293, mean_q: 1.273004, mean_eps: 0.420011\n",
      " 644935/1000000: episode: 2566, duration: 3.794s, episode steps: 261, steps per second:  69, episode reward:  4.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.586 [0.000, 3.000],  loss: 0.001706, mae: 0.945368, mean_q: 1.275990, mean_eps: 0.419676\n",
      " 645316/1000000: episode: 2567, duration: 5.081s, episode steps: 381, steps per second:  75, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.606 [0.000, 3.000],  loss: 0.002063, mae: 0.942797, mean_q: 1.272489, mean_eps: 0.419388\n",
      " 645790/1000000: episode: 2568, duration: 6.409s, episode steps: 474, steps per second:  74, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.719 [0.000, 3.000],  loss: 0.001807, mae: 0.945980, mean_q: 1.275464, mean_eps: 0.419003\n",
      " 646334/1000000: episode: 2569, duration: 7.518s, episode steps: 544, steps per second:  72, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.706 [0.000, 3.000],  loss: 0.001713, mae: 0.940658, mean_q: 1.270300, mean_eps: 0.418544\n",
      " 646944/1000000: episode: 2570, duration: 8.243s, episode steps: 610, steps per second:  74, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.720 [0.000, 3.000],  loss: 0.001633, mae: 0.943541, mean_q: 1.273164, mean_eps: 0.418026\n",
      " 647338/1000000: episode: 2571, duration: 5.484s, episode steps: 394, steps per second:  72, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.602 [0.000, 3.000],  loss: 0.001731, mae: 0.949511, mean_q: 1.280727, mean_eps: 0.417574\n",
      " 647679/1000000: episode: 2572, duration: 4.650s, episode steps: 341, steps per second:  73, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 0.001627, mae: 0.939775, mean_q: 1.268140, mean_eps: 0.417243\n",
      " 648157/1000000: episode: 2573, duration: 6.525s, episode steps: 478, steps per second:  73, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.701 [0.000, 3.000],  loss: 0.001677, mae: 0.942060, mean_q: 1.269951, mean_eps: 0.416874\n",
      " 648435/1000000: episode: 2574, duration: 3.906s, episode steps: 278, steps per second:  71, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.669 [0.000, 3.000],  loss: 0.001791, mae: 0.947185, mean_q: 1.276250, mean_eps: 0.416534\n",
      " 648901/1000000: episode: 2575, duration: 6.459s, episode steps: 466, steps per second:  72, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.704 [0.000, 3.000],  loss: 0.001686, mae: 0.944619, mean_q: 1.274302, mean_eps: 0.416199\n",
      " 649421/1000000: episode: 2576, duration: 7.165s, episode steps: 520, steps per second:  73, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.781 [0.000, 3.000],  loss: 0.001621, mae: 0.943873, mean_q: 1.275593, mean_eps: 0.415754\n",
      " 649892/1000000: episode: 2577, duration: 6.399s, episode steps: 471, steps per second:  74, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: 0.001773, mae: 0.944098, mean_q: 1.272676, mean_eps: 0.415310\n",
      " 650362/1000000: episode: 2578, duration: 6.450s, episode steps: 470, steps per second:  73, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.804 [0.000, 3.000],  loss: 0.002017, mae: 0.955511, mean_q: 1.288635, mean_eps: 0.414887\n",
      " 650826/1000000: episode: 2579, duration: 6.365s, episode steps: 464, steps per second:  73, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.573 [0.000, 3.000],  loss: 0.002104, mae: 0.953573, mean_q: 1.286156, mean_eps: 0.414465\n",
      " 651268/1000000: episode: 2580, duration: 5.989s, episode steps: 442, steps per second:  74, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.001932, mae: 0.947880, mean_q: 1.279156, mean_eps: 0.414059\n",
      " 651726/1000000: episode: 2581, duration: 6.334s, episode steps: 458, steps per second:  72, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.762 [0.000, 3.000],  loss: 0.002019, mae: 0.957132, mean_q: 1.290544, mean_eps: 0.413654\n",
      " 651978/1000000: episode: 2582, duration: 3.487s, episode steps: 252, steps per second:  72, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: 0.001796, mae: 0.953622, mean_q: 1.283736, mean_eps: 0.413333\n",
      " 652526/1000000: episode: 2583, duration: 7.397s, episode steps: 548, steps per second:  74, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.002056, mae: 0.952558, mean_q: 1.283944, mean_eps: 0.412973\n",
      " 653020/1000000: episode: 2584, duration: 6.996s, episode steps: 494, steps per second:  71, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.670 [0.000, 3.000],  loss: 0.001913, mae: 0.955170, mean_q: 1.286721, mean_eps: 0.412505\n",
      " 653580/1000000: episode: 2585, duration: 7.718s, episode steps: 560, steps per second:  73, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: 0.001596, mae: 0.951847, mean_q: 1.284837, mean_eps: 0.412032\n",
      " 654418/1000000: episode: 2586, duration: 11.691s, episode steps: 838, steps per second:  72, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.663 [0.000, 3.000],  loss: 0.001961, mae: 0.953528, mean_q: 1.285835, mean_eps: 0.411402\n",
      " 654725/1000000: episode: 2587, duration: 4.205s, episode steps: 307, steps per second:  73, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.368 [0.000, 3.000],  loss: 0.002161, mae: 0.958517, mean_q: 1.292056, mean_eps: 0.410885\n",
      " 655385/1000000: episode: 2588, duration: 9.049s, episode steps: 660, steps per second:  73, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.695 [0.000, 3.000],  loss: 0.001641, mae: 0.952973, mean_q: 1.285582, mean_eps: 0.410450\n",
      " 655768/1000000: episode: 2589, duration: 5.161s, episode steps: 383, steps per second:  74, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.569 [0.000, 3.000],  loss: 0.001513, mae: 0.949707, mean_q: 1.283949, mean_eps: 0.409982\n",
      " 656240/1000000: episode: 2590, duration: 6.516s, episode steps: 472, steps per second:  72, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.466 [0.000, 3.000],  loss: 0.001595, mae: 0.954596, mean_q: 1.289798, mean_eps: 0.409598\n",
      " 656789/1000000: episode: 2591, duration: 7.708s, episode steps: 549, steps per second:  71, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.001636, mae: 0.952797, mean_q: 1.285419, mean_eps: 0.409137\n",
      " 657153/1000000: episode: 2592, duration: 4.922s, episode steps: 364, steps per second:  74, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.668 [0.000, 3.000],  loss: 0.001530, mae: 0.954742, mean_q: 1.288801, mean_eps: 0.408725\n",
      " 657615/1000000: episode: 2593, duration: 6.499s, episode steps: 462, steps per second:  71, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.001663, mae: 0.956375, mean_q: 1.289818, mean_eps: 0.408354\n",
      " 657971/1000000: episode: 2594, duration: 4.725s, episode steps: 356, steps per second:  75, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.761 [0.000, 3.000],  loss: 0.001459, mae: 0.958448, mean_q: 1.292864, mean_eps: 0.407987\n",
      " 658642/1000000: episode: 2595, duration: 9.166s, episode steps: 671, steps per second:  73, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.726 [0.000, 3.000],  loss: 0.001878, mae: 0.955295, mean_q: 1.289483, mean_eps: 0.407525\n",
      " 659022/1000000: episode: 2596, duration: 5.371s, episode steps: 380, steps per second:  71, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 0.001604, mae: 0.949496, mean_q: 1.281298, mean_eps: 0.407051\n",
      " 659385/1000000: episode: 2597, duration: 5.057s, episode steps: 363, steps per second:  72, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.702 [0.000, 3.000],  loss: 0.001845, mae: 0.952940, mean_q: 1.287243, mean_eps: 0.406716\n",
      " 660191/1000000: episode: 2598, duration: 11.157s, episode steps: 806, steps per second:  72, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.625 [0.000, 3.000],  loss: 0.001854, mae: 0.960343, mean_q: 1.296202, mean_eps: 0.406191\n",
      " 660613/1000000: episode: 2599, duration: 5.841s, episode steps: 422, steps per second:  72, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.773 [0.000, 3.000],  loss: 0.002469, mae: 0.963726, mean_q: 1.298282, mean_eps: 0.405638\n",
      " 661036/1000000: episode: 2600, duration: 6.036s, episode steps: 423, steps per second:  70, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.619 [0.000, 3.000],  loss: 0.002392, mae: 0.962992, mean_q: 1.296285, mean_eps: 0.405258\n",
      " 661665/1000000: episode: 2601, duration: 8.792s, episode steps: 629, steps per second:  72, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.671 [0.000, 3.000],  loss: 0.002098, mae: 0.968158, mean_q: 1.307938, mean_eps: 0.404785\n",
      " 662034/1000000: episode: 2602, duration: 5.013s, episode steps: 369, steps per second:  74, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.726 [0.000, 3.000],  loss: 0.002306, mae: 0.963666, mean_q: 1.301562, mean_eps: 0.404335\n",
      " 662454/1000000: episode: 2603, duration: 5.950s, episode steps: 420, steps per second:  71, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.614 [0.000, 3.000],  loss: 0.002117, mae: 0.970827, mean_q: 1.309730, mean_eps: 0.403980\n",
      " 662983/1000000: episode: 2604, duration: 7.244s, episode steps: 529, steps per second:  73, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.724 [0.000, 3.000],  loss: 0.001877, mae: 0.971661, mean_q: 1.310517, mean_eps: 0.403554\n",
      " 663499/1000000: episode: 2605, duration: 7.307s, episode steps: 516, steps per second:  71, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.709 [0.000, 3.000],  loss: 0.001722, mae: 0.975462, mean_q: 1.317458, mean_eps: 0.403084\n",
      " 664005/1000000: episode: 2606, duration: 7.000s, episode steps: 506, steps per second:  72, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: 0.001699, mae: 0.966550, mean_q: 1.306457, mean_eps: 0.402623\n",
      " 664686/1000000: episode: 2607, duration: 9.598s, episode steps: 681, steps per second:  71, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.712 [0.000, 3.000],  loss: 0.001914, mae: 0.967386, mean_q: 1.306377, mean_eps: 0.402089\n",
      " 665192/1000000: episode: 2608, duration: 7.047s, episode steps: 506, steps per second:  72, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.686 [0.000, 3.000],  loss: 0.002261, mae: 0.962798, mean_q: 1.299932, mean_eps: 0.401556\n",
      " 665580/1000000: episode: 2609, duration: 5.495s, episode steps: 388, steps per second:  71, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.704 [0.000, 3.000],  loss: 0.001920, mae: 0.968106, mean_q: 1.308729, mean_eps: 0.401154\n",
      " 666265/1000000: episode: 2610, duration: 9.463s, episode steps: 685, steps per second:  72, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.002086, mae: 0.959432, mean_q: 1.295168, mean_eps: 0.400670\n",
      " 666815/1000000: episode: 2611, duration: 7.790s, episode steps: 550, steps per second:  71, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.525 [0.000, 3.000],  loss: 0.002148, mae: 0.964016, mean_q: 1.300704, mean_eps: 0.400114\n",
      " 667160/1000000: episode: 2612, duration: 4.763s, episode steps: 345, steps per second:  72, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.612 [0.000, 3.000],  loss: 0.001947, mae: 0.953289, mean_q: 1.287614, mean_eps: 0.399713\n",
      " 667504/1000000: episode: 2613, duration: 4.766s, episode steps: 344, steps per second:  72, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: 0.001633, mae: 0.964643, mean_q: 1.303044, mean_eps: 0.399403\n",
      " 667886/1000000: episode: 2614, duration: 5.589s, episode steps: 382, steps per second:  68, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.427 [0.000, 3.000],  loss: 0.002004, mae: 0.976829, mean_q: 1.317429, mean_eps: 0.399075\n",
      " 668218/1000000: episode: 2615, duration: 4.572s, episode steps: 332, steps per second:  73, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.611 [0.000, 3.000],  loss: 0.001815, mae: 0.965051, mean_q: 1.303534, mean_eps: 0.398753\n",
      " 668773/1000000: episode: 2616, duration: 7.640s, episode steps: 555, steps per second:  73, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.708 [0.000, 3.000],  loss: 0.001928, mae: 0.966625, mean_q: 1.305330, mean_eps: 0.398354\n",
      " 669183/1000000: episode: 2617, duration: 5.874s, episode steps: 410, steps per second:  70, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 0.001951, mae: 0.965295, mean_q: 1.301616, mean_eps: 0.397920\n",
      " 669594/1000000: episode: 2618, duration: 5.713s, episode steps: 411, steps per second:  72, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 0.002136, mae: 0.965006, mean_q: 1.302697, mean_eps: 0.397551\n",
      " 670090/1000000: episode: 2619, duration: 6.805s, episode steps: 496, steps per second:  73, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.524 [0.000, 3.000],  loss: 0.001843, mae: 0.968741, mean_q: 1.306991, mean_eps: 0.397142\n",
      " 670436/1000000: episode: 2620, duration: 5.003s, episode steps: 346, steps per second:  69, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.002224, mae: 0.978007, mean_q: 1.316988, mean_eps: 0.396764\n",
      " 671021/1000000: episode: 2621, duration: 8.168s, episode steps: 585, steps per second:  72, episode reward: 15.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.761 [0.000, 3.000],  loss: 0.002064, mae: 0.987991, mean_q: 1.333288, mean_eps: 0.396345\n",
      " 671418/1000000: episode: 2622, duration: 5.720s, episode steps: 397, steps per second:  69, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.720 [0.000, 3.000],  loss: 0.001970, mae: 0.978504, mean_q: 1.320710, mean_eps: 0.395902\n",
      " 671870/1000000: episode: 2623, duration: 6.377s, episode steps: 452, steps per second:  71, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.712 [0.000, 3.000],  loss: 0.002108, mae: 0.982088, mean_q: 1.325797, mean_eps: 0.395520\n",
      " 672218/1000000: episode: 2624, duration: 4.772s, episode steps: 348, steps per second:  73, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.710 [0.000, 3.000],  loss: 0.002061, mae: 0.980026, mean_q: 1.324073, mean_eps: 0.395160\n",
      " 672644/1000000: episode: 2625, duration: 6.103s, episode steps: 426, steps per second:  70, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.838 [0.000, 3.000],  loss: 0.001725, mae: 0.980855, mean_q: 1.322566, mean_eps: 0.394813\n",
      " 673167/1000000: episode: 2626, duration: 7.280s, episode steps: 523, steps per second:  72, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.769 [0.000, 3.000],  loss: 0.001626, mae: 0.978555, mean_q: 1.319682, mean_eps: 0.394386\n",
      " 673624/1000000: episode: 2627, duration: 6.620s, episode steps: 457, steps per second:  69, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002016, mae: 0.981622, mean_q: 1.324139, mean_eps: 0.393945\n",
      " 674131/1000000: episode: 2628, duration: 6.982s, episode steps: 507, steps per second:  73, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.576 [0.000, 3.000],  loss: 0.001665, mae: 0.978772, mean_q: 1.321189, mean_eps: 0.393512\n",
      " 674590/1000000: episode: 2629, duration: 6.394s, episode steps: 459, steps per second:  72, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.782 [0.000, 3.000],  loss: 0.001720, mae: 0.979806, mean_q: 1.322933, mean_eps: 0.393076\n",
      " 675326/1000000: episode: 2630, duration: 10.325s, episode steps: 736, steps per second:  71, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.629 [0.000, 3.000],  loss: 0.001616, mae: 0.978906, mean_q: 1.321588, mean_eps: 0.392538\n",
      " 675826/1000000: episode: 2631, duration: 7.055s, episode steps: 500, steps per second:  71, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.604 [0.000, 3.000],  loss: 0.001709, mae: 0.976540, mean_q: 1.317149, mean_eps: 0.391982\n",
      " 676397/1000000: episode: 2632, duration: 8.153s, episode steps: 571, steps per second:  70, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.553 [0.000, 3.000],  loss: 0.001739, mae: 0.980964, mean_q: 1.323429, mean_eps: 0.391499\n",
      " 676684/1000000: episode: 2633, duration: 4.193s, episode steps: 287, steps per second:  68, episode reward:  4.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.679 [0.000, 3.000],  loss: 0.001667, mae: 0.980659, mean_q: 1.322169, mean_eps: 0.391114\n",
      " 677131/1000000: episode: 2634, duration: 6.357s, episode steps: 447, steps per second:  70, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.001563, mae: 0.982961, mean_q: 1.327282, mean_eps: 0.390785\n",
      " 677552/1000000: episode: 2635, duration: 5.995s, episode steps: 421, steps per second:  70, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.713 [0.000, 3.000],  loss: 0.001758, mae: 0.980249, mean_q: 1.324503, mean_eps: 0.390394\n",
      " 678065/1000000: episode: 2636, duration: 7.254s, episode steps: 513, steps per second:  71, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.739 [0.000, 3.000],  loss: 0.001844, mae: 0.978027, mean_q: 1.320838, mean_eps: 0.389973\n",
      " 678601/1000000: episode: 2637, duration: 7.499s, episode steps: 536, steps per second:  71, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.636 [0.000, 3.000],  loss: 0.001968, mae: 0.978682, mean_q: 1.321431, mean_eps: 0.389499\n",
      " 678838/1000000: episode: 2638, duration: 3.392s, episode steps: 237, steps per second:  70, episode reward:  3.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.873 [0.000, 3.000],  loss: 0.001778, mae: 0.977837, mean_q: 1.319589, mean_eps: 0.389152\n",
      " 679294/1000000: episode: 2639, duration: 6.465s, episode steps: 456, steps per second:  71, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: 0.002194, mae: 0.976600, mean_q: 1.316948, mean_eps: 0.388841\n",
      " 679734/1000000: episode: 2640, duration: 6.187s, episode steps: 440, steps per second:  71, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.698 [0.000, 3.000],  loss: 0.001759, mae: 0.983336, mean_q: 1.327219, mean_eps: 0.388437\n",
      " 679991/1000000: episode: 2641, duration: 3.643s, episode steps: 257, steps per second:  71, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.510 [0.000, 3.000],  loss: 0.001888, mae: 0.983734, mean_q: 1.327277, mean_eps: 0.388124\n",
      " 680573/1000000: episode: 2642, duration: 8.194s, episode steps: 582, steps per second:  71, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.002009, mae: 0.997277, mean_q: 1.345348, mean_eps: 0.387746\n",
      " 681026/1000000: episode: 2643, duration: 6.336s, episode steps: 453, steps per second:  71, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.662 [0.000, 3.000],  loss: 0.002271, mae: 0.999356, mean_q: 1.349632, mean_eps: 0.387280\n",
      " 681935/1000000: episode: 2644, duration: 12.795s, episode steps: 909, steps per second:  71, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.706 [0.000, 3.000],  loss: 0.002178, mae: 0.993043, mean_q: 1.341089, mean_eps: 0.386668\n",
      " 682444/1000000: episode: 2645, duration: 7.202s, episode steps: 509, steps per second:  71, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.823 [0.000, 3.000],  loss: 0.002110, mae: 0.995779, mean_q: 1.344148, mean_eps: 0.386031\n",
      " 683049/1000000: episode: 2646, duration: 8.637s, episode steps: 605, steps per second:  70, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.689 [0.000, 3.000],  loss: 0.001903, mae: 0.993775, mean_q: 1.340833, mean_eps: 0.385529\n",
      " 683392/1000000: episode: 2647, duration: 4.733s, episode steps: 343, steps per second:  72, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.644 [0.000, 3.000],  loss: 0.001772, mae: 0.988794, mean_q: 1.334642, mean_eps: 0.385102\n",
      " 683828/1000000: episode: 2648, duration: 6.244s, episode steps: 436, steps per second:  70, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.001851, mae: 0.997646, mean_q: 1.346124, mean_eps: 0.384753\n",
      " 684294/1000000: episode: 2649, duration: 6.469s, episode steps: 466, steps per second:  72, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.575 [0.000, 3.000],  loss: 0.001683, mae: 0.997412, mean_q: 1.346842, mean_eps: 0.384346\n",
      " 684745/1000000: episode: 2650, duration: 6.320s, episode steps: 451, steps per second:  71, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.670 [0.000, 3.000],  loss: 0.001752, mae: 0.988594, mean_q: 1.334107, mean_eps: 0.383932\n",
      " 685618/1000000: episode: 2651, duration: 12.366s, episode steps: 873, steps per second:  71, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.679 [0.000, 3.000],  loss: 0.001879, mae: 0.994995, mean_q: 1.344093, mean_eps: 0.383336\n",
      " 686264/1000000: episode: 2652, duration: 9.160s, episode steps: 646, steps per second:  71, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.002151, mae: 1.001100, mean_q: 1.351930, mean_eps: 0.382654\n",
      " 686829/1000000: episode: 2653, duration: 8.012s, episode steps: 565, steps per second:  71, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.001803, mae: 1.001542, mean_q: 1.352233, mean_eps: 0.382109\n",
      " 687376/1000000: episode: 2654, duration: 7.715s, episode steps: 547, steps per second:  71, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.737 [0.000, 3.000],  loss: 0.001717, mae: 0.992518, mean_q: 1.341953, mean_eps: 0.381608\n",
      " 687964/1000000: episode: 2655, duration: 8.313s, episode steps: 588, steps per second:  71, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.001879, mae: 0.996328, mean_q: 1.346130, mean_eps: 0.381099\n",
      " 688893/1000000: episode: 2656, duration: 13.194s, episode steps: 929, steps per second:  70, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.674 [0.000, 3.000],  loss: 0.001563, mae: 0.997833, mean_q: 1.348520, mean_eps: 0.380415\n",
      " 689379/1000000: episode: 2657, duration: 6.753s, episode steps: 486, steps per second:  72, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.002166, mae: 1.003418, mean_q: 1.356659, mean_eps: 0.379778\n",
      " 690046/1000000: episode: 2658, duration: 9.351s, episode steps: 667, steps per second:  71, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.001786, mae: 1.000318, mean_q: 1.352092, mean_eps: 0.379259\n",
      " 690679/1000000: episode: 2659, duration: 9.028s, episode steps: 633, steps per second:  70, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.703 [0.000, 3.000],  loss: 0.002518, mae: 1.012153, mean_q: 1.366784, mean_eps: 0.378674\n",
      " 691258/1000000: episode: 2660, duration: 8.112s, episode steps: 579, steps per second:  71, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.737 [0.000, 3.000],  loss: 0.002155, mae: 1.005743, mean_q: 1.361043, mean_eps: 0.378129\n",
      " 691714/1000000: episode: 2661, duration: 6.592s, episode steps: 456, steps per second:  69, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.636 [0.000, 3.000],  loss: 0.001978, mae: 1.008093, mean_q: 1.363038, mean_eps: 0.377663\n",
      " 692036/1000000: episode: 2662, duration: 4.631s, episode steps: 322, steps per second:  70, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.466 [0.000, 3.000],  loss: 0.001911, mae: 1.002887, mean_q: 1.357882, mean_eps: 0.377313\n",
      " 692580/1000000: episode: 2663, duration: 7.583s, episode steps: 544, steps per second:  72, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.697 [0.000, 3.000],  loss: 0.002206, mae: 1.004952, mean_q: 1.357387, mean_eps: 0.376925\n",
      " 693382/1000000: episode: 2664, duration: 11.844s, episode steps: 802, steps per second:  68, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.743 [0.000, 3.000],  loss: 0.002468, mae: 1.004872, mean_q: 1.358683, mean_eps: 0.376318\n",
      " 693860/1000000: episode: 2665, duration: 6.882s, episode steps: 478, steps per second:  69, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.713 [0.000, 3.000],  loss: 0.002183, mae: 0.998654, mean_q: 1.351683, mean_eps: 0.375742\n",
      " 694585/1000000: episode: 2666, duration: 10.265s, episode steps: 725, steps per second:  71, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.513 [0.000, 3.000],  loss: 0.002171, mae: 1.008415, mean_q: 1.363765, mean_eps: 0.375200\n",
      " 694954/1000000: episode: 2667, duration: 5.188s, episode steps: 369, steps per second:  71, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.640 [0.000, 3.000],  loss: 0.002281, mae: 1.005394, mean_q: 1.360027, mean_eps: 0.374707\n",
      " 695382/1000000: episode: 2668, duration: 6.267s, episode steps: 428, steps per second:  68, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.605 [0.000, 3.000],  loss: 0.001882, mae: 1.001405, mean_q: 1.357687, mean_eps: 0.374349\n",
      " 696000/1000000: episode: 2669, duration: 8.765s, episode steps: 618, steps per second:  71, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.803 [0.000, 3.000],  loss: 0.001920, mae: 1.004786, mean_q: 1.358992, mean_eps: 0.373879\n",
      " 696659/1000000: episode: 2670, duration: 9.385s, episode steps: 659, steps per second:  70, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.734 [0.000, 3.000],  loss: 0.002187, mae: 1.011729, mean_q: 1.368727, mean_eps: 0.373305\n",
      " 697038/1000000: episode: 2671, duration: 5.401s, episode steps: 379, steps per second:  70, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.001957, mae: 1.010524, mean_q: 1.368098, mean_eps: 0.372837\n",
      " 697566/1000000: episode: 2672, duration: 7.582s, episode steps: 528, steps per second:  70, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.684 [0.000, 3.000],  loss: 0.002104, mae: 1.010331, mean_q: 1.367239, mean_eps: 0.372428\n",
      " 698115/1000000: episode: 2673, duration: 7.686s, episode steps: 549, steps per second:  71, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.001754, mae: 1.008038, mean_q: 1.363246, mean_eps: 0.371944\n",
      " 698455/1000000: episode: 2674, duration: 4.937s, episode steps: 340, steps per second:  69, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.665 [0.000, 3.000],  loss: 0.001840, mae: 1.014867, mean_q: 1.373611, mean_eps: 0.371544\n",
      " 698893/1000000: episode: 2675, duration: 6.275s, episode steps: 438, steps per second:  70, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.671 [0.000, 3.000],  loss: 0.002005, mae: 1.003424, mean_q: 1.356856, mean_eps: 0.371193\n",
      " 699334/1000000: episode: 2676, duration: 6.190s, episode steps: 441, steps per second:  71, episode reward: 10.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.603 [0.000, 3.000],  loss: 0.001987, mae: 1.009382, mean_q: 1.364016, mean_eps: 0.370797\n",
      " 699678/1000000: episode: 2677, duration: 5.038s, episode steps: 344, steps per second:  68, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.001868, mae: 0.998172, mean_q: 1.351140, mean_eps: 0.370445\n",
      " 700157/1000000: episode: 2678, duration: 6.811s, episode steps: 479, steps per second:  70, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.722 [0.000, 3.000],  loss: 0.002540, mae: 1.016060, mean_q: 1.372907, mean_eps: 0.370074\n",
      " 700912/1000000: episode: 2679, duration: 10.810s, episode steps: 755, steps per second:  70, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.743 [0.000, 3.000],  loss: 0.002751, mae: 1.030450, mean_q: 1.393517, mean_eps: 0.369519\n",
      " 701374/1000000: episode: 2680, duration: 6.540s, episode steps: 462, steps per second:  71, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.714 [0.000, 3.000],  loss: 0.002551, mae: 1.033848, mean_q: 1.398966, mean_eps: 0.368972\n",
      " 702085/1000000: episode: 2681, duration: 10.152s, episode steps: 711, steps per second:  70, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.788 [0.000, 3.000],  loss: 0.002129, mae: 1.029103, mean_q: 1.392504, mean_eps: 0.368443\n",
      " 702633/1000000: episode: 2682, duration: 7.710s, episode steps: 548, steps per second:  71, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.002357, mae: 1.037512, mean_q: 1.403228, mean_eps: 0.367876\n",
      " 703089/1000000: episode: 2683, duration: 6.630s, episode steps: 456, steps per second:  69, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.912 [0.000, 3.000],  loss: 0.002257, mae: 1.027451, mean_q: 1.390960, mean_eps: 0.367424\n",
      " 703495/1000000: episode: 2684, duration: 5.772s, episode steps: 406, steps per second:  70, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.002350, mae: 1.033608, mean_q: 1.400354, mean_eps: 0.367037\n",
      " 704082/1000000: episode: 2685, duration: 8.450s, episode steps: 587, steps per second:  69, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.595 [0.000, 3.000],  loss: 0.002400, mae: 1.030363, mean_q: 1.394705, mean_eps: 0.366591\n",
      " 704754/1000000: episode: 2686, duration: 9.614s, episode steps: 672, steps per second:  70, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.728 [0.000, 3.000],  loss: 0.002177, mae: 1.032072, mean_q: 1.399207, mean_eps: 0.366024\n",
      " 705299/1000000: episode: 2687, duration: 7.865s, episode steps: 545, steps per second:  69, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.708 [0.000, 3.000],  loss: 0.002310, mae: 1.028272, mean_q: 1.393745, mean_eps: 0.365477\n",
      " 705822/1000000: episode: 2688, duration: 7.461s, episode steps: 523, steps per second:  70, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.616 [0.000, 3.000],  loss: 0.002427, mae: 1.031445, mean_q: 1.397795, mean_eps: 0.364996\n",
      " 706149/1000000: episode: 2689, duration: 4.703s, episode steps: 327, steps per second:  70, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.682 [0.000, 3.000],  loss: 0.002245, mae: 1.026834, mean_q: 1.390492, mean_eps: 0.364613\n",
      " 706809/1000000: episode: 2690, duration: 10.170s, episode steps: 660, steps per second:  65, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.748 [0.000, 3.000],  loss: 0.002540, mae: 1.026129, mean_q: 1.390745, mean_eps: 0.364168\n",
      " 707515/1000000: episode: 2691, duration: 10.307s, episode steps: 706, steps per second:  69, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.647 [0.000, 3.000],  loss: 0.002055, mae: 1.041279, mean_q: 1.409725, mean_eps: 0.363554\n",
      " 707976/1000000: episode: 2692, duration: 6.718s, episode steps: 461, steps per second:  69, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.733 [0.000, 3.000],  loss: 0.002451, mae: 1.028606, mean_q: 1.396239, mean_eps: 0.363030\n",
      " 708375/1000000: episode: 2693, duration: 5.838s, episode steps: 399, steps per second:  68, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.002571, mae: 1.027693, mean_q: 1.391400, mean_eps: 0.362643\n",
      " 709145/1000000: episode: 2694, duration: 11.156s, episode steps: 770, steps per second:  69, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.681 [0.000, 3.000],  loss: 0.002371, mae: 1.029847, mean_q: 1.396287, mean_eps: 0.362116\n",
      " 709634/1000000: episode: 2695, duration: 7.154s, episode steps: 489, steps per second:  68, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.675 [0.000, 3.000],  loss: 0.002424, mae: 1.020099, mean_q: 1.384105, mean_eps: 0.361549\n",
      " 709957/1000000: episode: 2696, duration: 4.616s, episode steps: 323, steps per second:  70, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.749 [0.000, 3.000],  loss: 0.002074, mae: 1.028873, mean_q: 1.395817, mean_eps: 0.361184\n",
      " 710468/1000000: episode: 2697, duration: 7.337s, episode steps: 511, steps per second:  70, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.452 [0.000, 3.000],  loss: 0.002870, mae: 1.045017, mean_q: 1.414259, mean_eps: 0.360809\n",
      " 711196/1000000: episode: 2698, duration: 10.607s, episode steps: 728, steps per second:  69, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.710 [0.000, 3.000],  loss: 0.002006, mae: 1.045921, mean_q: 1.416077, mean_eps: 0.360253\n",
      " 711934/1000000: episode: 2699, duration: 10.649s, episode steps: 738, steps per second:  69, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.497 [0.000, 3.000],  loss: 0.002648, mae: 1.045374, mean_q: 1.417820, mean_eps: 0.359592\n",
      " 712467/1000000: episode: 2700, duration: 7.593s, episode steps: 533, steps per second:  70, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: 0.002476, mae: 1.045093, mean_q: 1.417292, mean_eps: 0.359020\n",
      " 713053/1000000: episode: 2701, duration: 8.569s, episode steps: 586, steps per second:  68, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.633 [0.000, 3.000],  loss: 0.002388, mae: 1.050593, mean_q: 1.422492, mean_eps: 0.358516\n",
      " 713703/1000000: episode: 2702, duration: 9.394s, episode steps: 650, steps per second:  69, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.002481, mae: 1.043101, mean_q: 1.414996, mean_eps: 0.357960\n",
      " 714432/1000000: episode: 2703, duration: 10.534s, episode steps: 729, steps per second:  69, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.664 [0.000, 3.000],  loss: 0.002386, mae: 1.039479, mean_q: 1.408804, mean_eps: 0.357341\n",
      " 715014/1000000: episode: 2704, duration: 8.546s, episode steps: 582, steps per second:  68, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 0.002091, mae: 1.043404, mean_q: 1.416017, mean_eps: 0.356750\n",
      " 715662/1000000: episode: 2705, duration: 9.479s, episode steps: 648, steps per second:  68, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.781 [0.000, 3.000],  loss: 0.002169, mae: 1.042301, mean_q: 1.415001, mean_eps: 0.356196\n",
      " 716426/1000000: episode: 2706, duration: 11.187s, episode steps: 764, steps per second:  68, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 0.002406, mae: 1.051606, mean_q: 1.427938, mean_eps: 0.355560\n",
      " 716727/1000000: episode: 2707, duration: 4.387s, episode steps: 301, steps per second:  69, episode reward:  6.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.002628, mae: 1.036550, mean_q: 1.406911, mean_eps: 0.355082\n",
      " 717343/1000000: episode: 2708, duration: 8.952s, episode steps: 616, steps per second:  69, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.002190, mae: 1.038102, mean_q: 1.407881, mean_eps: 0.354669\n",
      " 717972/1000000: episode: 2709, duration: 9.065s, episode steps: 629, steps per second:  69, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.684 [0.000, 3.000],  loss: 0.001961, mae: 1.039905, mean_q: 1.410825, mean_eps: 0.354110\n",
      " 718418/1000000: episode: 2710, duration: 6.643s, episode steps: 446, steps per second:  67, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.002177, mae: 1.037093, mean_q: 1.408422, mean_eps: 0.353625\n",
      " 718731/1000000: episode: 2711, duration: 4.542s, episode steps: 313, steps per second:  69, episode reward:  5.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.476 [0.000, 3.000],  loss: 0.002268, mae: 1.037847, mean_q: 1.409560, mean_eps: 0.353283\n",
      " 719215/1000000: episode: 2712, duration: 6.854s, episode steps: 484, steps per second:  71, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.002592, mae: 1.043989, mean_q: 1.417970, mean_eps: 0.352925\n",
      " 719971/1000000: episode: 2713, duration: 11.056s, episode steps: 756, steps per second:  68, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.791 [0.000, 3.000],  loss: 0.002390, mae: 1.045864, mean_q: 1.419405, mean_eps: 0.352367\n",
      " 720562/1000000: episode: 2714, duration: 8.713s, episode steps: 591, steps per second:  68, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.873 [0.000, 3.000],  loss: 0.002606, mae: 1.057392, mean_q: 1.431694, mean_eps: 0.351761\n",
      " 721235/1000000: episode: 2715, duration: 9.693s, episode steps: 673, steps per second:  69, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: 0.002436, mae: 1.061349, mean_q: 1.436721, mean_eps: 0.351192\n",
      " 721843/1000000: episode: 2716, duration: 8.928s, episode steps: 608, steps per second:  68, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.497 [0.000, 3.000],  loss: 0.002315, mae: 1.064126, mean_q: 1.442137, mean_eps: 0.350616\n",
      " 722413/1000000: episode: 2717, duration: 8.303s, episode steps: 570, steps per second:  69, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.677 [0.000, 3.000],  loss: 0.002102, mae: 1.068681, mean_q: 1.446497, mean_eps: 0.350085\n",
      " 722954/1000000: episode: 2718, duration: 7.981s, episode steps: 541, steps per second:  68, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.741 [0.000, 3.000],  loss: 0.002227, mae: 1.054472, mean_q: 1.427362, mean_eps: 0.349584\n",
      " 723438/1000000: episode: 2719, duration: 7.016s, episode steps: 484, steps per second:  69, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.818 [0.000, 3.000],  loss: 0.002205, mae: 1.068980, mean_q: 1.448179, mean_eps: 0.349124\n",
      " 723993/1000000: episode: 2720, duration: 8.172s, episode steps: 555, steps per second:  68, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.748 [0.000, 3.000],  loss: 0.002821, mae: 1.060059, mean_q: 1.435537, mean_eps: 0.348656\n",
      " 724296/1000000: episode: 2721, duration: 4.397s, episode steps: 303, steps per second:  69, episode reward:  5.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.485 [0.000, 3.000],  loss: 0.002434, mae: 1.061796, mean_q: 1.435528, mean_eps: 0.348270\n",
      " 724739/1000000: episode: 2722, duration: 6.434s, episode steps: 443, steps per second:  69, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.564 [0.000, 3.000],  loss: 0.002485, mae: 1.062053, mean_q: 1.438136, mean_eps: 0.347936\n",
      " 725556/1000000: episode: 2723, duration: 11.988s, episode steps: 817, steps per second:  68, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.665 [0.000, 3.000],  loss: 0.002355, mae: 1.060530, mean_q: 1.434945, mean_eps: 0.347369\n",
      " 726196/1000000: episode: 2724, duration: 9.540s, episode steps: 640, steps per second:  67, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.806 [0.000, 3.000],  loss: 0.002216, mae: 1.064827, mean_q: 1.441590, mean_eps: 0.346713\n",
      " 726647/1000000: episode: 2725, duration: 6.569s, episode steps: 451, steps per second:  69, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.681 [0.000, 3.000],  loss: 0.002149, mae: 1.058245, mean_q: 1.433166, mean_eps: 0.346222\n",
      " 726887/1000000: episode: 2726, duration: 3.498s, episode steps: 240, steps per second:  69, episode reward:  4.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.508 [0.000, 3.000],  loss: 0.002615, mae: 1.061581, mean_q: 1.441258, mean_eps: 0.345911\n",
      " 727461/1000000: episode: 2727, duration: 8.562s, episode steps: 574, steps per second:  67, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.834 [0.000, 3.000],  loss: 0.002247, mae: 1.055585, mean_q: 1.430202, mean_eps: 0.345543\n",
      " 728090/1000000: episode: 2728, duration: 9.275s, episode steps: 629, steps per second:  68, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.591 [0.000, 3.000],  loss: 0.002365, mae: 1.065687, mean_q: 1.443004, mean_eps: 0.345002\n",
      " 728730/1000000: episode: 2729, duration: 9.353s, episode steps: 640, steps per second:  68, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.627 [0.000, 3.000],  loss: 0.002544, mae: 1.063648, mean_q: 1.441212, mean_eps: 0.344431\n",
      " 729015/1000000: episode: 2730, duration: 4.185s, episode steps: 285, steps per second:  68, episode reward:  5.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.611 [0.000, 3.000],  loss: 0.002139, mae: 1.046589, mean_q: 1.419664, mean_eps: 0.344015\n",
      " 729423/1000000: episode: 2731, duration: 6.074s, episode steps: 408, steps per second:  67, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.001931, mae: 1.073268, mean_q: 1.453209, mean_eps: 0.343704\n",
      " 730070/1000000: episode: 2732, duration: 9.442s, episode steps: 647, steps per second:  69, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.802 [0.000, 3.000],  loss: 0.002405, mae: 1.061143, mean_q: 1.439435, mean_eps: 0.343229\n",
      " 730526/1000000: episode: 2733, duration: 6.884s, episode steps: 456, steps per second:  66, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.662 [0.000, 3.000],  loss: 0.002192, mae: 1.088829, mean_q: 1.472449, mean_eps: 0.342732\n",
      " 731160/1000000: episode: 2734, duration: 9.352s, episode steps: 634, steps per second:  68, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.746 [0.000, 3.000],  loss: 0.002327, mae: 1.077603, mean_q: 1.459352, mean_eps: 0.342242\n",
      " 731782/1000000: episode: 2735, duration: 9.281s, episode steps: 622, steps per second:  67, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.641 [0.000, 3.000],  loss: 0.002743, mae: 1.080379, mean_q: 1.461267, mean_eps: 0.341677\n",
      " 732364/1000000: episode: 2736, duration: 8.440s, episode steps: 582, steps per second:  69, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.002443, mae: 1.080071, mean_q: 1.462195, mean_eps: 0.341135\n",
      " 732856/1000000: episode: 2737, duration: 7.478s, episode steps: 492, steps per second:  66, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.699 [0.000, 3.000],  loss: 0.002689, mae: 1.077508, mean_q: 1.458298, mean_eps: 0.340653\n",
      " 733311/1000000: episode: 2738, duration: 6.529s, episode steps: 455, steps per second:  70, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.481 [0.000, 3.000],  loss: 0.003116, mae: 1.081633, mean_q: 1.462678, mean_eps: 0.340226\n",
      " 733821/1000000: episode: 2739, duration: 7.702s, episode steps: 510, steps per second:  66, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.829 [0.000, 3.000],  loss: 0.002934, mae: 1.073874, mean_q: 1.453913, mean_eps: 0.339791\n",
      " 734259/1000000: episode: 2740, duration: 6.392s, episode steps: 438, steps per second:  69, episode reward: 10.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.735 [0.000, 3.000],  loss: 0.002384, mae: 1.085740, mean_q: 1.469267, mean_eps: 0.339364\n",
      " 734675/1000000: episode: 2741, duration: 6.170s, episode steps: 416, steps per second:  67, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.812 [0.000, 3.000],  loss: 0.002191, mae: 1.081399, mean_q: 1.464224, mean_eps: 0.338981\n",
      " 735612/1000000: episode: 2742, duration: 13.805s, episode steps: 937, steps per second:  68, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.751 [0.000, 3.000],  loss: 0.002555, mae: 1.081699, mean_q: 1.465017, mean_eps: 0.338372\n",
      " 736070/1000000: episode: 2743, duration: 6.859s, episode steps: 458, steps per second:  67, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002569, mae: 1.084657, mean_q: 1.465185, mean_eps: 0.337744\n",
      " 736533/1000000: episode: 2744, duration: 6.742s, episode steps: 463, steps per second:  69, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.711 [0.000, 3.000],  loss: 0.002681, mae: 1.086420, mean_q: 1.471337, mean_eps: 0.337328\n",
      " 736875/1000000: episode: 2745, duration: 5.169s, episode steps: 342, steps per second:  66, episode reward:  8.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.895 [0.000, 3.000],  loss: 0.001809, mae: 1.084961, mean_q: 1.469542, mean_eps: 0.336966\n",
      " 737692/1000000: episode: 2746, duration: 12.110s, episode steps: 817, steps per second:  67, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.002440, mae: 1.084005, mean_q: 1.468157, mean_eps: 0.336446\n",
      " 738245/1000000: episode: 2747, duration: 8.284s, episode steps: 553, steps per second:  67, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: 0.002235, mae: 1.088932, mean_q: 1.475478, mean_eps: 0.335829\n",
      " 738628/1000000: episode: 2748, duration: 5.599s, episode steps: 383, steps per second:  68, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.572 [0.000, 3.000],  loss: 0.002451, mae: 1.088114, mean_q: 1.472990, mean_eps: 0.335408\n",
      " 739168/1000000: episode: 2749, duration: 8.374s, episode steps: 540, steps per second:  64, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.643 [0.000, 3.000],  loss: 0.002129, mae: 1.078280, mean_q: 1.459668, mean_eps: 0.334994\n",
      " 739723/1000000: episode: 2750, duration: 8.241s, episode steps: 555, steps per second:  67, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.629 [0.000, 3.000],  loss: 0.002691, mae: 1.084652, mean_q: 1.468228, mean_eps: 0.334500\n",
      " 740347/1000000: episode: 2751, duration: 9.267s, episode steps: 624, steps per second:  67, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.495 [0.000, 3.000],  loss: 0.002851, mae: 1.098675, mean_q: 1.485465, mean_eps: 0.333969\n",
      " 740759/1000000: episode: 2752, duration: 6.170s, episode steps: 412, steps per second:  67, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.609 [0.000, 3.000],  loss: 0.002353, mae: 1.098913, mean_q: 1.487020, mean_eps: 0.333503\n",
      " 741379/1000000: episode: 2753, duration: 9.285s, episode steps: 620, steps per second:  67, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.534 [0.000, 3.000],  loss: 0.002621, mae: 1.105139, mean_q: 1.495872, mean_eps: 0.333039\n",
      " 742047/1000000: episode: 2754, duration: 9.854s, episode steps: 668, steps per second:  68, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.862 [0.000, 3.000],  loss: 0.002694, mae: 1.090200, mean_q: 1.475948, mean_eps: 0.332459\n",
      " 742454/1000000: episode: 2755, duration: 6.127s, episode steps: 407, steps per second:  66, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 0.002341, mae: 1.104441, mean_q: 1.494665, mean_eps: 0.331975\n",
      " 743200/1000000: episode: 2756, duration: 11.047s, episode steps: 746, steps per second:  68, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.720 [0.000, 3.000],  loss: 0.002442, mae: 1.101948, mean_q: 1.492477, mean_eps: 0.331457\n",
      " 743648/1000000: episode: 2757, duration: 6.709s, episode steps: 448, steps per second:  67, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.638 [0.000, 3.000],  loss: 0.002343, mae: 1.086682, mean_q: 1.470979, mean_eps: 0.330920\n",
      " 744340/1000000: episode: 2758, duration: 10.379s, episode steps: 692, steps per second:  67, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.002453, mae: 1.105857, mean_q: 1.495896, mean_eps: 0.330407\n",
      " 744899/1000000: episode: 2759, duration: 8.225s, episode steps: 559, steps per second:  68, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.826 [0.000, 3.000],  loss: 0.002640, mae: 1.093969, mean_q: 1.481950, mean_eps: 0.329844\n",
      " 745613/1000000: episode: 2760, duration: 10.688s, episode steps: 714, steps per second:  67, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.678 [0.000, 3.000],  loss: 0.002345, mae: 1.099149, mean_q: 1.489771, mean_eps: 0.329270\n",
      " 746262/1000000: episode: 2761, duration: 9.584s, episode steps: 649, steps per second:  68, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.846 [0.000, 3.000],  loss: 0.002553, mae: 1.100804, mean_q: 1.488834, mean_eps: 0.328656\n",
      " 746989/1000000: episode: 2762, duration: 11.012s, episode steps: 727, steps per second:  66, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.699 [0.000, 3.000],  loss: 0.002693, mae: 1.092712, mean_q: 1.481326, mean_eps: 0.328037\n",
      " 747557/1000000: episode: 2763, duration: 8.469s, episode steps: 568, steps per second:  67, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.764 [0.000, 3.000],  loss: 0.002077, mae: 1.095472, mean_q: 1.485561, mean_eps: 0.327453\n",
      " 748107/1000000: episode: 2764, duration: 8.139s, episode steps: 550, steps per second:  68, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.645 [0.000, 3.000],  loss: 0.002516, mae: 1.096408, mean_q: 1.486140, mean_eps: 0.326951\n",
      " 748467/1000000: episode: 2765, duration: 5.253s, episode steps: 360, steps per second:  69, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.761 [0.000, 3.000],  loss: 0.002382, mae: 1.100933, mean_q: 1.492101, mean_eps: 0.326543\n",
      " 748956/1000000: episode: 2766, duration: 7.447s, episode steps: 489, steps per second:  66, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.652 [0.000, 3.000],  loss: 0.002345, mae: 1.103231, mean_q: 1.493210, mean_eps: 0.326161\n",
      " 749568/1000000: episode: 2767, duration: 9.183s, episode steps: 612, steps per second:  67, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002148, mae: 1.105037, mean_q: 1.496261, mean_eps: 0.325666\n",
      " 749811/1000000: episode: 2768, duration: 3.737s, episode steps: 243, steps per second:  65, episode reward:  3.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.770 [0.000, 3.000],  loss: 0.002829, mae: 1.104554, mean_q: 1.493806, mean_eps: 0.325281\n",
      " 750286/1000000: episode: 2769, duration: 6.948s, episode steps: 475, steps per second:  68, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.497 [0.000, 3.000],  loss: 0.002547, mae: 1.121256, mean_q: 1.514839, mean_eps: 0.324957\n",
      " 750715/1000000: episode: 2770, duration: 6.460s, episode steps: 429, steps per second:  66, episode reward: 10.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.002818, mae: 1.113811, mean_q: 1.506660, mean_eps: 0.324550\n",
      " 751336/1000000: episode: 2771, duration: 9.210s, episode steps: 621, steps per second:  67, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.786 [0.000, 3.000],  loss: 0.002982, mae: 1.117179, mean_q: 1.511660, mean_eps: 0.324078\n",
      " 751963/1000000: episode: 2772, duration: 9.352s, episode steps: 627, steps per second:  67, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.574 [0.000, 3.000],  loss: 0.002476, mae: 1.116563, mean_q: 1.510520, mean_eps: 0.323517\n",
      " 752872/1000000: episode: 2773, duration: 13.650s, episode steps: 909, steps per second:  67, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.560 [0.000, 3.000],  loss: 0.002410, mae: 1.121331, mean_q: 1.516992, mean_eps: 0.322826\n",
      " 753740/1000000: episode: 2774, duration: 12.925s, episode steps: 868, steps per second:  67, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.661 [0.000, 3.000],  loss: 0.002638, mae: 1.123219, mean_q: 1.521943, mean_eps: 0.322026\n",
      " 754410/1000000: episode: 2775, duration: 10.170s, episode steps: 670, steps per second:  66, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.713 [0.000, 3.000],  loss: 0.002462, mae: 1.115870, mean_q: 1.512998, mean_eps: 0.321333\n",
      " 754951/1000000: episode: 2776, duration: 8.081s, episode steps: 541, steps per second:  67, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 0.002175, mae: 1.111328, mean_q: 1.504702, mean_eps: 0.320788\n",
      " 755454/1000000: episode: 2777, duration: 7.550s, episode steps: 503, steps per second:  67, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.740 [0.000, 3.000],  loss: 0.002251, mae: 1.118712, mean_q: 1.514770, mean_eps: 0.320318\n",
      " 756098/1000000: episode: 2778, duration: 9.870s, episode steps: 644, steps per second:  65, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.689 [0.000, 3.000],  loss: 0.002393, mae: 1.122266, mean_q: 1.519315, mean_eps: 0.319802\n",
      " 757074/1000000: episode: 2779, duration: 14.687s, episode steps: 976, steps per second:  66, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.686 [0.000, 3.000],  loss: 0.002196, mae: 1.119602, mean_q: 1.516638, mean_eps: 0.319073\n",
      " 757630/1000000: episode: 2780, duration: 8.366s, episode steps: 556, steps per second:  66, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.442 [0.000, 3.000],  loss: 0.002828, mae: 1.125870, mean_q: 1.523667, mean_eps: 0.318383\n",
      " 758030/1000000: episode: 2781, duration: 6.125s, episode steps: 400, steps per second:  65, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 0.002383, mae: 1.123018, mean_q: 1.519511, mean_eps: 0.317953\n",
      " 758489/1000000: episode: 2782, duration: 7.156s, episode steps: 459, steps per second:  64, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.702 [0.000, 3.000],  loss: 0.001955, mae: 1.116219, mean_q: 1.511526, mean_eps: 0.317566\n",
      " 759227/1000000: episode: 2783, duration: 11.874s, episode steps: 738, steps per second:  62, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 0.002377, mae: 1.124950, mean_q: 1.521490, mean_eps: 0.317028\n",
      " 759749/1000000: episode: 2784, duration: 7.879s, episode steps: 522, steps per second:  66, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.705 [0.000, 3.000],  loss: 0.002413, mae: 1.120254, mean_q: 1.515218, mean_eps: 0.316461\n",
      " 760423/1000000: episode: 2785, duration: 10.175s, episode steps: 674, steps per second:  66, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.580 [0.000, 3.000],  loss: 0.002790, mae: 1.131514, mean_q: 1.531535, mean_eps: 0.315923\n",
      " 761045/1000000: episode: 2786, duration: 9.215s, episode steps: 622, steps per second:  68, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.725 [0.000, 3.000],  loss: 0.002933, mae: 1.135775, mean_q: 1.535217, mean_eps: 0.315339\n",
      " 761532/1000000: episode: 2787, duration: 7.435s, episode steps: 487, steps per second:  66, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.774 [0.000, 3.000],  loss: 0.002451, mae: 1.134265, mean_q: 1.534214, mean_eps: 0.314841\n",
      " 762289/1000000: episode: 2788, duration: 11.416s, episode steps: 757, steps per second:  66, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.663 [0.000, 3.000],  loss: 0.002352, mae: 1.131651, mean_q: 1.528714, mean_eps: 0.314281\n",
      " 762948/1000000: episode: 2789, duration: 9.951s, episode steps: 659, steps per second:  66, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002526, mae: 1.134947, mean_q: 1.535054, mean_eps: 0.313644\n",
      " 763517/1000000: episode: 2790, duration: 8.657s, episode steps: 569, steps per second:  66, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: 0.002360, mae: 1.135560, mean_q: 1.534299, mean_eps: 0.313091\n",
      " 763855/1000000: episode: 2791, duration: 5.084s, episode steps: 338, steps per second:  66, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.002394, mae: 1.132394, mean_q: 1.531732, mean_eps: 0.312683\n",
      " 764133/1000000: episode: 2792, duration: 4.174s, episode steps: 278, steps per second:  67, episode reward:  5.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.791 [0.000, 3.000],  loss: 0.001948, mae: 1.136941, mean_q: 1.537160, mean_eps: 0.312405\n",
      " 764631/1000000: episode: 2793, duration: 7.567s, episode steps: 498, steps per second:  66, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.721 [0.000, 3.000],  loss: 0.002154, mae: 1.132822, mean_q: 1.533775, mean_eps: 0.312056\n",
      " 765108/1000000: episode: 2794, duration: 7.273s, episode steps: 477, steps per second:  66, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.673 [0.000, 3.000],  loss: 0.002546, mae: 1.134641, mean_q: 1.534413, mean_eps: 0.311619\n",
      " 766101/1000000: episode: 2795, duration: 14.923s, episode steps: 993, steps per second:  67, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.666 [0.000, 3.000],  loss: 0.002753, mae: 1.134517, mean_q: 1.533313, mean_eps: 0.310956\n",
      " 766640/1000000: episode: 2796, duration: 8.269s, episode steps: 539, steps per second:  65, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.781 [0.000, 3.000],  loss: 0.002425, mae: 1.131787, mean_q: 1.530472, mean_eps: 0.310267\n",
      " 767241/1000000: episode: 2797, duration: 9.069s, episode steps: 601, steps per second:  66, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002603, mae: 1.133858, mean_q: 1.535192, mean_eps: 0.309754\n",
      " 767915/1000000: episode: 2798, duration: 10.264s, episode steps: 674, steps per second:  66, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.533 [0.000, 3.000],  loss: 0.002375, mae: 1.134857, mean_q: 1.535249, mean_eps: 0.309180\n",
      " 768586/1000000: episode: 2799, duration: 9.993s, episode steps: 671, steps per second:  67, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.578 [0.000, 3.000],  loss: 0.002374, mae: 1.142297, mean_q: 1.544480, mean_eps: 0.308575\n",
      " 769269/1000000: episode: 2800, duration: 10.468s, episode steps: 683, steps per second:  65, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.586 [0.000, 3.000],  loss: 0.001972, mae: 1.135045, mean_q: 1.535445, mean_eps: 0.307965\n",
      " 769895/1000000: episode: 2801, duration: 9.482s, episode steps: 626, steps per second:  66, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.601 [0.000, 3.000],  loss: 0.001944, mae: 1.136153, mean_q: 1.538183, mean_eps: 0.307376\n",
      " 770420/1000000: episode: 2802, duration: 7.803s, episode steps: 525, steps per second:  67, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.724 [0.000, 3.000],  loss: 0.003175, mae: 1.141403, mean_q: 1.544239, mean_eps: 0.306860\n",
      " 770905/1000000: episode: 2803, duration: 7.478s, episode steps: 485, steps per second:  65, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.761 [0.000, 3.000],  loss: 0.002732, mae: 1.140708, mean_q: 1.543075, mean_eps: 0.306404\n",
      " 771259/1000000: episode: 2804, duration: 5.225s, episode steps: 354, steps per second:  68, episode reward:  7.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 0.002644, mae: 1.148439, mean_q: 1.553927, mean_eps: 0.306026\n",
      " 771585/1000000: episode: 2805, duration: 4.851s, episode steps: 326, steps per second:  67, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.979 [0.000, 3.000],  loss: 0.002473, mae: 1.144766, mean_q: 1.549631, mean_eps: 0.305720\n",
      " 772264/1000000: episode: 2806, duration: 10.389s, episode steps: 679, steps per second:  65, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.002556, mae: 1.156316, mean_q: 1.562814, mean_eps: 0.305268\n",
      " 772738/1000000: episode: 2807, duration: 7.102s, episode steps: 474, steps per second:  67, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.002606, mae: 1.153783, mean_q: 1.560117, mean_eps: 0.304750\n",
      " 773412/1000000: episode: 2808, duration: 10.294s, episode steps: 674, steps per second:  65, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.632 [0.000, 3.000],  loss: 0.003168, mae: 1.151334, mean_q: 1.556506, mean_eps: 0.304233\n",
      " 773955/1000000: episode: 2809, duration: 8.279s, episode steps: 543, steps per second:  66, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.665 [0.000, 3.000],  loss: 0.002779, mae: 1.147062, mean_q: 1.552235, mean_eps: 0.303686\n",
      " 774598/1000000: episode: 2810, duration: 9.736s, episode steps: 643, steps per second:  66, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.002129, mae: 1.151135, mean_q: 1.557631, mean_eps: 0.303152\n",
      " 775069/1000000: episode: 2811, duration: 7.282s, episode steps: 471, steps per second:  65, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.594 [0.000, 3.000],  loss: 0.002201, mae: 1.148366, mean_q: 1.553184, mean_eps: 0.302649\n",
      " 775529/1000000: episode: 2812, duration: 6.960s, episode steps: 460, steps per second:  66, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.002675, mae: 1.149609, mean_q: 1.555174, mean_eps: 0.302230\n",
      " 776057/1000000: episode: 2813, duration: 8.031s, episode steps: 528, steps per second:  66, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.623 [0.000, 3.000],  loss: 0.002700, mae: 1.155368, mean_q: 1.561600, mean_eps: 0.301785\n",
      " 776604/1000000: episode: 2814, duration: 8.306s, episode steps: 547, steps per second:  66, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.753 [0.000, 3.000],  loss: 0.002575, mae: 1.153202, mean_q: 1.560623, mean_eps: 0.301303\n",
      " 777126/1000000: episode: 2815, duration: 8.008s, episode steps: 522, steps per second:  65, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.002232, mae: 1.158509, mean_q: 1.566737, mean_eps: 0.300822\n",
      " 777834/1000000: episode: 2816, duration: 10.712s, episode steps: 708, steps per second:  66, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.661 [0.000, 3.000],  loss: 0.002289, mae: 1.154272, mean_q: 1.561468, mean_eps: 0.300268\n",
      " 778501/1000000: episode: 2817, duration: 10.269s, episode steps: 667, steps per second:  65, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 0.002507, mae: 1.146490, mean_q: 1.553568, mean_eps: 0.299649\n",
      " 779086/1000000: episode: 2818, duration: 8.991s, episode steps: 585, steps per second:  65, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.537 [0.000, 3.000],  loss: 0.002217, mae: 1.146435, mean_q: 1.552057, mean_eps: 0.299085\n",
      " 779752/1000000: episode: 2819, duration: 10.275s, episode steps: 666, steps per second:  65, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.655 [0.000, 3.000],  loss: 0.002498, mae: 1.151991, mean_q: 1.557638, mean_eps: 0.298524\n",
      " 780493/1000000: episode: 2820, duration: 11.443s, episode steps: 741, steps per second:  65, episode reward: 20.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.551 [0.000, 3.000],  loss: 0.002813, mae: 1.157764, mean_q: 1.564749, mean_eps: 0.297890\n",
      " 781008/1000000: episode: 2821, duration: 7.888s, episode steps: 515, steps per second:  65, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.650 [0.000, 3.000],  loss: 0.003183, mae: 1.160770, mean_q: 1.568934, mean_eps: 0.297325\n",
      " 781478/1000000: episode: 2822, duration: 7.403s, episode steps: 470, steps per second:  63, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.002653, mae: 1.163389, mean_q: 1.572324, mean_eps: 0.296882\n",
      " 781997/1000000: episode: 2823, duration: 7.954s, episode steps: 519, steps per second:  65, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.788 [0.000, 3.000],  loss: 0.002638, mae: 1.153575, mean_q: 1.560256, mean_eps: 0.296436\n",
      " 782552/1000000: episode: 2824, duration: 8.688s, episode steps: 555, steps per second:  64, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.739 [0.000, 3.000],  loss: 0.002316, mae: 1.160375, mean_q: 1.567609, mean_eps: 0.295953\n",
      " 782849/1000000: episode: 2825, duration: 4.625s, episode steps: 297, steps per second:  64, episode reward:  4.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.002610, mae: 1.168639, mean_q: 1.578070, mean_eps: 0.295570\n",
      " 783458/1000000: episode: 2826, duration: 9.495s, episode steps: 609, steps per second:  64, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.713 [0.000, 3.000],  loss: 0.002479, mae: 1.159234, mean_q: 1.566579, mean_eps: 0.295161\n",
      " 783814/1000000: episode: 2827, duration: 5.478s, episode steps: 356, steps per second:  65, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.708 [0.000, 3.000],  loss: 0.002479, mae: 1.157526, mean_q: 1.566716, mean_eps: 0.294728\n",
      " 784215/1000000: episode: 2828, duration: 6.098s, episode steps: 401, steps per second:  66, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: 0.002503, mae: 1.162046, mean_q: 1.570070, mean_eps: 0.294387\n",
      " 784619/1000000: episode: 2829, duration: 6.358s, episode steps: 404, steps per second:  64, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.792 [0.000, 3.000],  loss: 0.002105, mae: 1.165140, mean_q: 1.574352, mean_eps: 0.294026\n",
      " 785335/1000000: episode: 2830, duration: 11.102s, episode steps: 716, steps per second:  64, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.002277, mae: 1.158791, mean_q: 1.565840, mean_eps: 0.293522\n",
      " 786322/1000000: episode: 2831, duration: 15.275s, episode steps: 987, steps per second:  65, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.687 [0.000, 3.000],  loss: 0.002520, mae: 1.154458, mean_q: 1.560548, mean_eps: 0.292755\n",
      " 787103/1000000: episode: 2832, duration: 14.237s, episode steps: 781, steps per second:  55, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.776 [0.000, 3.000],  loss: 0.002408, mae: 1.159224, mean_q: 1.567567, mean_eps: 0.291959\n",
      " 787661/1000000: episode: 2833, duration: 9.123s, episode steps: 558, steps per second:  61, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.794 [0.000, 3.000],  loss: 0.002712, mae: 1.161853, mean_q: 1.571064, mean_eps: 0.291356\n",
      " 788269/1000000: episode: 2834, duration: 9.368s, episode steps: 608, steps per second:  65, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.786 [0.000, 3.000],  loss: 0.002291, mae: 1.164324, mean_q: 1.572747, mean_eps: 0.290831\n",
      " 789136/1000000: episode: 2835, duration: 13.508s, episode steps: 867, steps per second:  64, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.660 [0.000, 3.000],  loss: 0.002309, mae: 1.164747, mean_q: 1.575541, mean_eps: 0.290168\n",
      " 789714/1000000: episode: 2836, duration: 9.105s, episode steps: 578, steps per second:  63, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.618 [0.000, 3.000],  loss: 0.002209, mae: 1.163642, mean_q: 1.573976, mean_eps: 0.289518\n",
      " 790247/1000000: episode: 2837, duration: 8.261s, episode steps: 533, steps per second:  65, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.664 [0.000, 3.000],  loss: 0.002484, mae: 1.164574, mean_q: 1.573559, mean_eps: 0.289018\n",
      " 790720/1000000: episode: 2838, duration: 7.442s, episode steps: 473, steps per second:  64, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.002372, mae: 1.178896, mean_q: 1.592159, mean_eps: 0.288566\n",
      " 791264/1000000: episode: 2839, duration: 8.507s, episode steps: 544, steps per second:  64, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.566 [0.000, 3.000],  loss: 0.003059, mae: 1.178152, mean_q: 1.589857, mean_eps: 0.288109\n",
      " 791944/1000000: episode: 2840, duration: 10.794s, episode steps: 680, steps per second:  63, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.712 [0.000, 3.000],  loss: 0.002349, mae: 1.179473, mean_q: 1.591902, mean_eps: 0.287558\n",
      " 792536/1000000: episode: 2841, duration: 9.359s, episode steps: 592, steps per second:  63, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.441 [0.000, 3.000],  loss: 0.002350, mae: 1.173706, mean_q: 1.586729, mean_eps: 0.286986\n",
      " 793197/1000000: episode: 2842, duration: 10.449s, episode steps: 661, steps per second:  63, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.755 [0.000, 3.000],  loss: 0.001987, mae: 1.175975, mean_q: 1.591072, mean_eps: 0.286421\n",
      " 793677/1000000: episode: 2843, duration: 7.573s, episode steps: 480, steps per second:  63, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.840 [0.000, 3.000],  loss: 0.001933, mae: 1.180973, mean_q: 1.595113, mean_eps: 0.285906\n",
      " 794446/1000000: episode: 2844, duration: 11.797s, episode steps: 769, steps per second:  65, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.614 [0.000, 3.000],  loss: 0.002364, mae: 1.176706, mean_q: 1.592187, mean_eps: 0.285344\n",
      " 795469/1000000: episode: 2845, duration: 16.185s, episode steps: 1023, steps per second:  63, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.795 [0.000, 3.000],  loss: 0.002588, mae: 1.180281, mean_q: 1.593271, mean_eps: 0.284538\n",
      " 796339/1000000: episode: 2846, duration: 13.341s, episode steps: 870, steps per second:  65, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.643 [0.000, 3.000],  loss: 0.002443, mae: 1.177431, mean_q: 1.589201, mean_eps: 0.283686\n",
      " 796870/1000000: episode: 2847, duration: 8.377s, episode steps: 531, steps per second:  63, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.808 [0.000, 3.000],  loss: 0.002024, mae: 1.180294, mean_q: 1.592743, mean_eps: 0.283056\n",
      " 797641/1000000: episode: 2848, duration: 12.056s, episode steps: 771, steps per second:  64, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.586 [0.000, 3.000],  loss: 0.002501, mae: 1.175177, mean_q: 1.588470, mean_eps: 0.282470\n",
      " 798541/1000000: episode: 2849, duration: 14.374s, episode steps: 900, steps per second:  63, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.717 [0.000, 3.000],  loss: 0.002678, mae: 1.173815, mean_q: 1.586065, mean_eps: 0.281717\n",
      " 799250/1000000: episode: 2850, duration: 11.161s, episode steps: 709, steps per second:  64, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.002714, mae: 1.176701, mean_q: 1.590679, mean_eps: 0.280994\n",
      " 799795/1000000: episode: 2851, duration: 8.809s, episode steps: 545, steps per second:  62, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.483 [0.000, 3.000],  loss: 0.002777, mae: 1.178471, mean_q: 1.591355, mean_eps: 0.280430\n",
      " 800148/1000000: episode: 2852, duration: 5.474s, episode steps: 353, steps per second:  64, episode reward:  7.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 0.002733, mae: 1.183634, mean_q: 1.598951, mean_eps: 0.280027\n",
      " 800608/1000000: episode: 2853, duration: 7.152s, episode steps: 460, steps per second:  64, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.003190, mae: 1.189765, mean_q: 1.607621, mean_eps: 0.279662\n",
      " 801111/1000000: episode: 2854, duration: 7.816s, episode steps: 503, steps per second:  64, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.712 [0.000, 3.000],  loss: 0.002802, mae: 1.194706, mean_q: 1.613140, mean_eps: 0.279228\n",
      " 801786/1000000: episode: 2855, duration: 10.521s, episode steps: 675, steps per second:  64, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.726 [0.000, 3.000],  loss: 0.002353, mae: 1.197353, mean_q: 1.617208, mean_eps: 0.278697\n",
      " 802469/1000000: episode: 2856, duration: 10.580s, episode steps: 683, steps per second:  65, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.624 [0.000, 3.000],  loss: 0.002611, mae: 1.195754, mean_q: 1.614507, mean_eps: 0.278085\n",
      " 803084/1000000: episode: 2857, duration: 9.636s, episode steps: 615, steps per second:  64, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.740 [0.000, 3.000],  loss: 0.002329, mae: 1.198903, mean_q: 1.620029, mean_eps: 0.277502\n",
      " 803886/1000000: episode: 2858, duration: 12.632s, episode steps: 802, steps per second:  63, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.792 [0.000, 3.000],  loss: 0.002255, mae: 1.195746, mean_q: 1.614755, mean_eps: 0.276864\n",
      " 804317/1000000: episode: 2859, duration: 6.635s, episode steps: 431, steps per second:  65, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.432 [0.000, 3.000],  loss: 0.002586, mae: 1.200315, mean_q: 1.620598, mean_eps: 0.276308\n",
      " 805416/1000000: episode: 2860, duration: 17.329s, episode steps: 1099, steps per second:  63, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.002916, mae: 1.195029, mean_q: 1.614098, mean_eps: 0.275621\n",
      " 806432/1000000: episode: 2861, duration: 15.883s, episode steps: 1016, steps per second:  64, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.002704, mae: 1.198131, mean_q: 1.618757, mean_eps: 0.274670\n",
      " 806979/1000000: episode: 2862, duration: 8.614s, episode steps: 547, steps per second:  64, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.558 [0.000, 3.000],  loss: 0.002399, mae: 1.196332, mean_q: 1.615316, mean_eps: 0.273966\n",
      " 807396/1000000: episode: 2863, duration: 6.557s, episode steps: 417, steps per second:  64, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.695 [0.000, 3.000],  loss: 0.003118, mae: 1.196980, mean_q: 1.617631, mean_eps: 0.273533\n",
      " 807854/1000000: episode: 2864, duration: 7.237s, episode steps: 458, steps per second:  63, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.705 [0.000, 3.000],  loss: 0.002729, mae: 1.194620, mean_q: 1.613913, mean_eps: 0.273138\n",
      " 808541/1000000: episode: 2865, duration: 10.535s, episode steps: 687, steps per second:  65, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.608 [0.000, 3.000],  loss: 0.002343, mae: 1.198940, mean_q: 1.618277, mean_eps: 0.272622\n",
      " 809360/1000000: episode: 2866, duration: 12.871s, episode steps: 819, steps per second:  64, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.488 [0.000, 3.000],  loss: 0.002565, mae: 1.191311, mean_q: 1.609040, mean_eps: 0.271945\n",
      " 809945/1000000: episode: 2867, duration: 9.250s, episode steps: 585, steps per second:  63, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.848 [0.000, 3.000],  loss: 0.002435, mae: 1.199481, mean_q: 1.617455, mean_eps: 0.271313\n",
      " 810443/1000000: episode: 2868, duration: 7.692s, episode steps: 498, steps per second:  65, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.715 [0.000, 3.000],  loss: 0.002733, mae: 1.208144, mean_q: 1.629349, mean_eps: 0.270825\n",
      " 811074/1000000: episode: 2869, duration: 10.000s, episode steps: 631, steps per second:  63, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.637 [0.000, 3.000],  loss: 0.002703, mae: 1.207925, mean_q: 1.629995, mean_eps: 0.270318\n",
      " 811750/1000000: episode: 2870, duration: 10.597s, episode steps: 676, steps per second:  64, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.611 [0.000, 3.000],  loss: 0.002717, mae: 1.208172, mean_q: 1.629752, mean_eps: 0.269729\n",
      " 812392/1000000: episode: 2871, duration: 10.227s, episode steps: 642, steps per second:  63, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.835 [0.000, 3.000],  loss: 0.002736, mae: 1.215727, mean_q: 1.641052, mean_eps: 0.269137\n",
      " 813110/1000000: episode: 2872, duration: 11.322s, episode steps: 718, steps per second:  63, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.847 [0.000, 3.000],  loss: 0.002788, mae: 1.211606, mean_q: 1.637352, mean_eps: 0.268525\n",
      " 814077/1000000: episode: 2873, duration: 15.186s, episode steps: 967, steps per second:  64, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.493 [0.000, 3.000],  loss: 0.002754, mae: 1.208884, mean_q: 1.634076, mean_eps: 0.267765\n",
      " 814621/1000000: episode: 2874, duration: 8.403s, episode steps: 544, steps per second:  65, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.546 [0.000, 3.000],  loss: 0.002669, mae: 1.210531, mean_q: 1.636903, mean_eps: 0.267085\n",
      " 815690/1000000: episode: 2875, duration: 16.747s, episode steps: 1069, steps per second:  64, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.638 [0.000, 3.000],  loss: 0.002795, mae: 1.212482, mean_q: 1.639304, mean_eps: 0.266360\n",
      " 816273/1000000: episode: 2876, duration: 9.247s, episode steps: 583, steps per second:  63, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.768 [0.000, 3.000],  loss: 0.003009, mae: 1.207447, mean_q: 1.633552, mean_eps: 0.265616\n",
      " 816813/1000000: episode: 2877, duration: 8.427s, episode steps: 540, steps per second:  64, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.822 [0.000, 3.000],  loss: 0.002646, mae: 1.210647, mean_q: 1.640070, mean_eps: 0.265110\n",
      " 817337/1000000: episode: 2878, duration: 8.339s, episode steps: 524, steps per second:  63, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.698 [0.000, 3.000],  loss: 0.002454, mae: 1.204302, mean_q: 1.626391, mean_eps: 0.264632\n",
      " 817991/1000000: episode: 2879, duration: 10.308s, episode steps: 654, steps per second:  63, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.624 [0.000, 3.000],  loss: 0.003079, mae: 1.207512, mean_q: 1.633377, mean_eps: 0.264102\n",
      " 818370/1000000: episode: 2880, duration: 5.912s, episode steps: 379, steps per second:  64, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.586 [0.000, 3.000],  loss: 0.002362, mae: 1.209753, mean_q: 1.637781, mean_eps: 0.263638\n",
      " 818698/1000000: episode: 2881, duration: 5.194s, episode steps: 328, steps per second:  63, episode reward:  5.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 0.002245, mae: 1.206147, mean_q: 1.631815, mean_eps: 0.263319\n",
      " 819355/1000000: episode: 2882, duration: 10.416s, episode steps: 657, steps per second:  63, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.429 [0.000, 3.000],  loss: 0.002468, mae: 1.213811, mean_q: 1.642493, mean_eps: 0.262877\n",
      " 820040/1000000: episode: 2883, duration: 11.013s, episode steps: 685, steps per second:  62, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: 0.002435, mae: 1.211899, mean_q: 1.640692, mean_eps: 0.262274\n",
      " 820748/1000000: episode: 2884, duration: 11.180s, episode steps: 708, steps per second:  63, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.507 [0.000, 3.000],  loss: 0.002947, mae: 1.237204, mean_q: 1.670289, mean_eps: 0.261647\n",
      " 821202/1000000: episode: 2885, duration: 7.186s, episode steps: 454, steps per second:  63, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.002689, mae: 1.233255, mean_q: 1.665377, mean_eps: 0.261123\n",
      " 821816/1000000: episode: 2886, duration: 9.429s, episode steps: 614, steps per second:  65, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 0.002210, mae: 1.231372, mean_q: 1.662844, mean_eps: 0.260643\n",
      " 822213/1000000: episode: 2887, duration: 6.397s, episode steps: 397, steps per second:  62, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.763 [0.000, 3.000],  loss: 0.002749, mae: 1.224389, mean_q: 1.654405, mean_eps: 0.260187\n",
      " 822913/1000000: episode: 2888, duration: 11.015s, episode steps: 700, steps per second:  64, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.529 [0.000, 3.000],  loss: 0.002354, mae: 1.228904, mean_q: 1.660062, mean_eps: 0.259692\n",
      " 823765/1000000: episode: 2889, duration: 13.489s, episode steps: 852, steps per second:  63, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.719 [0.000, 3.000],  loss: 0.002791, mae: 1.227867, mean_q: 1.657583, mean_eps: 0.258994\n",
      " 824581/1000000: episode: 2890, duration: 13.158s, episode steps: 816, steps per second:  62, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.439 [0.000, 3.000],  loss: 0.002626, mae: 1.228923, mean_q: 1.659209, mean_eps: 0.258243\n",
      " 825553/1000000: episode: 2891, duration: 15.311s, episode steps: 972, steps per second:  63, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.668 [0.000, 3.000],  loss: 0.003024, mae: 1.227301, mean_q: 1.658590, mean_eps: 0.257439\n",
      " 826478/1000000: episode: 2892, duration: 14.655s, episode steps: 925, steps per second:  63, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.695 [0.000, 3.000],  loss: 0.002921, mae: 1.228950, mean_q: 1.659580, mean_eps: 0.256586\n",
      " 827389/1000000: episode: 2893, duration: 14.677s, episode steps: 911, steps per second:  62, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.685 [0.000, 3.000],  loss: 0.002558, mae: 1.229621, mean_q: 1.659699, mean_eps: 0.255759\n",
      " 827941/1000000: episode: 2894, duration: 8.735s, episode steps: 552, steps per second:  63, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.647 [0.000, 3.000],  loss: 0.002609, mae: 1.227281, mean_q: 1.659347, mean_eps: 0.255101\n",
      " 828749/1000000: episode: 2895, duration: 12.968s, episode steps: 808, steps per second:  62, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.002349, mae: 1.236892, mean_q: 1.669032, mean_eps: 0.254489\n",
      " 829241/1000000: episode: 2896, duration: 7.917s, episode steps: 492, steps per second:  62, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.539 [0.000, 3.000],  loss: 0.002293, mae: 1.236569, mean_q: 1.667406, mean_eps: 0.253904\n",
      " 830060/1000000: episode: 2897, duration: 13.108s, episode steps: 819, steps per second:  62, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.614 [0.000, 3.000],  loss: 0.002680, mae: 1.228231, mean_q: 1.659320, mean_eps: 0.253315\n",
      " 830991/1000000: episode: 2898, duration: 14.757s, episode steps: 931, steps per second:  63, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.414 [0.000, 3.000],  loss: 0.002831, mae: 1.245202, mean_q: 1.680739, mean_eps: 0.252528\n",
      " 831563/1000000: episode: 2899, duration: 9.209s, episode steps: 572, steps per second:  62, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 0.003037, mae: 1.244282, mean_q: 1.680914, mean_eps: 0.251852\n",
      " 832305/1000000: episode: 2900, duration: 11.992s, episode steps: 742, steps per second:  62, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.745 [0.000, 3.000],  loss: 0.002326, mae: 1.239237, mean_q: 1.674258, mean_eps: 0.251259\n",
      " 832851/1000000: episode: 2901, duration: 8.638s, episode steps: 546, steps per second:  63, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.652 [0.000, 3.000],  loss: 0.003387, mae: 1.245074, mean_q: 1.683165, mean_eps: 0.250680\n",
      " 833355/1000000: episode: 2902, duration: 8.170s, episode steps: 504, steps per second:  62, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.472 [0.000, 3.000],  loss: 0.003424, mae: 1.241675, mean_q: 1.679970, mean_eps: 0.250208\n",
      " 833892/1000000: episode: 2903, duration: 8.474s, episode steps: 537, steps per second:  63, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.756 [0.000, 3.000],  loss: 0.002143, mae: 1.242621, mean_q: 1.678287, mean_eps: 0.249740\n",
      " 834702/1000000: episode: 2904, duration: 13.018s, episode steps: 810, steps per second:  62, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.644 [0.000, 3.000],  loss: 0.002494, mae: 1.239997, mean_q: 1.674949, mean_eps: 0.249134\n",
      " 835158/1000000: episode: 2905, duration: 7.384s, episode steps: 456, steps per second:  62, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.568 [0.000, 3.000],  loss: 0.002565, mae: 1.235946, mean_q: 1.668946, mean_eps: 0.248563\n",
      " 835876/1000000: episode: 2906, duration: 11.265s, episode steps: 718, steps per second:  64, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.670 [0.000, 3.000],  loss: 0.002539, mae: 1.240697, mean_q: 1.676058, mean_eps: 0.248036\n",
      " 836672/1000000: episode: 2907, duration: 12.787s, episode steps: 796, steps per second:  62, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.704 [0.000, 3.000],  loss: 0.002448, mae: 1.240941, mean_q: 1.675448, mean_eps: 0.247355\n",
      " 837163/1000000: episode: 2908, duration: 7.944s, episode steps: 491, steps per second:  62, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.656 [0.000, 3.000],  loss: 0.002642, mae: 1.243802, mean_q: 1.680277, mean_eps: 0.246776\n",
      " 837779/1000000: episode: 2909, duration: 9.723s, episode steps: 616, steps per second:  63, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.541 [0.000, 3.000],  loss: 0.002724, mae: 1.244198, mean_q: 1.681216, mean_eps: 0.246277\n",
      " 838287/1000000: episode: 2910, duration: 8.311s, episode steps: 508, steps per second:  61, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.002860, mae: 1.235420, mean_q: 1.670982, mean_eps: 0.245771\n",
      " 838912/1000000: episode: 2911, duration: 9.930s, episode steps: 625, steps per second:  63, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.394 [0.000, 3.000],  loss: 0.002289, mae: 1.245786, mean_q: 1.684079, mean_eps: 0.245262\n",
      " 839550/1000000: episode: 2912, duration: 10.346s, episode steps: 638, steps per second:  62, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.652 [0.000, 3.000],  loss: 0.002712, mae: 1.246108, mean_q: 1.684026, mean_eps: 0.244693\n",
      " 840273/1000000: episode: 2913, duration: 11.659s, episode steps: 723, steps per second:  62, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: 0.002513, mae: 1.249685, mean_q: 1.687617, mean_eps: 0.244079\n",
      " 840852/1000000: episode: 2914, duration: 9.336s, episode steps: 579, steps per second:  62, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.589 [0.000, 3.000],  loss: 0.002176, mae: 1.261010, mean_q: 1.700739, mean_eps: 0.243494\n",
      " 841392/1000000: episode: 2915, duration: 8.838s, episode steps: 540, steps per second:  61, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.900 [0.000, 3.000],  loss: 0.002964, mae: 1.252219, mean_q: 1.690650, mean_eps: 0.242992\n",
      " 841932/1000000: episode: 2916, duration: 8.699s, episode steps: 540, steps per second:  62, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.698 [0.000, 3.000],  loss: 0.002833, mae: 1.257336, mean_q: 1.694146, mean_eps: 0.242506\n",
      " 842784/1000000: episode: 2917, duration: 13.831s, episode steps: 852, steps per second:  62, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002707, mae: 1.248627, mean_q: 1.687371, mean_eps: 0.241880\n",
      " 843477/1000000: episode: 2918, duration: 11.248s, episode steps: 693, steps per second:  62, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.821 [0.000, 3.000],  loss: 0.002797, mae: 1.249816, mean_q: 1.687543, mean_eps: 0.241183\n",
      " 843992/1000000: episode: 2919, duration: 8.464s, episode steps: 515, steps per second:  61, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.538 [0.000, 3.000],  loss: 0.002890, mae: 1.246816, mean_q: 1.683602, mean_eps: 0.240639\n",
      " 844605/1000000: episode: 2920, duration: 9.957s, episode steps: 613, steps per second:  62, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.672 [0.000, 3.000],  loss: 0.002541, mae: 1.255517, mean_q: 1.695246, mean_eps: 0.240132\n",
      " 845231/1000000: episode: 2921, duration: 10.250s, episode steps: 626, steps per second:  61, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.673 [0.000, 3.000],  loss: 0.002542, mae: 1.250892, mean_q: 1.689204, mean_eps: 0.239574\n",
      " 845898/1000000: episode: 2922, duration: 10.773s, episode steps: 667, steps per second:  62, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.739 [0.000, 3.000],  loss: 0.002682, mae: 1.256032, mean_q: 1.695293, mean_eps: 0.238992\n",
      " 846448/1000000: episode: 2923, duration: 8.969s, episode steps: 550, steps per second:  61, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.684 [0.000, 3.000],  loss: 0.002490, mae: 1.250271, mean_q: 1.688018, mean_eps: 0.238445\n",
      " 847076/1000000: episode: 2924, duration: 10.186s, episode steps: 628, steps per second:  62, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.766 [0.000, 3.000],  loss: 0.003076, mae: 1.249680, mean_q: 1.686845, mean_eps: 0.237916\n",
      " 847664/1000000: episode: 2925, duration: 9.400s, episode steps: 588, steps per second:  63, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.711 [0.000, 3.000],  loss: 0.003351, mae: 1.245109, mean_q: 1.680303, mean_eps: 0.237369\n",
      " 848818/1000000: episode: 2926, duration: 18.937s, episode steps: 1154, steps per second:  61, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.692 [0.000, 3.000],  loss: 0.002827, mae: 1.256148, mean_q: 1.694497, mean_eps: 0.236584\n",
      " 849539/1000000: episode: 2927, duration: 11.591s, episode steps: 721, steps per second:  62, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.002838, mae: 1.251671, mean_q: 1.688005, mean_eps: 0.235740\n",
      " 850393/1000000: episode: 2928, duration: 13.932s, episode steps: 854, steps per second:  61, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.001893, mae: 1.253914, mean_q: 1.690653, mean_eps: 0.235031\n",
      " 851176/1000000: episode: 2929, duration: 12.756s, episode steps: 783, steps per second:  61, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.660 [0.000, 3.000],  loss: 0.002595, mae: 1.268246, mean_q: 1.708817, mean_eps: 0.234294\n",
      " 851632/1000000: episode: 2930, duration: 7.377s, episode steps: 456, steps per second:  62, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.002232, mae: 1.264575, mean_q: 1.702440, mean_eps: 0.233738\n",
      " 852336/1000000: episode: 2931, duration: 11.498s, episode steps: 704, steps per second:  61, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.666 [0.000, 3.000],  loss: 0.002282, mae: 1.263194, mean_q: 1.701079, mean_eps: 0.233216\n",
      " 853016/1000000: episode: 2932, duration: 11.418s, episode steps: 680, steps per second:  60, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.002365, mae: 1.257215, mean_q: 1.694988, mean_eps: 0.232593\n",
      " 853562/1000000: episode: 2933, duration: 8.835s, episode steps: 546, steps per second:  62, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.694 [0.000, 3.000],  loss: 0.001832, mae: 1.258167, mean_q: 1.695514, mean_eps: 0.232041\n",
      " 853995/1000000: episode: 2934, duration: 7.164s, episode steps: 433, steps per second:  60, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.506 [0.000, 3.000],  loss: 0.002427, mae: 1.264152, mean_q: 1.701492, mean_eps: 0.231600\n",
      " 854814/1000000: episode: 2935, duration: 13.487s, episode steps: 819, steps per second:  61, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.700 [0.000, 3.000],  loss: 0.002746, mae: 1.262561, mean_q: 1.700640, mean_eps: 0.231036\n",
      " 855769/1000000: episode: 2936, duration: 16.144s, episode steps: 955, steps per second:  59, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.810 [0.000, 3.000],  loss: 0.002487, mae: 1.261539, mean_q: 1.700345, mean_eps: 0.230237\n",
      " 856245/1000000: episode: 2937, duration: 7.638s, episode steps: 476, steps per second:  62, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.527 [0.000, 3.000],  loss: 0.002745, mae: 1.259968, mean_q: 1.697546, mean_eps: 0.229593\n",
      " 856724/1000000: episode: 2938, duration: 8.052s, episode steps: 479, steps per second:  59, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.662 [0.000, 3.000],  loss: 0.002051, mae: 1.259950, mean_q: 1.697602, mean_eps: 0.229164\n",
      " 857559/1000000: episode: 2939, duration: 13.651s, episode steps: 835, steps per second:  61, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.600 [0.000, 3.000],  loss: 0.002376, mae: 1.258765, mean_q: 1.695310, mean_eps: 0.228574\n",
      " 858421/1000000: episode: 2940, duration: 14.095s, episode steps: 862, steps per second:  61, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.619 [0.000, 3.000],  loss: 0.002196, mae: 1.261481, mean_q: 1.700386, mean_eps: 0.227809\n",
      " 858992/1000000: episode: 2941, duration: 9.362s, episode steps: 571, steps per second:  61, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.001863, mae: 1.267295, mean_q: 1.705980, mean_eps: 0.227165\n",
      " 859794/1000000: episode: 2942, duration: 13.076s, episode steps: 802, steps per second:  61, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.596 [0.000, 3.000],  loss: 0.002646, mae: 1.265559, mean_q: 1.703195, mean_eps: 0.226547\n",
      " 860442/1000000: episode: 2943, duration: 10.419s, episode steps: 648, steps per second:  62, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.858 [0.000, 3.000],  loss: 0.002648, mae: 1.272584, mean_q: 1.713375, mean_eps: 0.225894\n",
      " 861612/1000000: episode: 2944, duration: 19.406s, episode steps: 1170, steps per second:  60, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.639 [0.000, 3.000],  loss: 0.002265, mae: 1.276518, mean_q: 1.720068, mean_eps: 0.225077\n",
      " 862265/1000000: episode: 2945, duration: 10.589s, episode steps: 653, steps per second:  62, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.738 [0.000, 3.000],  loss: 0.001811, mae: 1.272109, mean_q: 1.713471, mean_eps: 0.224256\n",
      " 863168/1000000: episode: 2946, duration: 14.760s, episode steps: 903, steps per second:  61, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.648 [0.000, 3.000],  loss: 0.002765, mae: 1.272014, mean_q: 1.713212, mean_eps: 0.223556\n",
      " 864001/1000000: episode: 2947, duration: 13.802s, episode steps: 833, steps per second:  60, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.522 [0.000, 3.000],  loss: 0.002308, mae: 1.275400, mean_q: 1.715847, mean_eps: 0.222774\n",
      " 864656/1000000: episode: 2948, duration: 10.835s, episode steps: 655, steps per second:  60, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.530 [0.000, 3.000],  loss: 0.002560, mae: 1.273876, mean_q: 1.715189, mean_eps: 0.222105\n",
      " 865326/1000000: episode: 2949, duration: 10.854s, episode steps: 670, steps per second:  62, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.002863, mae: 1.274357, mean_q: 1.716913, mean_eps: 0.221509\n",
      " 865879/1000000: episode: 2950, duration: 9.076s, episode steps: 553, steps per second:  61, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.559 [0.000, 3.000],  loss: 0.002403, mae: 1.278498, mean_q: 1.724095, mean_eps: 0.220958\n",
      " 866549/1000000: episode: 2951, duration: 11.087s, episode steps: 670, steps per second:  60, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.834 [0.000, 3.000],  loss: 0.002662, mae: 1.275055, mean_q: 1.719515, mean_eps: 0.220407\n",
      " 867032/1000000: episode: 2952, duration: 7.854s, episode steps: 483, steps per second:  62, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.814 [0.000, 3.000],  loss: 0.002850, mae: 1.271561, mean_q: 1.713959, mean_eps: 0.219889\n",
      " 867881/1000000: episode: 2953, duration: 14.037s, episode steps: 849, steps per second:  60, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.698 [0.000, 3.000],  loss: 0.002389, mae: 1.271455, mean_q: 1.714316, mean_eps: 0.219290\n",
      " 868670/1000000: episode: 2954, duration: 13.099s, episode steps: 789, steps per second:  60, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.570 [0.000, 3.000],  loss: 0.001973, mae: 1.275584, mean_q: 1.717491, mean_eps: 0.218552\n",
      " 869224/1000000: episode: 2955, duration: 9.022s, episode steps: 554, steps per second:  61, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.002096, mae: 1.273856, mean_q: 1.716580, mean_eps: 0.217949\n",
      " 869822/1000000: episode: 2956, duration: 10.047s, episode steps: 598, steps per second:  60, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.651 [0.000, 3.000],  loss: 0.002041, mae: 1.270169, mean_q: 1.710825, mean_eps: 0.217430\n",
      " 870380/1000000: episode: 2957, duration: 9.443s, episode steps: 558, steps per second:  59, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.588 [0.000, 3.000],  loss: 0.002706, mae: 1.282859, mean_q: 1.728091, mean_eps: 0.216910\n",
      " 870927/1000000: episode: 2958, duration: 8.954s, episode steps: 547, steps per second:  61, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.845 [0.000, 3.000],  loss: 0.002664, mae: 1.275088, mean_q: 1.717777, mean_eps: 0.216413\n",
      " 871416/1000000: episode: 2959, duration: 8.216s, episode steps: 489, steps per second:  60, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.587 [0.000, 3.000],  loss: 0.003190, mae: 1.275383, mean_q: 1.719542, mean_eps: 0.215947\n",
      " 872497/1000000: episode: 2960, duration: 17.916s, episode steps: 1081, steps per second:  60, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.722 [0.000, 3.000],  loss: 0.002285, mae: 1.281565, mean_q: 1.727450, mean_eps: 0.215240\n",
      " 872933/1000000: episode: 2961, duration: 7.111s, episode steps: 436, steps per second:  61, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.567 [0.000, 3.000],  loss: 0.003096, mae: 1.272117, mean_q: 1.714816, mean_eps: 0.214556\n",
      " 873544/1000000: episode: 2962, duration: 10.127s, episode steps: 611, steps per second:  60, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.414 [0.000, 3.000],  loss: 0.002267, mae: 1.277717, mean_q: 1.721718, mean_eps: 0.214086\n",
      " 874371/1000000: episode: 2963, duration: 13.753s, episode steps: 827, steps per second:  60, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.577 [0.000, 3.000],  loss: 0.001968, mae: 1.278169, mean_q: 1.722174, mean_eps: 0.213440\n",
      " 875165/1000000: episode: 2964, duration: 13.250s, episode steps: 794, steps per second:  60, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: 0.002203, mae: 1.280885, mean_q: 1.725949, mean_eps: 0.212709\n",
      " 875695/1000000: episode: 2965, duration: 8.758s, episode steps: 530, steps per second:  61, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.547 [0.000, 3.000],  loss: 0.002781, mae: 1.281974, mean_q: 1.726552, mean_eps: 0.212113\n",
      " 876435/1000000: episode: 2966, duration: 12.314s, episode steps: 740, steps per second:  60, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.830 [0.000, 3.000],  loss: 0.002162, mae: 1.274511, mean_q: 1.716926, mean_eps: 0.211542\n",
      " 876816/1000000: episode: 2967, duration: 6.303s, episode steps: 381, steps per second:  60, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.785 [0.000, 3.000],  loss: 0.001862, mae: 1.287757, mean_q: 1.735172, mean_eps: 0.211038\n",
      " 877499/1000000: episode: 2968, duration: 11.336s, episode steps: 683, steps per second:  60, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.682 [0.000, 3.000],  loss: 0.002565, mae: 1.283397, mean_q: 1.729682, mean_eps: 0.210560\n",
      " 878036/1000000: episode: 2969, duration: 8.939s, episode steps: 537, steps per second:  60, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002095, mae: 1.276649, mean_q: 1.722778, mean_eps: 0.210011\n",
      " 878772/1000000: episode: 2970, duration: 12.177s, episode steps: 736, steps per second:  60, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.001898, mae: 1.275573, mean_q: 1.719844, mean_eps: 0.209438\n",
      " 879375/1000000: episode: 2971, duration: 10.092s, episode steps: 603, steps per second:  60, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.715 [0.000, 3.000],  loss: 0.001786, mae: 1.276076, mean_q: 1.719890, mean_eps: 0.208835\n",
      " 879905/1000000: episode: 2972, duration: 8.970s, episode steps: 530, steps per second:  59, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 0.001781, mae: 1.277433, mean_q: 1.722481, mean_eps: 0.208324\n",
      " 880293/1000000: episode: 2973, duration: 6.490s, episode steps: 388, steps per second:  60, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.477 [0.000, 3.000],  loss: 0.002074, mae: 1.286100, mean_q: 1.729679, mean_eps: 0.207910\n",
      " 880793/1000000: episode: 2974, duration: 8.122s, episode steps: 500, steps per second:  62, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 0.002619, mae: 1.285073, mean_q: 1.729480, mean_eps: 0.207510\n",
      " 881491/1000000: episode: 2975, duration: 11.576s, episode steps: 698, steps per second:  60, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.744 [0.000, 3.000],  loss: 0.002852, mae: 1.283789, mean_q: 1.727259, mean_eps: 0.206972\n",
      " 881916/1000000: episode: 2976, duration: 7.210s, episode steps: 425, steps per second:  59, episode reward: 10.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.647 [0.000, 3.000],  loss: 0.002761, mae: 1.299363, mean_q: 1.746509, mean_eps: 0.206468\n",
      " 882597/1000000: episode: 2977, duration: 11.224s, episode steps: 681, steps per second:  61, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.819 [0.000, 3.000],  loss: 0.002392, mae: 1.285809, mean_q: 1.728097, mean_eps: 0.205970\n",
      " 883222/1000000: episode: 2978, duration: 10.475s, episode steps: 625, steps per second:  60, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.584 [0.000, 3.000],  loss: 0.002567, mae: 1.287287, mean_q: 1.733828, mean_eps: 0.205381\n",
      " 883698/1000000: episode: 2979, duration: 8.005s, episode steps: 476, steps per second:  59, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.710 [0.000, 3.000],  loss: 0.002185, mae: 1.286081, mean_q: 1.730423, mean_eps: 0.204886\n",
      " 884497/1000000: episode: 2980, duration: 13.238s, episode steps: 799, steps per second:  60, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.583 [0.000, 3.000],  loss: 0.002284, mae: 1.288478, mean_q: 1.732855, mean_eps: 0.204312\n",
      " 885042/1000000: episode: 2981, duration: 9.085s, episode steps: 545, steps per second:  60, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.554 [0.000, 3.000],  loss: 0.001688, mae: 1.297831, mean_q: 1.744566, mean_eps: 0.203707\n",
      " 885627/1000000: episode: 2982, duration: 9.822s, episode steps: 585, steps per second:  60, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.656 [0.000, 3.000],  loss: 0.002369, mae: 1.289548, mean_q: 1.734770, mean_eps: 0.203199\n",
      " 886254/1000000: episode: 2983, duration: 10.412s, episode steps: 627, steps per second:  60, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.407 [0.000, 3.000],  loss: 0.002267, mae: 1.288278, mean_q: 1.732174, mean_eps: 0.202654\n",
      " 887123/1000000: episode: 2984, duration: 14.432s, episode steps: 869, steps per second:  60, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.002134, mae: 1.292173, mean_q: 1.738115, mean_eps: 0.201981\n",
      " 888159/1000000: episode: 2985, duration: 17.290s, episode steps: 1036, steps per second:  60, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.622 [0.000, 3.000],  loss: 0.002049, mae: 1.291902, mean_q: 1.738448, mean_eps: 0.201124\n",
      " 889041/1000000: episode: 2986, duration: 14.710s, episode steps: 882, steps per second:  60, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.752 [0.000, 3.000],  loss: 0.002423, mae: 1.290812, mean_q: 1.736838, mean_eps: 0.200260\n",
      " 890048/1000000: episode: 2987, duration: 16.700s, episode steps: 1007, steps per second:  60, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.635 [0.000, 3.000],  loss: 0.001961, mae: 1.293828, mean_q: 1.739602, mean_eps: 0.199410\n",
      " 891072/1000000: episode: 2988, duration: 17.150s, episode steps: 1024, steps per second:  60, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.836 [0.000, 3.000],  loss: 0.002243, mae: 1.303839, mean_q: 1.752372, mean_eps: 0.198498\n",
      " 891655/1000000: episode: 2989, duration: 9.744s, episode steps: 583, steps per second:  60, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.714 [0.000, 3.000],  loss: 0.001790, mae: 1.298849, mean_q: 1.748135, mean_eps: 0.197774\n",
      " 892317/1000000: episode: 2990, duration: 11.016s, episode steps: 662, steps per second:  60, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.002122, mae: 1.299089, mean_q: 1.746788, mean_eps: 0.197213\n",
      " 892793/1000000: episode: 2991, duration: 8.157s, episode steps: 476, steps per second:  58, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.582 [0.000, 3.000],  loss: 0.001852, mae: 1.306020, mean_q: 1.755286, mean_eps: 0.196700\n",
      " 893572/1000000: episode: 2992, duration: 13.137s, episode steps: 779, steps per second:  59, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.772 [0.000, 3.000],  loss: 0.002742, mae: 1.298473, mean_q: 1.746804, mean_eps: 0.196136\n",
      " 894257/1000000: episode: 2993, duration: 11.437s, episode steps: 685, steps per second:  60, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.597 [0.000, 3.000],  loss: 0.001617, mae: 1.303594, mean_q: 1.752550, mean_eps: 0.195477\n",
      " 895586/1000000: episode: 2994, duration: 22.348s, episode steps: 1329, steps per second:  59, episode reward: 34.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.800 [0.000, 3.000],  loss: 0.002188, mae: 1.301644, mean_q: 1.750072, mean_eps: 0.194570\n",
      " 895949/1000000: episode: 2995, duration: 6.061s, episode steps: 363, steps per second:  60, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.446 [0.000, 3.000],  loss: 0.002405, mae: 1.299115, mean_q: 1.746548, mean_eps: 0.193809\n",
      " 896744/1000000: episode: 2996, duration: 13.423s, episode steps: 795, steps per second:  59, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.911 [0.000, 3.000],  loss: 0.001990, mae: 1.300239, mean_q: 1.749022, mean_eps: 0.193289\n",
      " 897541/1000000: episode: 2997, duration: 13.535s, episode steps: 797, steps per second:  59, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.867 [0.000, 3.000],  loss: 0.002372, mae: 1.300595, mean_q: 1.749221, mean_eps: 0.192572\n",
      " 898827/1000000: episode: 2998, duration: 21.341s, episode steps: 1286, steps per second:  60, episode reward: 34.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.764 [0.000, 3.000],  loss: 0.002305, mae: 1.301078, mean_q: 1.750261, mean_eps: 0.191634\n",
      " 899812/1000000: episode: 2999, duration: 16.508s, episode steps: 985, steps per second:  60, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.816 [0.000, 3.000],  loss: 0.002165, mae: 1.299491, mean_q: 1.748133, mean_eps: 0.190614\n",
      " 900604/1000000: episode: 3000, duration: 13.421s, episode steps: 792, steps per second:  59, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.629 [0.000, 3.000],  loss: 0.002737, mae: 1.309263, mean_q: 1.759334, mean_eps: 0.189815\n",
      " 901560/1000000: episode: 3001, duration: 16.089s, episode steps: 956, steps per second:  59, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.659 [0.000, 3.000],  loss: 0.002225, mae: 1.311518, mean_q: 1.762813, mean_eps: 0.189028\n",
      " 902167/1000000: episode: 3002, duration: 10.309s, episode steps: 607, steps per second:  59, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.002222, mae: 1.311994, mean_q: 1.762722, mean_eps: 0.188324\n",
      " 902961/1000000: episode: 3003, duration: 13.324s, episode steps: 794, steps per second:  60, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.812 [0.000, 3.000],  loss: 0.001957, mae: 1.312543, mean_q: 1.765291, mean_eps: 0.187692\n",
      " 903721/1000000: episode: 3004, duration: 12.604s, episode steps: 760, steps per second:  60, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.637 [0.000, 3.000],  loss: 0.002412, mae: 1.308167, mean_q: 1.759803, mean_eps: 0.186992\n",
      " 904560/1000000: episode: 3005, duration: 14.047s, episode steps: 839, steps per second:  60, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.677 [0.000, 3.000],  loss: 0.002691, mae: 1.309514, mean_q: 1.759821, mean_eps: 0.186274\n",
      " 905443/1000000: episode: 3006, duration: 15.089s, episode steps: 883, steps per second:  59, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.403 [0.000, 3.000],  loss: 0.002503, mae: 1.315348, mean_q: 1.768360, mean_eps: 0.185500\n",
      " 906086/1000000: episode: 3007, duration: 10.904s, episode steps: 643, steps per second:  59, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.650 [0.000, 3.000],  loss: 0.002048, mae: 1.311870, mean_q: 1.763685, mean_eps: 0.184812\n",
      " 906867/1000000: episode: 3008, duration: 13.342s, episode steps: 781, steps per second:  59, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.548 [0.000, 3.000],  loss: 0.002825, mae: 1.306186, mean_q: 1.754842, mean_eps: 0.184172\n",
      " 907642/1000000: episode: 3009, duration: 13.134s, episode steps: 775, steps per second:  59, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.707 [0.000, 3.000],  loss: 0.001915, mae: 1.314791, mean_q: 1.767082, mean_eps: 0.183471\n",
      " 908766/1000000: episode: 3010, duration: 18.786s, episode steps: 1124, steps per second:  60, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.689 [0.000, 3.000],  loss: 0.001808, mae: 1.307266, mean_q: 1.758863, mean_eps: 0.182616\n",
      " 909477/1000000: episode: 3011, duration: 11.974s, episode steps: 711, steps per second:  59, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.655 [0.000, 3.000],  loss: 0.002096, mae: 1.307285, mean_q: 1.757375, mean_eps: 0.181790\n",
      " 910336/1000000: episode: 3012, duration: 15.100s, episode steps: 859, steps per second:  57, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.573 [0.000, 3.000],  loss: 0.001872, mae: 1.313467, mean_q: 1.764857, mean_eps: 0.181085\n",
      " 911077/1000000: episode: 3013, duration: 12.623s, episode steps: 741, steps per second:  59, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: 0.002347, mae: 1.314504, mean_q: 1.765221, mean_eps: 0.180365\n",
      " 911815/1000000: episode: 3014, duration: 12.521s, episode steps: 738, steps per second:  59, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.676 [0.000, 3.000],  loss: 0.002136, mae: 1.311070, mean_q: 1.760606, mean_eps: 0.179699\n",
      " 913084/1000000: episode: 3015, duration: 21.488s, episode steps: 1269, steps per second:  59, episode reward: 31.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.774 [0.000, 3.000],  loss: 0.002364, mae: 1.310294, mean_q: 1.759370, mean_eps: 0.178797\n",
      " 913694/1000000: episode: 3016, duration: 10.353s, episode steps: 610, steps per second:  59, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.002051, mae: 1.319449, mean_q: 1.771245, mean_eps: 0.177951\n",
      " 914311/1000000: episode: 3017, duration: 10.506s, episode steps: 617, steps per second:  59, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.744 [0.000, 3.000],  loss: 0.002514, mae: 1.312280, mean_q: 1.763632, mean_eps: 0.177398\n",
      " 915193/1000000: episode: 3018, duration: 15.070s, episode steps: 882, steps per second:  59, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.677 [0.000, 3.000],  loss: 0.002491, mae: 1.318025, mean_q: 1.769855, mean_eps: 0.176723\n",
      " 915876/1000000: episode: 3019, duration: 11.486s, episode steps: 683, steps per second:  59, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.653 [0.000, 3.000],  loss: 0.002183, mae: 1.309993, mean_q: 1.760566, mean_eps: 0.176019\n",
      " 916460/1000000: episode: 3020, duration: 10.003s, episode steps: 584, steps per second:  58, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.454 [0.000, 3.000],  loss: 0.002333, mae: 1.312417, mean_q: 1.764106, mean_eps: 0.175451\n",
      " 916918/1000000: episode: 3021, duration: 7.692s, episode steps: 458, steps per second:  60, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.646 [0.000, 3.000],  loss: 0.001751, mae: 1.316719, mean_q: 1.768791, mean_eps: 0.174981\n",
      " 917819/1000000: episode: 3022, duration: 15.279s, episode steps: 901, steps per second:  59, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.668 [0.000, 3.000],  loss: 0.002077, mae: 1.316154, mean_q: 1.767014, mean_eps: 0.174369\n",
      " 918400/1000000: episode: 3023, duration: 9.902s, episode steps: 581, steps per second:  59, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 0.001978, mae: 1.313987, mean_q: 1.763545, mean_eps: 0.173703\n",
      " 919319/1000000: episode: 3024, duration: 15.753s, episode steps: 919, steps per second:  58, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.824 [0.000, 3.000],  loss: 0.002303, mae: 1.318571, mean_q: 1.771407, mean_eps: 0.173028\n",
      " 920016/1000000: episode: 3025, duration: 11.924s, episode steps: 697, steps per second:  58, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.747 [0.000, 3.000],  loss: 0.002310, mae: 1.306663, mean_q: 1.757156, mean_eps: 0.172301\n",
      " 921017/1000000: episode: 3026, duration: 17.045s, episode steps: 1001, steps per second:  59, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.630 [0.000, 3.000],  loss: 0.002275, mae: 1.320720, mean_q: 1.774833, mean_eps: 0.171536\n",
      " 921738/1000000: episode: 3027, duration: 12.348s, episode steps: 721, steps per second:  58, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.742 [0.000, 3.000],  loss: 0.002285, mae: 1.320809, mean_q: 1.774722, mean_eps: 0.170760\n",
      " 922481/1000000: episode: 3028, duration: 12.570s, episode steps: 743, steps per second:  59, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.662 [0.000, 3.000],  loss: 0.002536, mae: 1.322794, mean_q: 1.776732, mean_eps: 0.170101\n",
      " 923080/1000000: episode: 3029, duration: 10.242s, episode steps: 599, steps per second:  58, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.504 [0.000, 3.000],  loss: 0.001983, mae: 1.319830, mean_q: 1.773299, mean_eps: 0.169498\n",
      " 923983/1000000: episode: 3030, duration: 15.419s, episode steps: 903, steps per second:  59, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.769 [0.000, 3.000],  loss: 0.002596, mae: 1.322639, mean_q: 1.777478, mean_eps: 0.168823\n",
      " 924504/1000000: episode: 3031, duration: 8.888s, episode steps: 521, steps per second:  59, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.754 [0.000, 3.000],  loss: 0.001955, mae: 1.319404, mean_q: 1.771964, mean_eps: 0.168182\n",
      " 925276/1000000: episode: 3032, duration: 13.068s, episode steps: 772, steps per second:  59, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.731 [0.000, 3.000],  loss: 0.002368, mae: 1.323118, mean_q: 1.777997, mean_eps: 0.167601\n",
      " 926174/1000000: episode: 3033, duration: 15.431s, episode steps: 898, steps per second:  58, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.679 [0.000, 3.000],  loss: 0.002164, mae: 1.321533, mean_q: 1.775910, mean_eps: 0.166848\n",
      " 927011/1000000: episode: 3034, duration: 14.300s, episode steps: 837, steps per second:  59, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.693 [0.000, 3.000],  loss: 0.002091, mae: 1.319400, mean_q: 1.773563, mean_eps: 0.166067\n",
      " 927837/1000000: episode: 3035, duration: 14.269s, episode steps: 826, steps per second:  58, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.511 [0.000, 3.000],  loss: 0.002152, mae: 1.328425, mean_q: 1.785314, mean_eps: 0.165318\n",
      " 928665/1000000: episode: 3036, duration: 14.078s, episode steps: 828, steps per second:  59, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.579 [0.000, 3.000],  loss: 0.002879, mae: 1.325623, mean_q: 1.781182, mean_eps: 0.164573\n",
      " 929783/1000000: episode: 3037, duration: 19.032s, episode steps: 1118, steps per second:  59, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.634 [0.000, 3.000],  loss: 0.002299, mae: 1.321916, mean_q: 1.776355, mean_eps: 0.163698\n",
      " 930509/1000000: episode: 3038, duration: 12.425s, episode steps: 726, steps per second:  58, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.585 [0.000, 3.000],  loss: 0.002419, mae: 1.326103, mean_q: 1.780294, mean_eps: 0.162869\n",
      " 931885/1000000: episode: 3039, duration: 23.629s, episode steps: 1376, steps per second:  58, episode reward: 30.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.550 [0.000, 3.000],  loss: 0.001646, mae: 1.325542, mean_q: 1.780361, mean_eps: 0.161922\n",
      " 932438/1000000: episode: 3040, duration: 9.525s, episode steps: 553, steps per second:  58, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.667 [0.000, 3.000],  loss: 0.002036, mae: 1.328913, mean_q: 1.785440, mean_eps: 0.161054\n",
      " 933441/1000000: episode: 3041, duration: 17.346s, episode steps: 1003, steps per second:  58, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.708 [0.000, 3.000],  loss: 0.002091, mae: 1.324047, mean_q: 1.779741, mean_eps: 0.160354\n",
      " 934340/1000000: episode: 3042, duration: 15.656s, episode steps: 899, steps per second:  57, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.790 [0.000, 3.000],  loss: 0.002272, mae: 1.330622, mean_q: 1.787541, mean_eps: 0.159499\n",
      " 935284/1000000: episode: 3043, duration: 16.378s, episode steps: 944, steps per second:  58, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.716 [0.000, 3.000],  loss: 0.002406, mae: 1.327647, mean_q: 1.783592, mean_eps: 0.158671\n",
      " 935888/1000000: episode: 3044, duration: 10.529s, episode steps: 604, steps per second:  57, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.773 [0.000, 3.000],  loss: 0.001849, mae: 1.337467, mean_q: 1.796485, mean_eps: 0.157974\n",
      " 936481/1000000: episode: 3045, duration: 10.130s, episode steps: 593, steps per second:  59, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.501 [0.000, 3.000],  loss: 0.002343, mae: 1.324341, mean_q: 1.778367, mean_eps: 0.157434\n",
      " 937216/1000000: episode: 3046, duration: 12.748s, episode steps: 735, steps per second:  58, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.906 [0.000, 3.000],  loss: 0.002756, mae: 1.326387, mean_q: 1.783416, mean_eps: 0.156837\n",
      " 937904/1000000: episode: 3047, duration: 11.879s, episode steps: 688, steps per second:  58, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.961 [0.000, 3.000],  loss: 0.002579, mae: 1.325077, mean_q: 1.780504, mean_eps: 0.156198\n",
      " 938859/1000000: episode: 3048, duration: 16.387s, episode steps: 955, steps per second:  58, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.691 [0.000, 3.000],  loss: 0.001759, mae: 1.328647, mean_q: 1.783916, mean_eps: 0.155458\n",
      " 939834/1000000: episode: 3049, duration: 16.873s, episode steps: 975, steps per second:  58, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.515 [0.000, 3.000],  loss: 0.002298, mae: 1.336616, mean_q: 1.793962, mean_eps: 0.154589\n",
      " 940672/1000000: episode: 3050, duration: 14.689s, episode steps: 838, steps per second:  57, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.786 [0.000, 3.000],  loss: 0.002683, mae: 1.334971, mean_q: 1.790383, mean_eps: 0.153773\n",
      " 941388/1000000: episode: 3051, duration: 12.380s, episode steps: 716, steps per second:  58, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.682 [0.000, 3.000],  loss: 0.002421, mae: 1.339510, mean_q: 1.797618, mean_eps: 0.153075\n",
      " 942378/1000000: episode: 3052, duration: 17.076s, episode steps: 990, steps per second:  58, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.790 [0.000, 3.000],  loss: 0.002633, mae: 1.329166, mean_q: 1.784841, mean_eps: 0.152306\n",
      " 943090/1000000: episode: 3053, duration: 12.394s, episode steps: 712, steps per second:  57, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.388 [0.000, 3.000],  loss: 0.002622, mae: 1.331653, mean_q: 1.787194, mean_eps: 0.151539\n",
      " 944094/1000000: episode: 3054, duration: 17.190s, episode steps: 1004, steps per second:  58, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.427 [0.000, 3.000],  loss: 0.002572, mae: 1.330752, mean_q: 1.784727, mean_eps: 0.150767\n",
      " 944653/1000000: episode: 3055, duration: 9.439s, episode steps: 559, steps per second:  59, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.544 [0.000, 3.000],  loss: 0.002320, mae: 1.330433, mean_q: 1.786576, mean_eps: 0.150063\n",
      " 945165/1000000: episode: 3056, duration: 8.961s, episode steps: 512, steps per second:  57, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.002268, mae: 1.330654, mean_q: 1.785594, mean_eps: 0.149581\n",
      " 946108/1000000: episode: 3057, duration: 16.392s, episode steps: 943, steps per second:  58, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.373 [0.000, 3.000],  loss: 0.002272, mae: 1.337953, mean_q: 1.795584, mean_eps: 0.148928\n",
      " 946775/1000000: episode: 3058, duration: 11.538s, episode steps: 667, steps per second:  58, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.837 [0.000, 3.000],  loss: 0.002116, mae: 1.327747, mean_q: 1.783152, mean_eps: 0.148204\n",
      " 947389/1000000: episode: 3059, duration: 10.586s, episode steps: 614, steps per second:  58, episode reward: 17.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.502 [0.000, 3.000],  loss: 0.001981, mae: 1.329919, mean_q: 1.786249, mean_eps: 0.147626\n",
      " 948111/1000000: episode: 3060, duration: 12.565s, episode steps: 722, steps per second:  57, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.535 [0.000, 3.000],  loss: 0.002374, mae: 1.334894, mean_q: 1.792272, mean_eps: 0.147025\n",
      " 948635/1000000: episode: 3061, duration: 9.121s, episode steps: 524, steps per second:  57, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.693 [0.000, 3.000],  loss: 0.002561, mae: 1.329591, mean_q: 1.784596, mean_eps: 0.146465\n",
      " 949392/1000000: episode: 3062, duration: 13.296s, episode steps: 757, steps per second:  57, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.613 [0.000, 3.000],  loss: 0.002726, mae: 1.330915, mean_q: 1.787162, mean_eps: 0.145889\n",
      " 950137/1000000: episode: 3063, duration: 13.227s, episode steps: 745, steps per second:  56, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.166 [0.000, 3.000],  loss: 0.002543, mae: 1.334978, mean_q: 1.791712, mean_eps: 0.145212\n",
      " 951070/1000000: episode: 3064, duration: 16.245s, episode steps: 933, steps per second:  57, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.706 [0.000, 3.000],  loss: 0.002668, mae: 1.344919, mean_q: 1.803773, mean_eps: 0.144456\n",
      " 951894/1000000: episode: 3065, duration: 14.214s, episode steps: 824, steps per second:  58, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.684 [0.000, 3.000],  loss: 0.002598, mae: 1.340372, mean_q: 1.799100, mean_eps: 0.143666\n",
      " 952653/1000000: episode: 3066, duration: 13.359s, episode steps: 759, steps per second:  57, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.563 [0.000, 3.000],  loss: 0.002486, mae: 1.349111, mean_q: 1.811967, mean_eps: 0.142953\n",
      " 953198/1000000: episode: 3067, duration: 9.583s, episode steps: 545, steps per second:  57, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.743 [0.000, 3.000],  loss: 0.002321, mae: 1.339503, mean_q: 1.800620, mean_eps: 0.142367\n",
      " 953855/1000000: episode: 3068, duration: 11.454s, episode steps: 657, steps per second:  57, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.813 [0.000, 3.000],  loss: 0.002523, mae: 1.347022, mean_q: 1.810154, mean_eps: 0.141827\n",
      " 954530/1000000: episode: 3069, duration: 11.789s, episode steps: 675, steps per second:  57, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.689 [0.000, 3.000],  loss: 0.002501, mae: 1.337508, mean_q: 1.797542, mean_eps: 0.141227\n",
      " 955368/1000000: episode: 3070, duration: 14.485s, episode steps: 838, steps per second:  58, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.557 [0.000, 3.000],  loss: 0.002247, mae: 1.340964, mean_q: 1.800811, mean_eps: 0.140547\n",
      " 956233/1000000: episode: 3071, duration: 15.111s, episode steps: 865, steps per second:  57, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.702 [0.000, 3.000],  loss: 0.002817, mae: 1.342763, mean_q: 1.804069, mean_eps: 0.139780\n",
      " 957186/1000000: episode: 3072, duration: 16.729s, episode steps: 953, steps per second:  57, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.727 [0.000, 3.000],  loss: 0.003061, mae: 1.342889, mean_q: 1.801623, mean_eps: 0.138961\n",
      " 957772/1000000: episode: 3073, duration: 10.129s, episode steps: 586, steps per second:  58, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.666 [0.000, 3.000],  loss: 0.002651, mae: 1.350930, mean_q: 1.813702, mean_eps: 0.138270\n",
      " 958491/1000000: episode: 3074, duration: 12.670s, episode steps: 719, steps per second:  57, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.750 [0.000, 3.000],  loss: 0.002698, mae: 1.345790, mean_q: 1.809578, mean_eps: 0.137683\n",
      " 959343/1000000: episode: 3075, duration: 14.940s, episode steps: 852, steps per second:  57, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: 0.002173, mae: 1.342156, mean_q: 1.802926, mean_eps: 0.136976\n",
      " 959924/1000000: episode: 3076, duration: 10.275s, episode steps: 581, steps per second:  57, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.518 [0.000, 3.000],  loss: 0.002294, mae: 1.339534, mean_q: 1.800140, mean_eps: 0.136331\n",
      " 960700/1000000: episode: 3077, duration: 13.816s, episode steps: 776, steps per second:  56, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.644 [0.000, 3.000],  loss: 0.002172, mae: 1.343918, mean_q: 1.803088, mean_eps: 0.135721\n",
      " 961882/1000000: episode: 3078, duration: 20.970s, episode steps: 1182, steps per second:  56, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.607 [0.000, 3.000],  loss: 0.001983, mae: 1.341826, mean_q: 1.801840, mean_eps: 0.134839\n",
      " 962569/1000000: episode: 3079, duration: 13.498s, episode steps: 687, steps per second:  51, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.753 [0.000, 3.000],  loss: 0.002407, mae: 1.345067, mean_q: 1.807763, mean_eps: 0.133997\n",
      " 963105/1000000: episode: 3080, duration: 9.488s, episode steps: 536, steps per second:  56, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.728 [0.000, 3.000],  loss: 0.003086, mae: 1.348633, mean_q: 1.812661, mean_eps: 0.133446\n",
      " 964244/1000000: episode: 3081, duration: 20.115s, episode steps: 1139, steps per second:  57, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.696 [0.000, 3.000],  loss: 0.002307, mae: 1.355404, mean_q: 1.820718, mean_eps: 0.132693\n",
      " 965067/1000000: episode: 3082, duration: 14.562s, episode steps: 823, steps per second:  57, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.711 [0.000, 3.000],  loss: 0.003255, mae: 1.346364, mean_q: 1.808570, mean_eps: 0.131811\n",
      " 966102/1000000: episode: 3083, duration: 18.192s, episode steps: 1035, steps per second:  57, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.765 [0.000, 3.000],  loss: 0.002412, mae: 1.349187, mean_q: 1.812390, mean_eps: 0.130974\n",
      " 966696/1000000: episode: 3084, duration: 10.497s, episode steps: 594, steps per second:  57, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.532 [0.000, 3.000],  loss: 0.002482, mae: 1.351300, mean_q: 1.813371, mean_eps: 0.130242\n",
      " 967701/1000000: episode: 3085, duration: 17.840s, episode steps: 1005, steps per second:  56, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.002476, mae: 1.364595, mean_q: 1.833854, mean_eps: 0.129522\n",
      " 968363/1000000: episode: 3086, duration: 11.410s, episode steps: 662, steps per second:  58, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.822 [0.000, 3.000],  loss: 0.002180, mae: 1.354157, mean_q: 1.819006, mean_eps: 0.128771\n",
      " 968988/1000000: episode: 3087, duration: 11.074s, episode steps: 625, steps per second:  56, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.624 [0.000, 3.000],  loss: 0.002513, mae: 1.351638, mean_q: 1.815732, mean_eps: 0.128193\n",
      " 969807/1000000: episode: 3088, duration: 14.461s, episode steps: 819, steps per second:  57, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.751 [0.000, 3.000],  loss: 0.003492, mae: 1.359443, mean_q: 1.827020, mean_eps: 0.127544\n",
      " 971128/1000000: episode: 3089, duration: 23.130s, episode steps: 1321, steps per second:  57, episode reward: 35.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.739 [0.000, 3.000],  loss: 0.002828, mae: 1.356149, mean_q: 1.822621, mean_eps: 0.126581\n",
      " 972087/1000000: episode: 3090, duration: 16.935s, episode steps: 959, steps per second:  57, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.592 [0.000, 3.000],  loss: 0.002324, mae: 1.355298, mean_q: 1.822893, mean_eps: 0.125555\n",
      " 972874/1000000: episode: 3091, duration: 13.849s, episode steps: 787, steps per second:  57, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.694 [0.000, 3.000],  loss: 0.002772, mae: 1.365472, mean_q: 1.834563, mean_eps: 0.124768\n",
      " 973647/1000000: episode: 3092, duration: 13.706s, episode steps: 773, steps per second:  56, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.552 [0.000, 3.000],  loss: 0.002509, mae: 1.362037, mean_q: 1.829009, mean_eps: 0.124066\n",
      " 974914/1000000: episode: 3093, duration: 22.611s, episode steps: 1267, steps per second:  56, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.666 [0.000, 3.000],  loss: 0.002545, mae: 1.360107, mean_q: 1.828486, mean_eps: 0.123148\n",
      " 975743/1000000: episode: 3094, duration: 14.637s, episode steps: 829, steps per second:  57, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002179, mae: 1.355543, mean_q: 1.822652, mean_eps: 0.122205\n",
      " 976667/1000000: episode: 3095, duration: 16.566s, episode steps: 924, steps per second:  56, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.689 [0.000, 3.000],  loss: 0.002202, mae: 1.356317, mean_q: 1.824440, mean_eps: 0.121416\n",
      " 977287/1000000: episode: 3096, duration: 11.206s, episode steps: 620, steps per second:  55, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.866 [0.000, 3.000],  loss: 0.002443, mae: 1.355719, mean_q: 1.821465, mean_eps: 0.120722\n",
      " 977790/1000000: episode: 3097, duration: 10.373s, episode steps: 503, steps per second:  48, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.571 [0.000, 3.000],  loss: 0.002851, mae: 1.358832, mean_q: 1.827418, mean_eps: 0.120216\n",
      " 978578/1000000: episode: 3098, duration: 13.934s, episode steps: 788, steps per second:  57, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.581 [0.000, 3.000],  loss: 0.002497, mae: 1.355546, mean_q: 1.822999, mean_eps: 0.119634\n",
      " 979263/1000000: episode: 3099, duration: 11.955s, episode steps: 685, steps per second:  57, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.464 [0.000, 3.000],  loss: 0.002663, mae: 1.356703, mean_q: 1.823199, mean_eps: 0.118972\n",
      " 980033/1000000: episode: 3100, duration: 13.736s, episode steps: 770, steps per second:  56, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.799 [0.000, 3.000],  loss: 0.002418, mae: 1.354132, mean_q: 1.822649, mean_eps: 0.118317\n",
      " 981176/1000000: episode: 3101, duration: 20.600s, episode steps: 1143, steps per second:  55, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.626 [0.000, 3.000],  loss: 0.002554, mae: 1.353393, mean_q: 1.818803, mean_eps: 0.117456\n",
      " 982358/1000000: episode: 3102, duration: 20.886s, episode steps: 1182, steps per second:  57, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.706 [0.000, 3.000],  loss: 0.002692, mae: 1.360516, mean_q: 1.829160, mean_eps: 0.116411\n",
      " 983103/1000000: episode: 3103, duration: 13.049s, episode steps: 745, steps per second:  57, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.358 [0.000, 3.000],  loss: 0.002631, mae: 1.361121, mean_q: 1.831231, mean_eps: 0.115543\n",
      " 983894/1000000: episode: 3104, duration: 14.143s, episode steps: 791, steps per second:  56, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.002302, mae: 1.358454, mean_q: 1.827201, mean_eps: 0.114852\n",
      " 985052/1000000: episode: 3105, duration: 20.506s, episode steps: 1158, steps per second:  56, episode reward: 30.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.002244, mae: 1.352489, mean_q: 1.821617, mean_eps: 0.113975\n",
      " 985589/1000000: episode: 3106, duration: 9.564s, episode steps: 537, steps per second:  56, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.683 [0.000, 3.000],  loss: 0.002714, mae: 1.358430, mean_q: 1.827322, mean_eps: 0.113212\n",
      " 986482/1000000: episode: 3107, duration: 15.892s, episode steps: 893, steps per second:  56, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.811 [0.000, 3.000],  loss: 0.002922, mae: 1.365159, mean_q: 1.837660, mean_eps: 0.112568\n",
      " 987172/1000000: episode: 3108, duration: 12.383s, episode steps: 690, steps per second:  56, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.533 [0.000, 3.000],  loss: 0.002357, mae: 1.363193, mean_q: 1.833571, mean_eps: 0.111857\n",
      " 988372/1000000: episode: 3109, duration: 21.528s, episode steps: 1200, steps per second:  56, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.736 [0.000, 3.000],  loss: 0.002604, mae: 1.361583, mean_q: 1.832861, mean_eps: 0.111007\n",
      " 989190/1000000: episode: 3110, duration: 14.445s, episode steps: 818, steps per second:  57, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.628 [0.000, 3.000],  loss: 0.002091, mae: 1.355742, mean_q: 1.825924, mean_eps: 0.110098\n",
      " 989941/1000000: episode: 3111, duration: 13.396s, episode steps: 751, steps per second:  56, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.434 [0.000, 3.000],  loss: 0.002771, mae: 1.354953, mean_q: 1.825242, mean_eps: 0.109391\n",
      " 990774/1000000: episode: 3112, duration: 14.864s, episode steps: 833, steps per second:  56, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.610 [0.000, 3.000],  loss: 0.003065, mae: 1.374140, mean_q: 1.847791, mean_eps: 0.108678\n",
      " 991519/1000000: episode: 3113, duration: 13.249s, episode steps: 745, steps per second:  56, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.621 [0.000, 3.000],  loss: 0.002831, mae: 1.370043, mean_q: 1.842970, mean_eps: 0.107969\n",
      " 992389/1000000: episode: 3114, duration: 15.573s, episode steps: 870, steps per second:  56, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.777 [0.000, 3.000],  loss: 0.002667, mae: 1.368873, mean_q: 1.842902, mean_eps: 0.107241\n",
      " 993029/1000000: episode: 3115, duration: 11.451s, episode steps: 640, steps per second:  56, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.875 [0.000, 3.000],  loss: 0.002326, mae: 1.374164, mean_q: 1.848851, mean_eps: 0.106561\n",
      " 994335/1000000: episode: 3116, duration: 23.213s, episode steps: 1306, steps per second:  56, episode reward: 34.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.649 [0.000, 3.000],  loss: 0.002157, mae: 1.374116, mean_q: 1.847940, mean_eps: 0.105686\n",
      " 995201/1000000: episode: 3117, duration: 15.519s, episode steps: 866, steps per second:  56, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.615 [0.000, 3.000],  loss: 0.002767, mae: 1.372314, mean_q: 1.846000, mean_eps: 0.104709\n",
      " 996150/1000000: episode: 3118, duration: 16.994s, episode steps: 949, steps per second:  56, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.717 [0.000, 3.000],  loss: 0.002345, mae: 1.371006, mean_q: 1.845309, mean_eps: 0.103892\n",
      " 996933/1000000: episode: 3119, duration: 14.180s, episode steps: 783, steps per second:  55, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.728 [0.000, 3.000],  loss: 0.002382, mae: 1.368377, mean_q: 1.839788, mean_eps: 0.103112\n",
      " 997492/1000000: episode: 3120, duration: 10.177s, episode steps: 559, steps per second:  55, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.526 [0.000, 3.000],  loss: 0.002940, mae: 1.372311, mean_q: 1.844442, mean_eps: 0.102509\n",
      " 998353/1000000: episode: 3121, duration: 15.509s, episode steps: 861, steps per second:  56, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.699 [0.000, 3.000],  loss: 0.002191, mae: 1.369013, mean_q: 1.843491, mean_eps: 0.101870\n",
      " 999526/1000000: episode: 3122, duration: 21.106s, episode steps: 1173, steps per second:  56, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.556 [0.000, 3.000],  loss: 0.002383, mae: 1.374455, mean_q: 1.849745, mean_eps: 0.100954\n",
      "done, took 12917.505 seconds\n"
     ]
    }
   ],
   "source": [
    "history = dqn.fit(env, nb_steps=1000000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618ba503-2471-4564-8c02-3311eba727a0",
   "metadata": {},
   "source": [
    "## 6. Sauvegarde du modèle et de l'historique\n",
    "\n",
    "Les poids du modèle sont sauvegardés dans `policy.h5`.\n",
    "L'historique d'entraînement est sauvegardé dans `training_history.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8961cf17-3209-47c1-b773-367c78c5a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('policy.h5', overwrite=True)\n",
    "\n",
    "with open('training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bef9d419-0692-4930-ac9b-93a98b952b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualisation de la performance\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(history.history['episode_reward'])\n",
    "plt.title('Episode Reward')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_performance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dafcf3a-ad98-4be8-987f-e93fe376601c",
   "metadata": {},
   "source": [
    "## 7. Test\n",
    "\n",
    "* Création d'un env de test\n",
    "* application du prétraitement à l'env\n",
    "* test sur 10 épisodes avec visualisation\n",
    "* Affichage du score moyen sur les 10 épisodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e6386d0-4a85-4ff9-9bde-ab651c756661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33626\\anaconda3\\envs\\deep\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 15.000, steps: 642\n",
      "Episode 2: reward: 33.000, steps: 1208\n",
      "Episode 3: reward: 29.000, steps: 1209\n",
      "Episode 4: reward: 25.000, steps: 1017\n",
      "Episode 5: reward: 26.000, steps: 1038\n",
      "Episode 6: reward: 22.000, steps: 840\n",
      "Episode 7: reward: 22.000, steps: 800\n",
      "Episode 8: reward: 19.000, steps: 765\n",
      "Episode 9: reward: 30.000, steps: 1162\n",
      "Episode 10: reward: 27.000, steps: 1085\n",
      "Average score over 10 test episodes: 24.8\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make('ALE/Breakout-v5', render_mode='human')\n",
    "test_env = AtariPreprocessing(test_env,\n",
    "                              screen_size=84,\n",
    "                              grayscale_obs=True,\n",
    "                              frame_skip=1,\n",
    "                              noop_max=30)\n",
    "test_env = CompatibilityWrapper(test_env)\n",
    "\n",
    "class RenderWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        return self.env.render()  # Ignorer les arguments mode\n",
    "\n",
    "test_env = RenderWrapper(test_env)\n",
    "\n",
    "scores = dqn.test(test_env,\n",
    "                  nb_episodes=10,\n",
    "                  visualize=True)\n",
    "print('Average score over 10 test episodes:',\n",
    "      np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6cca5e-ac77-4c97-8e24-bfdf86f401df",
   "metadata": {},
   "source": [
    "## 8. Clore l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a15ba6-0ed9-4150-ac06-28b63249ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
