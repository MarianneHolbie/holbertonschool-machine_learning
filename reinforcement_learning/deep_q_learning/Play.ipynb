{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc54506-dd4a-4331-a155-512701d5cee3",
   "metadata": {},
   "source": [
    "# Deep Q Learning pour Breakout d'Atari: **play script**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dafb96-b25d-4e85-aad8-baa449141466",
   "metadata": {},
   "source": [
    "## 1. Import et bilbiothèques\n",
    "\n",
    "* **gymnasium** : Création et gestion de l'environnement Atari\n",
    "* **tensorflow.keras**: Construction du réseau de neurones\n",
    "* **pygame** : Gestion de l'affichage du jeu\n",
    "* **rl** : Implémentation de l'agent DQN et de ses composants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe32f9c-9438-408b-8e97-4d98130a33f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33626\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Permute\n",
    "import time\n",
    "import pygame\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import GreedyQPolicy\n",
    "from rl.util import *\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852549dd-bbdd-4bcd-a60b-167314deedb9",
   "metadata": {},
   "source": [
    "## 2. Configuration de l'environnement\n",
    "\n",
    "On réutilise les classes et fonctions créées dans le script `train.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d301daf-e800-4cdc-9ea8-2a39e117e101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibility wrapper\n",
    "class CompatibilityWrapper(gym.Wrapper):\n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observation, info = self.env.reset(**kwargs)\n",
    "        return observation\n",
    "\n",
    "# fonction pour la création de l'environnement\n",
    "def create_atari_environment(env_name):\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    env = AtariPreprocessing(env,\n",
    "                             screen_size=84,\n",
    "                             grayscale_obs=True,\n",
    "                             frame_skip=1,\n",
    "                             noop_max=30)\n",
    "    env = CompatibilityWrapper(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327f7675-14b4-4c27-a0d1-8ba452b27fab",
   "metadata": {},
   "source": [
    "## 3. Construction du modèle CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c659cdda-ce82-4908-9d87-2b17d893940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(window_length, shape, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Permute((2, 3, 1), input_shape=(window_length,) + shape))\n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu'))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6c2e58-4133-4010-b525-4039374a6e26",
   "metadata": {},
   "source": [
    "## 4.  Définition du Processeur Atari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5c74256-c4b2-47fc-b613-d0ab2fb93875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        if isinstance(observation, tuple):\n",
    "            observation = observation[0]\n",
    "        img = np.array(observation)\n",
    "        img = img.astype('uint8')\n",
    "        return img\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cf113e-558f-48fd-aece-0846be419903",
   "metadata": {},
   "source": [
    "## 5. Création d'un Callback Pygame\n",
    "\n",
    "Ce callback gère l'affichage du jeu avec Pygame :\n",
    "\n",
    "1. Initialise la fenêtre Pygame\n",
    "2. Affiche chaque frame du jeu après chaque action\n",
    "3. Gère les événements Pygame (comme la fermeture de la fenêtre)\n",
    "4. Ajoute un délai entre les épisodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e38a47cb-bf0a-4fb6-96b6-1cee2e329d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PygameCallback(Callback):\n",
    "    def __init__(self, env, delay=0.02):\n",
    "        self.env = env\n",
    "        self.delay = delay\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((420, 320))\n",
    "        pygame.display.set_caption(\"Atari Breakout - DQN Agent\")\n",
    "\n",
    "    def on_action_end(self, action, logs={}):\n",
    "        frame = self.env.render()\n",
    "        surf = pygame.surfarray.make_surface(frame.swapaxes(0, 1))\n",
    "        surf = pygame.transform.scale(surf, (420, 320))\n",
    "        self.screen.blit(surf, (0, 0))\n",
    "        pygame.display.flip()\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                self.env.close()\n",
    "                pygame.quit()\n",
    "        time.sleep(self.delay)\n",
    "\n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        pygame.time.wait(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b1a72b-9e30-4ebf-9441-42daa988f6f4",
   "metadata": {},
   "source": [
    "## 6. Programme principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "744cb6ed-c0b6-47ae-ad1a-536a2fb3100e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33626\\anaconda3\\envs\\deep\\lib\\site-packages\\keras\\src\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "C:\\Users\\33626\\anaconda3\\envs\\deep\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 28.000, steps: 1071\n",
      "Episode 2: reward: 32.000, steps: 1212\n",
      "Episode 3: reward: 41.000, steps: 1516\n",
      "Episode 4: reward: 22.000, steps: 907\n",
      "Episode 5: reward: 20.000, steps: 832\n",
      "Average score over 5 test episodes: 28.6\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1. CREATE ENV\n",
    "    env = create_atari_environment('ALE/Breakout-v5')\n",
    "    nb_actions = env.action_space.n\n",
    "\n",
    "    # 2. BUILD MODEL\n",
    "    window_length = 4\n",
    "    input_shape = (84, 84)\n",
    "    model = build_model(window_length, input_shape, nb_actions)\n",
    "\n",
    "    # 3. LOAD TRAINED WEIGHTS\n",
    "    model.load_weights('policy.h5')\n",
    "\n",
    "    # 4. CONFIGURE AGENT\n",
    "    memory = SequentialMemory(limit=1000000, window_length=window_length)\n",
    "    processor = AtariProcessor()\n",
    "    policy = GreedyQPolicy()\n",
    "\n",
    "    dqn = DQNAgent(model=model,\n",
    "                   nb_actions=nb_actions,\n",
    "                   policy=policy,\n",
    "                   memory=memory,\n",
    "                   processor=processor,\n",
    "                   nb_steps_warmup=50000,\n",
    "                   gamma=.99,\n",
    "                   target_model_update=10000,\n",
    "                   train_interval=4,\n",
    "                   delta_clip=1.)\n",
    "    dqn.compile(optimizer='adam', metrics=['mae'])\n",
    "\n",
    "    # 5. TEST AGENT\n",
    "    pygame_callback = PygameCallback(env, delay=0.02)\n",
    "    scores = dqn.test(env, nb_episodes=5, visualize=False, callbacks=[pygame_callback])\n",
    "\n",
    "    # 6. DISPLAY RESULT\n",
    "    print('Average score over 5 test episodes:', np.mean(scores.history['episode_reward']))\n",
    "\n",
    "    # 7. CLOSE ENV AND PYGAME\n",
    "    env.close()\n",
    "    pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab524ec-3aeb-4a6e-bcb6-45d518748626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
